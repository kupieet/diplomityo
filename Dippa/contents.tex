\chapter{Introduction}
\label{chapter:intro}
 % Software engineering is at a crossroads as there are new leaner and more
% agile software development methods appearing next to the traditional
% software development methods.
% \mika{Kappaleen pointti: No literature reviews of actual metric use}
Software metrics have been studied for decades and several literature reviews
have been published.
Yet, the literature reviews have been written from an academic viewpoint. For
example, Catal et al. review fault prediction metrics \citepsrc{catal2009systematic},
\citetsrc{purao2003product} review metrics for object oriented systems and
\citetsrc{kitchenham_whats_2010} performs a mapping of most cited software
metrics papers. According to the researcher's knowledge there are no
systematic literature reviews on the actual use of software metrics in the
industry.

%\mika{Kappaleen pointti: Agile on tärkeää eikä metriikkoja tutkittu}

% \juha{Yritin konkretisoida trad vs. agile kontrastia}
Agile software development is becoming increasingly popular in the software
industry. The agile approach seems to be contradicting with the traditional
metrics approaches. For example, the agile emphasizes working software over
measuring progress in terms of intermediate products or documentation, and
embracing the change invalidates the traditional approach of tracking progress
against pre-made plan.
However, at the same time agile software development highlights some measures
that should be used, e.g., burndown graphs and 100\% automated unit testing
coverage. However, metric research in the context of agile methods
remains scarce.

The goal of this paper is to review the literature of actual use of software
metrics in the context of agile software development. This study lays out
the current state of metrics usage in industrial agile software development
based on literature. Moreover, the study uncovers the reasons for metric
usage as well as highlights actions that the use of metrics can trigger.
%Due to our research
%goal, we focus this paper on case studies and actual empirical findings
%excluding theoretical discussion and models lacking empirical validation.

This study covers the following research questions: 

\begin{itemize}
  \item Research Question 1: \emph{What metrics are used in industrial
  agile software development?}
  \item Research Question 2: \emph{Why are metrics used in industrial
  agile software development?} 
  \item Research Question 3: \emph{What are the effects of using metrics in industrial
  agile software development?} 
  \item Research Question 4: \emph{What metrics are important in industrial
  agile software development?}
\end{itemize} 

%\section{Problem statement}

\section{Agile software development}
\label{sec:agileSoftwareDevelopment}
%\eetu{tässä voisi olla historianäkökulma, voipi mennä pitkäks eikä niin
%mielenkiintoinen ehkä tässä paperissa} 

Agile development methods have emerged to the software world ruled by
traditional heavyweight methods. In agile methods the focus is in lightweight
working practices, constant deliveries and customer collaboration over long
planning periods, heavy documentation and inflexible development phases.

Agile manifesto created by agile enthusiasts \citepsrc{beck2001agile} lists
agile principles that give an idea what agile development is about. Popular
agile development methods include Scrum \citepsrc{schwaber2002agile}, Extreme
Programming \citepsrc{beck2004extreme}, Lean Software Development
\citepsrc{poppendieck2003lean} and Kanban \citepsrc{anderson2010kanban}.

Scrum \citepsrc{schwaber2002agile} got its name from rugby, where the team
groups up and plans its next move. Similarly in software development, the scrum team has daily meetings
where they see what has been done and what should be done next. On high level
the development is constructed from multiple subsequent sprints, where an
increment of the software is developed. Sprints are planned by
selecting items from a backlog and estimating the effort for needed to
complete each item selected for the sprint. During sprints the team groups
up every day for the daily meeting where the status of the tasks is tracked.
At the end of the sprint a sprint review is organized where each of the tasks
are reviewed. Learning is emphasized every sprint with a sprint retrospective
meeting where issues related to work practices are discussed and improved. 

Extreme Programming (XP) \citepsrc{beck2004extreme} got its name from a set of
principles and practices that were emphasized to the extreme. For example, if you think testing is a
good practice then test all the time with automated unit tests. XP aims at
embracing change. This is achieved in XP by continuously refactoring code
base, creating and maintaining a comprehensive unit test suite and designing
simple. Changes in business requirements can then be flexibly developed.
Communication is also very important in XP. Communication is handled with unit
tests, pair-programming and having a customer available to answer business
questions. 

Lean Software Development (LeanSD) \citepsrc{poppendieck2003lean} contains
many attributes from Scrum and XP, but it also utilises principles from Lean
manufacturing, which Toyota used to build their competitive edge.
\citetsrc{womack2007machine}. The modified lean principles used in LeanSD are:

\begin{itemize}
  \item Eliminate Waste
  \item Amplify Learning
  \item Decide as Late as Possible
  \item Deliver as Fast as Possible
  \item Empower the Team
  \item Build Integrity In
  \item See the Whole 
\end{itemize}


The aforementioned agile methods are often well defined and they require their
practices to be used comprehensively. Kanban \citepsrc{anderson2010kanban} on
the other hand is more of an evolutionary process model that allows each
Kanban implementation to be different, suited for each context. Some
principles however are the same in Kanban. For example, Kanban systems are
always pull-systems. Work is pulled to development only when there is
capacity, compared to some other systems where work is pushed to development
according to demand. The pull system is enabled by limiting work in progress
(WIP). For example, if WIP limit is two for development and there is already
two work items under development then no more work can be started before
either of them is completed.

\section{Structure of the Thesis}
\label{section:structure} 

This thesis is structured as follows. %\Cref{sec:background} provides
% information about related concepts regarding this study.
\Cref{sec:Method} describes how the systematic literature review (SLR) was conducted. \Cref{sec:Results}
reports the results from the study.
\Cref{sec:Discussion} discusses about the findings and how they map to agile
principles. \Cref{sec:Conclusions} concludes the paper.


%\section{Software measurement}
%Measurements for software industry started from a technical point of view.

%\section{Related work}


%\fixme{tää jäi vähän laihaks. pitäiskö siirtää kamat discussionin alle}


%\section{Evidence based software engineering}

%\subsection{Measurement}
%According to Fenton et al.\cite{fenton1998software} ``Measurement is the
%process by which numbers of symbols are assigned to attributes of entities in
%the real world in such way as to describe them according to clearly defined
%rules.''


%\section{Previous metric research}

%There are a few mapping studies on software metrics(tähän vois lisätä ne
% kitch whats 2010:ssä olevat muut mut en tiiä mitä lisäarvoa ne tois).
%\cite{kitchenham_whats_2010} says there is a large body of research related
% to software metrics. However, she highlights that all evidence should be
%critically appraised so that further studies can be based on good quality
%evidence. She also reminds researchers to understand the context where
% metrics are taken from - failure to understand context will probably not provide
%answers to industry-related questions. 

%--EBSE
%--Mittaamisen SR:t
%--Agile mittaamisen muut tutkimukset vai enemmänkin tutkimuksessa käytettyjen
%termien ja käsitteiden selittäminen


%\section{Aims and research questions} 
%The aim of this paper is to provide
% preliminary results from a systematic review (SR) on agile metrics.
%Moreover, we are interested on the industrial use of metrics in agile
% context.

\chapter{Review method}
\label{sec:Method}


Systematic literature review (SLR) was chosen as research method because the
study is more about trying to understand a problem instead of trying to find a
solution to it. Also, there was already existing literature that could be
synthesized. Systematic literature review is a research method originated from
the field of medicine \citepsrc{kitchenham2004procedures}. The overarching
idea is to aggregate and synthesise existing knowledge regarding a research
topic. This rigorous and audible evaluation method can facilitate theory
development and point out gaps in research \citepsrc{webster2002analyzing}.


\section{Protocol development}
Kitchenham's guide for SLRs \citepsrc{kitchenham2004procedures} was used as a
basis for developing the review protocol. Additionally, an SLR on agile
development \citepsrc{dyba_empirical_2008} and an SLR on SLR
\citepsrc{kitchenham2013systematic} were used to further understand the
challenges and opportunities of SLRs. The protocol was also iterated in weekly
meetings with the instructors, as well as in a pilot study.

%otherguidelines \cite{webster2002analyzing}, a lessons learned from SRs
% \cite{brereton2007lessons},
\section{Search and selection process}

The strategy for finding primary studies was following:

\begin{itemize}
  \item Stage 1: Automated search
  \item Stage 2: Selection based on title and abstract
  \item Stage 3: Selection based on full text. Conduct data
  extraction and quality assessment.
\end{itemize}

\Cref{SelectionFunnel} shows the selection funnel in terms of the number
of papers after each stage.

%\includegraphics{SelectionFunnel.jpg}

\begin{table}
\centering
\caption{Paper selection funnel}
\begin{tabular}{lcr}
\toprule
Stage & Amount of papers \\
\midrule
Stage 1 & 774 \\  
Stage 2 & 163 \\ 
Stage 3 & 30\\
\bottomrule
\label{SelectionFunnel}
\end{tabular}
\end{table}

% \subsubsection{Search strings}
Scopus database \footnote{http://www.scopus.com} was used to find the primary
documents with automated search. Keywords include popular agile development
methods and synonyms for the word metric. The search was improved
incrementally in three phases because some key papers and XP
conferences were not found initially. The search strings, hits and dates can
be found from \cref{app:Strings}.

%\juha{Laittaisin myös inclusion ja exclusion kriteerit appendixiin ja
% jättäisin vain tämänkaltaisen lyhyen kuvauksen tänne} 
The selection of the primary documents was based on inclusion criteria:
\emph{papers that present empirical findings on the industrial use and
experiences of metrics in agile context.} The papers were excluded based on
multiple criteria, mainly due to not conforming to requirements regarding
empirical findings and agile and industrial context. Full criteria are listed
in \cref{app:Criteria}.

%\juha{Inclusion criteria ja exclusion criteria siirretty -> appendix}
%\subsubsection{Inclusion criteria} \juha{->APPENDIX}
%
%\begin{itemize}
%  \item Papers that present the use and experiences of metrics in an agile
%  industry setting.
%\end{itemize}
%
%\subsubsection{Exclusion criteria} \juha{->APPENDIX}
%
%\begin{itemize}
%  \item Papers that don't contain empirical data from industry cases.
%  \item Papers that are not in English.
%  \item Papers that don't have agile context. There is evidence of
%  clearly non-agile practices or there is no agile method named. For example,
%  paper mentions agile but case company has only three releases per year.
%  \item Paper is only about one agile practice, which is not related to
%  measuring.
%  \item Papers that don't seem to have any data about metric usage. Similarly,
%  if there are only a few descriptions of metrics but no other info regarding
%  reasons or usage.
%  \item Papers that have serious issues with grammar or vocabulary and
%  therefore it takes considerable effort to understand sentences.
%  \item Papers that refer to another paper where the actual case is discussed.
%  \item Papers that are in academic or semi-academic setting - customer or
%  part of workers are from industry. The reason for this is that it doesn't
%  fully represent industry setting as software development methods are likely
%  enforced by academia.
%  \item Papers where results cannot be separated by setting, for example
%  surveys where there is data both from academia and industry. Similar
%  exclusion if results cannot be separated by software development method. 
%  \item Papers where the setting is not clear. For example only mention of
%  context is ``100 junior developers''.
%  \item Papers that are full conference proceedings. Individual papers should
%  be already listed separately.
%  \item Papers where the measurements are only used for the research. For
%  example author measures which agile practices correlate with success.
%  \item Papers that don't even show measurement usage in a pilot setting. For
%  example method or metric is used against static industrial data set.
%  %\item Papers that are about the same case, from the same author and same
%  %research focus, basically the paper format has just changed slightly.
%\end{itemize}


%\subsubsection{Stage 1 - Automatic search}

In stage 1, Scopus was used as the only search engine as it contained the most
relevant databases IEEE and ACM. Also, it was able to find Agile and XP conference
papers. Only XP Conference 2013 was searched manually because it could not be
found through Scopus.

%\subsubsection{Pilot}
%\label{pilot}
%We conducted a pilot study in order to refine the aim of the research and get
%familiar with the research method. Moreover, it was possible to modify the
%method and tools.

%15 papers were selected for the pilot; 5 by relevance, 5 by number of
%citations, and 5 by random selection.
% \begin{itemize} \item 5 by top relevance \item 5 by top citations \item 5 by
% random \end{itemize}
%\juha{tämä mahdollista poistaa (ainakin omana alilukuna) jos tila ei
% riitä}The pilot resulted in changing citation manager tool from Zotero to Jabref.
%Also, selection by title and selection by abstract steps were joined
% together.
%The quality assessment checklist was decided based on the pilot results.

%\subsubsection{Stage 2 - Selection by title and abstract}

In stage 2, papers were included and excluded by the researcher based on their
title and abstract. As the quality of abstracts can be poor in computer
science \citepsrc{kitchenham2004procedures}, full texts were also skimmed
through in case of unclear abstracts.
% - especially introduction, case description and conclusions were checked
% briefly.
Unclear cases were discussed with the instructors in weekly meetings and an
exclusion rule was documented if necessary.

The validity of the selection process was analysed by performing the selection
for a random sample of 26 papers also by the second instructor. The level of
agreement was 'substantial' with Kappa 0.67
\citepsrc{landis_measurement_1977}.

%\subsubsection{Stage 3 - Selection by full text}

Stage 3 included multiple activities in one workflow. Selection by full text
was done, data was coded and quality assessment was done. Once again, if there
were unclear papers, they were discussed in the meetings. Also, selection of 7
papers was conducted by the second instructor with an 'almost perfect'
agreement, Kappa 1.0 \citepsrc{landis_measurement_1977}.

\section{Data extraction}
\label{sec:dataExtraction}

Integrated coding was selected for data extraction strategy
\citepsrc{6092576}. Integrated coding includes having a start list of codes as
well as creating new codes if necessary (ground-up). It provided focus to
research questions but flexibility regarding findings.
Deductive coding would have been too restraining and inductive coding might
have caused too much bias. Integrated coding made it possible to create a
sample list of code categories:

\begin{itemize}
  \item Why is metric used?
  \item How is metric used?
  \item Metric
  \item Importance related to metric
  \item Context
\end{itemize}

The coding started with the researcher reading the full text and marking
interesting quotes with a temporary code. After, reading the full text the
researcher checked each quote and coded again with an appropriate code based
on the built understanding. In weekly meetings with the instructors, a rule
set for collecting metrics was slowly built:

\begin{itemize}
  \item Collect metric if team or company uses it.
  \item Collect metric only if something is said about why it is used, what
  actions it causes or if it is described as important.
  \item Do not collect metrics that are only used for the comparison and
  selection of development methods. 
  \item Do not collect metrics that are primarily used to compare teams.
  (There were cases where a researcher or management uses a metric to compare teams. We wanted to find metrics a team could use.) 
\end{itemize}

Atlas.ti Visual QDA (Qualitative Data Analysis), version 7.1.x was used to
collect and synthesize the qualitative data. Amount of found quotes per code
can be seen in \Cref{tab:quotes}.

To evaluate the repeatability of finding the same metrics, second instructor
coded metrics from three papers. Capture-recapture method
\citepsrc{seber2002estimation} was then used which showed that 90\% of metrics
were found.

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Amount of found quotes}
    \begin{tabular}{rr}
    \toprule
    Code  & Amount of quotations\\
    \midrule
    Why use this metric? & 151\\
    How is the metric used? & 61\\
    Metrics & 108 \\
    Importance related to metric & 45 \\
    Context & 158 \\
    \bottomrule
    \end{tabular}%
  \label{tab:quotes}%
\end{table}%

A quality assessment form adopted from \citepsrc{dyba_empirical_2008} was used
to evaluate the quality of each primary study. Detailed list of quality
assessment questions can be found in \cref{app:quality}. Additionally, a
relevancy factor was added to the same assessment to describe how useful the
paper was for this study. The scale for the relevancy factor is:

\begin{itemize}
  \item 0 = does not contain any information regarding metrics and should be
  already excluded
  \item 1 = only descriptions of metrics with no additional info
  \item 2 = some useful information related to metrics
  \item 3 = a good amount of relevant information regarding metrics and metric
  usage
\end{itemize}

\section{Data synthesis}
Data synthesis followed the steps recommended by \citetsrc{6092576}.
Process started by going through all quotes within one code and giving each quote a
more descriptive code describing the quote in high level. Then the descriptive
codes were organized in groups based on their similarity. These groups were
then given high level codes which are seen as categories in
\Cref{tab:whyCategories} and \Cref{tab:howCategories}.

\chapter{Results} % - Why and how are metrics used
\label{sec:Results}

This chapter presents the results from the systematic literature 
review. \Cref{sec:overviewOfStudies} describes the
overview of studies and results from the quality evaluation.
\Cref{sec:metricsCategorization} categorizes found metrics according to
categorization by \citetsrc{fenton1998software}. \Cref{sec:WhyMetricsUsed}
describes the reasons for using metrics. \Cref{sec:effects} describes the
effects of metric use. \Cref{sec:importantMetrics} describes
important metrics by statements from the primary studies as well as by amount
of evidence from primary studies.

\section{Overview of studies}
\label{sec:overviewOfStudies}

This section gives an overview of the primary studies.
\Cref{tab:PublicationDistribution} shows the distribution of primary studies
by publication channels. \Cref{tab:overviewOfPdocs} lists the primary studies
by context factors. 

Most primary studies (43\%) were published in Agile Conference as can be seen
from \Cref{tab:PublicationDistribution}. Rest of the studies have been
published in a wide range of journals, conferences and workshops.

\begin{table}
\centering
\caption{Publication distribution of primary studies}
\label{tab:PublicationDistribution}
\begin{tabular}{llll}
\toprule Publication channel & Type & \# & \%\\
\midrule
	Agile Conference & Conference & 9    & 43 \\
    HICCS & Conference & 3     & 14 \\
    ICSE SDG & Workshop & 2     & 10 \\
    XP Conference & Conference & 2     & 10 \\
    Agile Development Conference & Conference & 1     & 5 \\
    APSEC & Conference & 1     & 5 \\
    ASWEC & Conference & 1     & 5 \\
    ECIS  & Conference & 1     & 5 \\
    Elektronika ir Elektrotechnika & Journal & 1     & 5 \\
    Empirical Software Engineering & Journal & 1     & 5 \\
    EUROMICRO & Conference & 1     & 5 \\
    ICSE  & Conference & 1     & 5 \\
    IST   & Journal & 1     & 5 \\
    IJPQM & Journal & 1     & 5 \\
    JSS   & Journal & 1     & 5 \\
    PROFES & Conference & 1     & 5 \\
    Software - Prac. and Exp. & Journal & 1     & 5 \\
    WETSoM & Workshop & 1     & 5 \\
\bottomrule
\end{tabular}
\end{table}

Primary studies and their context information can be seen in
\Cref{tab:overviewOfPdocs}. The earliest study is from 2002 and rest of the
studies are quite evenly distributed from 2002 to 2013.  Singlecase was most
used research method with 18 studies, then experience report with 8 studies,
multicase with with 3 studies and finally survey with 2 studies. 

Agile method for the studies was set based on the assessment of the
researcher. A specific method was chosen if it seemed to be a primary method
in the case. If it was not clear what agile method was used then 'NA' was set
as agile method. Based on the results, Scrum was the most used agile method
(35\%). XP was the second most used (20\%). LeanSD and Kanban were used in 5\%
of the cases. In 33\% of the cases the used agile method was not clear.

Telecom was the most represented domain with 10 cases, enterprise information
systems was the second with 7 cases and web applications with 4 cases. There
were 15 cases that were other domains or cases without domain information.


\begin{comment}
% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Distribution of research methods}
    \begin{tabular}{rr}
    \toprule
    Research method & Amount \\
    \midrule
    Multicase & 2 \\
    Experience report & 7 \\
    Singlecase & 19 \\
    Survey & 1 \\
    \bottomrule
    \end{tabular}%
  \label{tab:ResearchMethods}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Distribution of agile methods}
    \begin{tabular}{rr}
    \toprule
    Agile method & Amount \\
    \midrule
    Scrum & 15 \\
    XP    & 7 \\
    Lean  & 5 \\
    Other & 5 \\
    \bottomrule
    \end{tabular}%
  \label{tab:AgileMethods}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Distribution of domains}
    \begin{tabular}{rr}
    \toprule
    Domain & Amount \\
    \midrule
    Telecom & 10 \\
    Enterprise information system & 7 \\
    Web application & 4 \\
    Other & 11 \\
    \bottomrule
    \end{tabular}%
  \label{tab:Domains}%
\end{table}%
\end{comment}
%


%\caption{Overview of primary studies}
\begin{longtable}{p{0,5cm}p{1cm}p{2cm}p{2cm}p{2cm}p{3cm}}
\caption{Overview of primary studies}\\
\toprule
ID & Year & Resear. meth. & Agile method & Team size & Domain\\
\midrule\\\relax
[S1] & 2010 & Survey & NA & NA & NA\\ \relax
[S2] & 2005 & Experience r. & MSF v4.0\footnote{Microsoft Solutions Framework v4.0} & NA & NA\\\relax
[S3] & 2009 & Multicase &  NA/Scrum/ Scrum & 2-10/2-7/4-8
& ERP/Graphic design plug-in/Facility management\\\relax
[S4] & 2013 & Experience r. & Scrum & 25 teams & Software
for oil and gas industry\\\relax
[S5] & 2005 & Singlecase & XP & 15 & Enterprise information
system\\\relax
[S6] & 2002 & Experience r. & XP & 50 & Enterprise
resource solution for the leasing industry\\\relax
[S7] & 2011 & Survey & Scrum & 26 teams & Desktop and SaaS
products\\\relax
[S8] & 2010 & Experience r. & Scrum & 5-9 & NA\\\relax
[S9] & 2006 & Singlecase & XP & 15-20 & Broadband order
system\\\relax
[S10] & 2004 & Multicase & XP/Scrum & 4-18/6-9 &
b-2-b e-commerce solutions/Criminal justice system development\\\relax
[S11] & 2007 & Singlecase & Scrum & 500 & Security
services\\\relax
[S12] & 2010 & Singlecase & Scrum & NA & E-commerce\\\relax
[S13] & 2011 & Singlecase & LeanSD & 5$\pm$2 & Information and
communication software development\\\relax
[S14] & 2012 & Experience r. & XP & NA & Web application
development\\\relax
[S15] & 2006 & Multicase & NA/NA/ NA/NA & 2-5/12-15/1-10/6-7 &
NA/NA/NA/NA\\\relax
[S16] & 2012 & Singlecase & Scrum & 6-8 & Web page development\\\relax
[S17] & 2007 & Singlecase & NA & Comp. 160 devs & Various\\\relax
[S18] & 2010 & Singlecase & LeanSD & Dev site 600 &
Telecom\\\relax
[S19] & 2010 & Singlecase & XP & 6-7 & Telecom\\\relax
[S20] & 2010 &  Singlecase & NA & NA & Telecom\\\relax 
[S21] & 2011 & Singlecase & Scrum & Dev site 500 &
Telecom\\\relax
[S22] & 2012 & Singlecase & NA & NA & Telecom\\\relax
[S23] & 2011 & Experience r. & Scrum / Kanban & 9 and 6 & 
Casino games\\\relax
[S24] & 2011 & Singlecase & Kanban & 6-8 & Telecom
maintenance\\\relax
%\cite{Shen200725} & 2007 & Singlecase & ScrumXPMix & 4 & Independent software
% developer & Adaptability, innovation, productivity, ROI\\
[S25] & 2010 & Singlecase & NA & project size 100 &
Telecom\\\relax
[S26] & 2011 & Singlecase & NA & project size 200 & Telecom\\\relax
[S27] & 2006 & Singlecase & XP & 15 & Enterprise information
system\\\relax
[S28] & 2009 & Singlecase & XP & 15 & Enterprise information
system\\\relax
[S29] & 2006 &  Experience r. & NA & NA & Telecom\\\relax
[S30] & 2013 & Singlecase & NA & 5 & Space mission
control software\\
\bottomrule
%[S31] & 2006 & Experience r. & DSDM & NA & Library software\\\hline
\label{tab:overviewOfPdocs}
\end{longtable}


\begin{comment}
\begin{table*}
%\centering
\caption{Overview of primary studies}
\label{OverviewOfPdocs} 
\begin{tabular}{lllllp{3cm}p{8cm}} \hline
ID & Year & Resear. meth. & Agile method & Team size & Domain & Metrics\\
\hline 
\cite{S3} & 2009 & Multicase &  NA/Scrum/Scrum & 2-10/2-7/4-8
& ERP/Graphic design plug-in/Facility management & Team available hours, team
effective hours, critical defects sent by customer, open defects, test
failure rate\\
\cite{S4} & 2013 & Experience r. & Scrum & 25 teams & Software
for oil and gas industry & Technical debt in categories, build status,
technical debt in effort\\
\cite{S5} & 2005 & Singlecase & XP & 15 & Enterprise information
system & Burndown, check-ins per day, number of automated test steps, faults
per iteration\\
[S6] & 2002 & Experience r. & XP & 50 & Enterprise
resource solution for the leasing industry & Velocity\\
\cite{S7} & 2011 & Survey & Scrum & 26 teams & Desktop and SaaS
products & Burndown, story points, \# of open defects, defects found in
system test, defects deferred, net promoter score\\
\cite{S8} & 2010 & Experience r. & Scrum & 5-9 & NA & Story
points, task effort, velocity\\
\cite{S9} & 2006 & Singlecase & XPMix & 15-20 & Broadband order
system & Effort estimate, actual effort\\
\cite{S10} & 2004 & Multicase & XP/Scrum & 4-18/6-9 &
b-2-b e-commerce solutions/Criminal justice system development & \# of
defects/velocity\\
\cite{S11} & 2007 & Singlecase & ScrumMix & 500 & Security
services & Revenue per customer\\
\cite{S12} & 2010 & Singlecase & ScrumMix & NA & E-commerce & Task
expected start and end date, effort estimate, completed web pages, task done\\
\cite{S13} & 2011 & Singlecase &
LeanScrumFDD & 5$\pm$2 & Information and communication software development & Fix time of failed
build, story flow percentage, percentage of stories prepared for sprint,
time to establish project foundation, velocity of elaborating features,
velocity of implementing features\\
\cite{S14} & 2012 & Experience r. & XPMix & NA & Web application
development & Broken build, test coverage, test growth ratio, violations of
static code checks, \# of unit tests\\
\cite{S16} & 2012 & Singlecase & Scrum & 6-8 &Web page development
& Sprint velocity, release velocity, cost performance index, schedule
performance index, planned velocity\\
\cite{S17} & 2007 & Singlecase & Lean & Comp. 160 devs& Various
& Common tempo time, number of bounce backs, cycle time, work in progress\\
\cite{S18} & 2010 & Singlecase & ScrumXPMix & Dev site 600 &
Telecom & Lead time, processing time, queue time\\
\cite{S19} & 2010 & Singlecase & AgileMix & 6-7 & Telecom &
Change request per requirement, fault slips, implemented vs wasted
requirements, maintenance effort\\
\cite{S21} & 2011 & Singlecase & ScrumXPMix & Dev site 500 &
Telecom & Rate of requirements per phase, variance in handovers,
requirement's cost types\\
\cite{S22} & 2012 & Singlecase & LeanMix & NA & Telecom &
Cumulative flow of maintenance requests, lead time\\
\cite{S20} & 2010 &  Singlecase & LeanMix & NA & Telecom &
\# of faults, fault-slip-through, \# of requests from customer,
\# of requirements per phase\\
[S23] & 2011 & Experience r. & AgileMix & 9 and 6 & 
Casino games & Work in progress, average velocity, cycle time\\
\cite{S24} & 2011 & Singlecase & ScrumBan & 6-8 & Telecom
maintenance & Lead time, work in progress, \# of days in maintenance,
\# of days to overdue, reported hours on CSR\\
%\cite{Shen200725} & 2007 & Singlecase & ScrumXPMix & 4 & Independent software
% developer & Adaptability, innovation, productivity, ROI\\
\cite{S26} & 2011 & Singlecase & LeanMix & project size 200 & Telecom
& Throughput\\
\cite{S25} & 2010 & Singlecase & LeanMix & project size 100 &
Telecom & Defect trend indicator, \# of defects, predicted \# of
defects\\
\cite{S28} & 2009 & Singlecase & XP & 15 & Enterprise information
system & Burndown, check-ins per day, number of automated test steps\\
\cite{S27} & 2006 & Singlecase & XPMix & 15 & Enterprise information
system & Burndown, \# of new defects, number of written and passed tests,
task estimated vs actual time, time reported for overhead activities,
check-ins per day\\
\cite{S29} & 2006 &  Experience r. & ScrumXPMix & NA & Telecom
& Story estimate, story complete percentage\\
\cite{S30} & 2013 & Singlecase & ScrumMix & 5 & Space mission
control software & Progress as working code\\
\cite{S31} & 2006 & Experience r. & DSDM & NA & Library software &
Costs, schedule\\
\hline
\end{tabular}
\end{table*}
\end{comment}

\section{Quality evaluation of studies}

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Quality evaluation of primary studies}
    \begin{tabular}{p{0,3cm}p{0,5cm}p{0,4cm}p{0,4cm}p{0,5cm}p{0,6cm}p{0,5cm}p{0,6cm}p{0,5cm}p{0,4cm}p{0,6cm}p{0,5cm}p{0,5cm}p{0,5cm}}
\toprule
    Study & Res-earch & Aim   & Con-text & R.d-esign & Sam-pling & Ctrl. grp
    & Data coll. & Data anal. & Re-flex. & Find-ings & Val-ue & Tot-al &
    Rele-vancy
    \\
    \midrule
     {[}S1{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{2} \\
     {[}S2{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 2     & \multicolumn{1}{l}{2} \\
     {[}S3{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 3     & \multicolumn{1}{l}{2} \\
     {[}S4{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 2     & \multicolumn{1}{l}{3} \\
     {[}S5{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{3} \\
     {[}S6{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{2} \\
     {[}S7{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 5     & \multicolumn{1}{l}{2} \\
     {[}S8{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{3} \\
     {[}S9{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & 8     & \multicolumn{1}{l}{2} \\
     {[}S10{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 5     & \multicolumn{1}{l}{2} \\
     {[}S11{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{3} \\
     {[}S12{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 1     & \multicolumn{1}{l}{3} \\
     {[}S13{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{3} \\
     {[}S14{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 0     & \multicolumn{1}{l}{2} \\
     {[}S15{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{2} \\
     {[}S16{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 3     & \multicolumn{1}{l}{2} \\
     {[}S17{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 8     & \multicolumn{1}{l}{3} \\
     {[}S18{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{3} \\
     {[}S19{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{2} \\
     {[}S20{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & 4     & \multicolumn{1}{l}{2} \\
     {[}S21{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{2} \\
     {[}S22{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{2} \\
     {[}S23{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 4     & \multicolumn{1}{l}{2} \\
     {[}S24{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 4     & \multicolumn{1}{l}{2} \\
     {[}S25{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{3} \\
     {[}S26{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{2} \\
     {[}S27{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{2} \\
     {[}S28{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{3} \\
     {[}S29{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 1     & \multicolumn{1}{l}{3} \\
     {[}S30{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{2} \\
    \multicolumn{1}{l}{Total} & \multicolumn{1}{l}{16} & \multicolumn{1}{l}{15} & \multicolumn{1}{l}{20} & \multicolumn{1}{l}{15} & \multicolumn{1}{l}{18} & \multicolumn{1}{l}{6} & \multicolumn{1}{l}{14} & \multicolumn{1}{l}{13} & \multicolumn{1}{l}{7} & \multicolumn{1}{l}{21} & \multicolumn{1}{l}{24} &       &  \\
    \bottomrule
    \end{tabular}%
  \label{tab:qualityEvaluation}%
\end{table}%




The quality evaluation was done by the researcher after each study's data
extraction. Each category was evaluated with a scale from 0 to 1. The
evaluation form was adopted from \citepsrc{dyba_empirical_2008}. Detailed list
of quality assessment questions can be found in \cref{app:quality}.
Additionally, a relevancy factor was assigned for each study describing its
relevancy for this study. The scale for the relevancy can be found from
\cref{sec:dataExtraction}.

The perceived quality of the studies varied a lot (from 0 to 10). Even with
many low quality studies they were included since they still provided valuable
insight. For example, in some cases an experience report [S4] provided more
valuable data than a high scoring research paper [S25].

According to the evaluation control group and reflexivity had the lowest total
scores while value for research, context and findings scored the highest.

\section{Metrics}
\label{sec:metricsCategorization}

This section lists, categorizes and compares the found metrics from the SLR.
First, the found metrics are listed by primary study in \Cref{tab:Metrics}.
Second, the metrics are categorized according to the categorization by
\citetsrc{fenton1998software} in \Cref{tab:metricsCategorization}. Third, the
found metrics are compared to metrics suggested by agile methods in
\Cref{tab:comparisonOfMetrics}. Only metrics, which reason of use, effect of
use or importance was described were collected. A total of 102 metrics were
found from the primary studies.

\begin{longtable}{lp{12cm}} 
\caption{Metrics by primary studies}\\
\toprule\\
ID & Metrics\\
\midrule\\ \relax
[S1] & Business value delivered, customer satisfaction, defect count
after testing, number of test cases, running tested features\\ \relax
[S2] & Velocity\\ \relax
[S3] & Critical defects sent by customer, open defects, test
failure rate, test success rate, remaining task effort, team effectiveness\\
\relax
[S4] & Technical debt board, build status,
technical debt in effort\\\relax
[S5] & Burndown, check-ins per day, number of automated passing test steps,
faults per iteration\\\relax
[S6] & Velocity, story estimates\\\relax
[S7] & Burndown, story points, \# of open defects, \# of defects found
in system test, defects deferred, Net Promoter Score\\\relax
[S8] & Story points, task effort, velocity, operation's
velocity\\\relax 
[S9] & Effort estimate\\\relax
[S10] & \# of defects/velocity\\\relax
[S11] & Revenue per customer\\\relax
[S12] & Task's expected end date, effort estimate,
completed web pages, task done\\\relax
[S13] & Fix time of failed build, story flow percentage,
percentage of stories prepared for sprint, velocity of elaborating features,
velocity of implementing features\\\relax
[S14] & Build status, test coverage, test growth ratio, violations
of static code analysis, \# of unit tests\\ \relax
[S15] & Effort estimate / effort estimate / effort estimate / effort
estimate\\\relax
[S16] & Sprint burndown, release burndown, cost
performance index, schedule performance index, planned velocity\\\relax 
[S17] & Common tempo time, number of bounce backs, cycle
time, work in progress, customer satisfaction (Kano analysis), effort
estimate kits\\\relax
[S18] & Lead time, processing time, queue time\\\relax
[S19] & Change requests per requirement, fault slips,
implemented vs wasted requirements, maintenance effort, lead time\\\relax
[S20] & Number of requests from customers, inventory of requirements
over time\\\relax
[S21] & Rate of requirements per phase, variance in
handovers, requirement's cost types\\\relax
[S22] & \# of requirements per phase, lead time\\\relax
[S23] & Average velocity / work in progress, cycle time, pseudo
velocity\\\relax
[S24] & Lead time, work in progress%, \# of days in maintenance, \# of days to
% overdue, reported hours on CSR
\\\relax
[S25] & Defect trend indicator, \# of defects in backlog,
predicted \# of defects\\\relax
[S26] & Throughput, queue\\\relax
[S27] & Burndown, check-ins per day, number of automated passing test
steps, number of new and open defects\\\relax
[S28] & Burndown, number of automated passing test steps, check-ins per
day\\\relax 
[S29] & Story estimate, story complete percentage\\\relax
[S30] & Progress as working code\\
%[S31] & Costs, schedule\\\relax
\bottomrule
\label{tab:Metrics} 
\end{longtable}

%    \begin{tabular}{p{2cm}p{7cm}p{5,5cm}} fenton kategorisoinnin taulukonkoko

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Metric categorization based on \citetsrc{fenton1998software}}
 	\begin{tabular}{p{2,1cm}p{7cm}p{5,5cm}}
    \toprule
    Entities & Attributes &  \\
    \midrule
\textbf{Products} & \textbf{Internal} & \textbf{External} \\
    Products & Running tested features & Customer satisfaction x2, Net
    Promoter Score, number of requests from customers, progress as working code \\
    Test plans & Number of test cases &  \\
    Code  & Technical debt in categories, technical debt in effort, violations of static code analysis &  \\
    Builds & Build status x2, fix time of failed build &  \\
    Features & Remaining task effort, task's expected end date, task done, effort estimate x14, work in progress x3, number of days in maintenance, story complete percentage & Business value delivered, change requests per requirements \\
    Require-ments & Implemented vs wasted requirements, requirement's cost types, Percentage of stories prepared for sprint &  \\
    Defects &       & Defect trend indicator, predicted number of defects \\
    \textbf{Processes} &       &  \\
    Testing & Defect count after testing, critical defects sent by customer, open defects x5, test success rate, test failure rate, faults per iteration, defects found in system test, defects deferred, test coverage, test growth ratio & Number of bounce backs, fault slips \\
    Implementa-tion & Velocity x13, number of unit tests x4, completed web
    pages, cost performance index, schedule performance index, planned
    velocity, common tempo time, average velocity, check-ins per day x3 &
    Story flow percentage \\
    Requirements engineering & velocity of elaborating features &  \\
    Whole development cycle & cycle time x2, lead time x4, processing time, queue time, maintenance effort, number of work items per phase x2, variance in handovers, rate of requirements per phase, throughput, queue &  \\
    \textbf{Resources} &       &  \\
    Team  &  & Team effectiveness \\
    Customer & Revenue per customer &  \\
    \bottomrule
    \end{tabular}%
  \label{tab:metricsCategorization}%
\end{table}%

According to the categorization in \Cref{tab:metricsCategorization}, metrics
are used to measure products, test plans, code, builds, features, requirements
and defects. Most of the entities in Products class are measured internally,
except the products entity, which is measured mostly externally. Furthermore,
testing, implementation, requirements engineering and whole development cycle
are measured mostly internally in the Processes class. Only two metrics are
related to measuring Resources class.

% Table generated by Excel2LaTeX from sheet 'Taul2'
%\begin{comment}
\begin{longtable}{p{3cm}p{1,4cm}p{1,5cm}p{1cm}p{1,5cm}p{1,5cm}p{1,5cm}}
  \caption{Comparison of metrics found from agile literature compared to the
   metrics found from this study}\\
   
 \toprule
    Metrics suggested by literature & Method & Scrum & XP    & Kanban & LeanSD
    & NA
    \\
    \midrule
    Effort estimate & Scrum, XP, LeanSD & 2[S7], 1[S8], 2[S8], 3[S12] & 1[S9] &       &       & 1[S15], 2[S15], 3[S15], 4[S15], 6[S17], 1[S29] \\
    Velocity\footnote{Includes total work remaining from Scrum and effort left
    from Scrum and XP.} & Scrum, XP, LeanSD & 3[S8], 2[S10],  4[S8], 1[S16], 2[S16], 5[S16], 1[S23] & 1[S6], 1[S5], 1[S27], 1[S28] & 4[S23] & 5[S13] & 1[S2], 5[S3] \\
    Written and passed unit tests & XP, LeanSD &       & 3[S5], 3[S27], 2[S28], 5[S14] &       &       & 5[S1] \\
    Actual development time & XP    &       &       &       &       &  \\
    Load factor & XP    &       &       &       &       &  \\
    Work in progress & Kanban & 1[S21] &       & 2[S23], 2[S24] &       & 4[S17], 2[S20], 1[S22] \\
    Lead time & Kanban &       & 5[S19] & 1[S24] & 1[S18] & 2[S22] \\
    Due date performance & Kanban &       &       &       &       &  \\
    Throughput & Kanban &       &       &       &       & 1[S26] \\
    Issues and blocked work items & Kanban &       &       &       &       &  \\
    Flow effiency & Kanban &       &       &       &       &  \\
    Initial quality & Kanban, LeanSD & 3[S7], 4[S7] & 4[S5], 1[S10], 4[S27] &       &       & 3[S1],  2[S3], 2[S25] \\
    Failure load & Kanban &       &       &       &       & 2[S17] \\
    Cycle time & LeanSD &       &       & 3[S23] &       & 3[S17] \\
    Value Stream Maps (Work time, wait time) & LeanSD &       &       &       & 2[S18], 3[S18] &  \\
    Amount of written and passed acceptance tests per iteration & LeanSD &       &       &       &       & 4[S1], 3[S3], 4[S3] \\
    Not suggested &       & 1[S4], 2[S4],  3[S4], 4[S7],  5[S7],  1[S11], 1[S12],   3[S12], 4[S12], 3[S16], 4[S16], 2[S21], 3[S21] & 2[S5], 2[S27], 3[S28], 1[S14], 2[S14], 3[S14], 4[S14], 1[S19], 2[S19], 3[S19], 4[S19] &       & 1[S13],  2[S13],  3[S13], 4[S13] & 1[S1], 2[S1],  1[S3], 6[S3], 1[S17], 5[S17],  1[S20],  1[S25], 3[S25], 2[S26], 2[S29], 1[S30] \\
    \bottomrule
    \label{tab:comparisonOfMetrics}
    \end{longtable}%
%\end{comment}

Metrics found from the primary studies are compared to the metrics suggested
by agile literature in \Cref{tab:comparisonOfMetrics}. The metrics found
from literature are from Scrum \citepsrc{schwaber2011scrum}, XP
\citepsrc{beck2004extreme}, Kanban \citepsrc{anderson2010kanban} and LeanSD
\citepsrc{poppendieck2003lean}. The four rightmost columns in
\Cref{tab:comparisonOfMetrics} are describing the agile method used in the
primary studies. In some primary studies it was hard to identify a specific
agile method, thus 'NA' is used to describe those cases. The number before
primary study reference defines the index of the metric in \Cref{tab:Metrics}.

Actual development time (XP), load factor (XP), due date performance (Kanban),
issues and blocked work items (Kanban) and flow efficiency (Kanban) were not
described in primary studies. Many metrics (40 of 102, 39\%) found
from the primary studies were not suggested in agile literature.

\begin{comment}
Metrics could help in decreasing software project failures due to
prioritization and resource \& schedule issues in iteration tracking. Metrics
here are all reactive.

Iteration tracking metrics could help in monitoring issues that lead to
project failures. Metrics are mostly reactive.

Metrics that are used to motivate and improve people can be used to solve
issues related to values \& responsibilities and company policies that lead to
project failures. Metrics are both reactive and proactive.

Metrics that are used to point identify problems can be used to prevent Method
causes for failures. Metrics are almost all proactive.

Metrics that are used to improve or understand pre-release quality can be used
to prevent failures that are caused by value \& responsibility, task output
and existing product related issues. Metrics are mostly reactive.

Metrics that are used for post-release quality can be used to prevent failures
that are caused by customers' and users' opinions. Metrics are mostly
proactive.

In general, used metrics were more reactive than proactive.

In general, it seems that metrics are used the most to prevent project
failures that are caused by Methods, then by Environment and not so much about
People or Tasks.
\end{comment}

\section{Why are metrics used?}
\label{sec:WhyMetricsUsed}
The following sections describe the reasons for the use of metrics.
\Cref{tab:whyCategories} lists the categories for reasons of metric use by
sources. Some of the descriptions for the reasons of metric use might seem to
be incomplete. This is because the effects of metric use are described in the
following \cref{sec:effects}. Also, some reason categories might seem to be
describing more the effect than the reason, e.g.,
\nameref{sec:balanceWorkflow} in \cref{sec:balanceWorkflow}. However, the
reasons for metric use are described here for \nameref{sec:balanceWorkflow}
and then the actual actions and effects are described in the next
\cref{sec:effects}.

%    \begin{tabular}{rp{7cm}}
% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Reasons for the use of metrics by sources}
    \begin{tabular}{rp{7cm}}
    \toprule
    Categories & Sources \\
    \midrule
    Planning & [S6, S8, S9, S11, S12, S16, S17, S21, S23, S24, S25, S29] \\
    Progress tracking & [S2, S4, S5, S7, S8, S12, S13, S16, S17, S20, S21, S22, S23, S25, S27, S28] \\
    Understand and improve quality & [S3, S4, S5, S7, S14, S17, S19, S22, S28, S29] \\
    Identify problems & [S2, S13, S16, S18, S20, S21, S22, S25, S29] \\
    \bottomrule
    \end{tabular}%
  \label{tab:whyCategories}%
\end{table}%

\subsection{Planning}
\label{sec:planning}

Prioritization of tasks was one of the main activities metrics were used for.
At Objectnet, effort estimates were used to prioritize the features for next
release and as basis for resourcing [S9]. Teams at Adobe Systems used effort
estimates to prioritize activities based on relative value and relative effort
[S8]. At Verisign Managed Security Services, they used revenue per customer to
prioritize their backlog [S11]. At Timberline Inc, they used Kano analysis as
a 'voice of customer' so that prioritization decisions could be based on facts
instead of political power [S17]. Practitioners at Ericsson used cost types,
rate of requirements over phases and variance in handovers for short term
decisions related to requirements prioritization, staff allocation and
planning decisions [S21].

Metrics were used to estimate the size and amount of features that could be
taken under development. Velocity was used to improve effort estimates for
next planning session, which helped to understand how large the scope can be
for the next iteration [S16]. Scrum master and product owner at a Korean
e-commerce company used estimates to check if the planned scope would be
possible to complete during the next iteration [S12]. At WMS Gaming, they used
pseudo-velocity and average velocity to plan their releases [S23]. At Ericsson
product maintenance team, lead time was used to understand if all planned
corrections can be completed before release date [S24]. At Avaya
Communications, they used story estimates to understand the iteration where a
feature would be completed [S29]. 

Other planning uses for metrics were resourcing decisions and development
flexibility. At Timberline Inc, they broke down requirements into smaller
pieces that were estimated in effort to understand what skills are needed to complete
the work [S17]. At a Korean e-commerce company, they marked tasks done and
undone which made it possible to take undone tasks for the next iteration
[S12]. Also, they marked expected end date for tasks so the next person in
workflow could plan their own work as effectively as possible thus reducing
idle time. At ThoughtWorks, stories and their effort estimates were used as
the fundamental units of development for the iteration and as basis for
resourcing [S6]. At Ericsson, predicted number of defects was used to plan the
removal of defects [S25]. If the removal of defects would not be well planned
it could cause delays for the release and thus increase costs for the project.

\subsection{Progress tracking}
\label{sec:progressTracking}
Reasons for metrics usage in progress tracking are divided into project
progress, increase visibility, accomplishing project goals and balance workflow.

\subsubsection{Project progress}
\label{sec:projectProgress}

Metrics were used to monitor the progress of the project. Completed web
pages metric was used as a measure of progress at Korean e-commerce company
[S12].
Number of automated passing test steps was used as a measure of progress in terms of
completed work at Mamdas [S5]. At Timberline Inc, breaking down tasks to
'kits' between 2 to 5 days enabled progress monitoring [S17]. Set of metrics
(burndown, check-ins per day, number of automated passing test steps, number
of new and open defects) was developed to manage risks and provide timely
progress monitoring [S27]. Developers at Avaya Communications used story
percent complete metric to give assessment of progress [S29]. A team at NASA
Ames Reserch Center did not want to spend resources on estimating features and
instead they focused on designing and developing their software solution
[S30]. Every six weeks they demonstrated their progress to
customer with working code.

Metrics were also used to give higher level understanding of progress.
Release burndown showed project trends and could be used to predict completion
date [S16]. Also, release burndown could reflect addition or removal of
stories. At Ericsson, cost types, rate of requirements over phases and
variance in handovers were used to provide overview of progress [S21]. Metrics
(burndown, check-ins per day, number of automated passing test steps) were used to
communicate progress to upper management [S5] and ensure good progress to
external observers and ensure that key risks were under control [S5,S27,S28].

\subsubsection{Increase visibility}
\label{sec:increaseVisibility}
Metrics were used to simplify and understand complexity, and increase
visibility for all stakeholders. Cost types, rate of requirements over phases
and variance in handovers were used to increase the transparency of end-to-end
flow in a complex system [S21]. At Petrobras, technical debt board was used to
make technical debt issues visible and easier to manage [S4]. Metrics (burndown,
check-ins per day, number of automated passing test steps, number of open and
new defects) were used to replace individual perception with facts [S27].

Metrics were used to keep the team informed. At Ericsson, defect trend
indicator was used to monitor defect backlog and spread the information to
project members [S25]. At WMS Gaming, cycle time metric was used to let the
team track their performance [S23]. At Avaya Communications, story percent
complete metrics were generated automatically when tests were run and thus
kept everyone on the same page and eliminated schedule surprises [S29].
Additionally, the metric results were required to be reported periodically as
well. At Slovenian publishing company, release burndown made the correlation
clear between work remaining and team's progress in reducing it [S16].
 
\subsubsection{Accomplishing project goals}
\label{sec:accomplishProjectGoals}
Metrics were used to understand if project goals can be achieved. At
Timberline Inc, there was a need for simple indicator that would quickly tell
if project is under control [S17]. They used common tempo time to understand if
project was in target for delivery. At Microsoft Corporation, they monitored
work in progress to predict lead time which in turn would predict project
schedule [S2].
At Adobe Systems, sprint burndown was used to tell the team if they were on
track regarding the sprint commitments [S7]. Similarly at Mamdas, burndown was
used to see if the team could meet their goals, and if not, what could be done
[S5]. Burndown was also used to mitigate the risk where developers spend too
much time perfecting features over finishing all tasks of the iteration [S28].
Story flow percentage was used so that a developer could finish a story in a
steady flow [S13]. Story implementation flow metric describes how efficiently
a developer has been able to complete a story compared to the estimate.

\subsubsection{Balance workflow}
\label{sec:balanceWorkflow}
Metrics were used to balance workflow to prevent overloading people. At
Ericsson, inventory of requirements over time was used to identify large
handovers of requirements that would cause overloading situations to employees
[S20]. The aim was to have steady flow of requirements.
Similarly at Citrix Online, operations department was overloaded so they
decided to start evaluating incoming work with Ops story points to level the
workload [S8]. People should be respected by having balanced workload to avoid
overload situations [S22]. This could be achieved by measuring number of
requirements per phase which would notice peaks of workload. Timberline
Inc tried to pace work according to customer demand [S17]. However, too much
work was pushed to development, which caused many problems, including developers feeling
overworked. They started using common tempo time to make sure there would be
balance of workflow.
% Component level burndown was used to decide on resource mobility [S5].

At Ericsson, variance in handovers was used to guarantee that requirements
would flow evenly [S21]. Mamdas was measuring check-ins per day metric, which
measured how often code was committed to main trunk [S5]. The point was to
avoid people from committing only at the end of the iteration, and instead integrate
early and often. At WMS Gaming, they had problems with large tasks
blocking other work, so they set a rule that only certain size of tasks (8
story points) can be taken for development [S23].

\subsection{Understand and improve quality}
\label{sec:quality}
The following sections describe how metrics were used to understand the
quality of the product both before and after release. Also, the sections will
describe that metrics were used to improve the quality of the product and
ensure that the product will be tested thoroughly.

\subsubsection{Understand level of quality}
\label{sec:understandQuality}
Metrics were used to understand the level of quality after the release.
Number of change requests from customer was used as indicator customer
satisfaction [S19]. Maintenance effort was used as an indicator of overall
quality of the release product [S19]. Number of maintenance requests was used
as an indicator of built in quality [S22].

Metrics were also used to understand the level of quality before the release.
At Adobe Systems, they measured pre-release quality with Net Promoter Score
which was measured from pre-release customer surveys [S7]. Net Promoter Score
measures how willing a customer is to recommend the product to another
potential customer. They also measured defects found in system test which was
used to measure the quality of software delivered to system test process.
Additionally, they measured defects deferred, which was used to predict the
quality customers would experience. Defects deferred were defined as the
defects that are known but are not fixed for a release, usually due to time
constraints. At Mamdas, faults per iteration were used to measure the
product's quality [S5].

\subsubsection{Increase quality}
\label{sec:increaseQuality}
Metrics were used to increase the level of quality. Governance mechanisms,
which included a set of metrics (burndown, check-ins per day and number of
automated passing test steps), were used to increase product quality [S28].
At T-Systems International, they used a set of metrics(build status, number of
unit tests, test coverage, test growth ratio, violations of static code
analysis) to improve project's internal software quality [S14]. Build status
was measured to prevent defects reaching production environment.
Similarly, violations of static code analysis was used to prevent the
existence of critical violations. Furthermore, critical defects sent by
customers were tracked and fixed to prevent losing customers [S3].
Finally, technical debt board was used to reduce technical debt [S4].

\subsubsection{Ensure level of testing}
\label{sec:ensureTesting}
Metrics were used to make sure the product is tested well enough.
At T-Systems International, test coverage was used to evaluate how well the
code was tested [S14]. However, in Brown-field (legacy) projects it's better
to measure test-growth-ratio since there might not be many tests in the
existing code base. At Timberline Inc, work in progress was measured so it
could be minimized [S17]. Large amount of work in progress would contain many
unidentified defects which would need to be eventually discovered. At Mamdas,
using number of automated passing test steps dealt with decreasing the risk
that the product would be unthoroughly tested [S5]. Similarly, number of
automated passing test steps was used to make sure regression tests are ran
and passed every iteration. Finally, story percent complete metric supports
test driven development by requiring unit tests to be written for progress
tracking [S29].

\subsection{Identify problems}
\label{sec:identifyProblems}
Metrics were used to identify problems, bottlenecks and waste in the process.
Cumulative number of work items over time metric was used to identify
bottlenecks in the development process [S22]. Similarly, monitoring work in
progress was used to identify blocked work items and also the development
phase where the blockage occurred [S2]. Cost types, rate of requirements over phases
and variance in handovers were used for process improvement by spotting
bottlenecks and uneven requirement flows [S21]. A lot of requirements were
transferred to System Test phase, but only a small amount of requirements were
transferred to Ready for Release phase, see \Cref{fig:handOvers}.

\begin{figure}
	\includegraphics{images/HandoverPet2011}
	\centering
	\caption{Rate of requirements identified a bottleneck in System Test phase
	[S21]}
	\label{fig:handOvers}
\end{figure}

Metrics were able to identify waste, as in development phases were no value is
added, in software processes. At Ericsson, Value Stream Maps (VSM) were used
to spot waste in the development process [S18]. In another case at Ericsson,
long lead times led to identification of waste of waiting [S22].
Similarly, measuring story flow percentage allowed identification of waste related to context
shifts [S13].

Metrics were used to identify problems and find improvement opportunities.
Defect trend indicator was used to provide the project manager an ISO/IEC
15939:2007 compatible indicator for problems with the defect backlog [S25].
Basically, the indicator showed if the defect backlog will increase, stay the
same or decrease in the coming week. The project manager could then use the
info to take necessary actions to avoid possible problems. At the Slovenian
publishing company, schedule performance index and cost performance index were
used to monitor for deviances in the project's progress and providing early
signs if something goes wrong [S16]. Developers at Avaya had issues with the
80/20 rule, where the last 20\% of iteration takes the longest [S29]. With the
metrics that their tool T3 provided (e.g Story percent complete) they were
able to see the early symptoms of various problems that can cause delays, and
thus react early.

Metrics were also used to find improvement opportunities. Number of work items
per phase and lead time was used to spot instabilities in the process [S20].
They had set control limits to the metrics and if a measured value was outside
the control limits it meant that there was some kind of instability.

%Similarly, monitoring schedule and costs with a dashboard allowed to spot for
% improvement opportunities at OCLC [S31].

\section{What are the effects of using metrics?}
\label{sec:effects}
The following subsections describe the effects that the use of metrics had.
\Cref{tab:howCategories} lists the effect categories by sources.

Some of the categories for the reasons and the effects of metrics are close to
each other so sometimes it can be hard to understand why some categories are listed
under the reasons and not under the effects. For example
\cref{sec:balanceWorkflow} describes that the purpose for metrics was the
balancing of workflow and then the actual actions and effects are described
under \cref{sec:effects}.

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Effects for metric usage by sources}
    \begin{tabular}{rp{7cm}}
    \toprule
    Categories & Sources \\
    \midrule
    Planning actions & [S2, S11, S12, S23] \\
    Reactive actions & [S3, S5, S10, S14, S16, S17, S28] \\
    Motivate people & [S3, S4, S6, S13, S14, S17, S25] \\
    Create improvement ideas & [S8, S10, S13, S14, S17, S18, S20, S21, S22, S26, S27, S28] \\
    \bottomrule
    \end{tabular}%
  \label{tab:howCategories}%
\end{table}%


\subsection{Planning actions}
\label{sec:planningActions}
This section describes the planning actions that happened due to the
use of metrics.

Product owners at WMS Gaming used lead time to schedule high priority features
and planned demo dates with customers [S23]. Similarly, at Verisign
Managed Security Services they used revenue per customer metric to allow
higher valued features to be prioritized higher in the backlog [S11].

Velocity / 2 metric was used as scoping tool for a release [S23]. The team had
enough work not to sit idle, but there was still enough time fix high priority
defects. Similarly, effort estimates were used to scope the iteration and if
there were tasks that cannot be completed before release date then they were
excluded from the backlog [S12]. Furthermore, velocity was used to define a
minimum delivery level for the iteration where 'must have' requirements are
assigned, and a stretch goal where lower priority requirements are assigned
[S2].

Expected date of completion was used so that other team members could plan
their own work [S12]. For example, developer could know when she can start
implementation because designer had informed the expected date of completion
for the design.

\subsection{Reactive actions}
\label{sec:reactiveActions}
This section describes reactive actions that occurred due to the use of
metrics.

Metrics were used to cut down the scope of an iteration or to add more
resources if it seemed not all tasks could be completed with current pace.
When component level burndown was used to notice that a component was behind
schedule at Mamdas, resources were added and scope was reduced for the release
[S5]. Similarly, release burndown showed that work remaining was not
decreasing fast enough so the scope of the release was decreased [S16].
Furthermore, if common tempo time would indicate too much planned work, then
the tasks would be cut or more resources would be added [S17]. Similarly,
employees were trained with multiple skills, e.g., customer support did
testing and documentation engineers were taught how to input their material into the
system, so in case of imbalanced workload the work could be reorganized to
achieve more balanced workflow.  If team effectiveness is not high enough to
complete tasks, resources from other teams can be used [S3]. Other actions
that were suggested were reduction of tasks and working overtime.

Metrics were also used to react to quality information. At Timberline Inc,
monitoring cycle times revealed high time consumption on manual testing [S17].
The cause was an unmotivated person who was moved to writing automated test
scripts which he preferred. At Escrow.com, number of defects was used to
delay a release when too many defects were noticed in a QA cycle [S10].
At T-Systems International, quality manager interpreted results of static code
analysis from the build tool and he would then make plans for necessary
refactorings [S14]. When amount of written and passed unit tests was not
increasing an alarm was raised at Mamdas [S28]. The issue was discussed in a
reflection meeting where they understood that too much work was put to a
single tester writing the tests and once she was doing work for another
project no tests were written. The team then started to learn to write tests
themselves, and later a dedicated tester was assigned to write the
tests.

\subsection{Motivate people}
\label{sec:motivatePeople}
This section describes the motivating effects that the metrics had on people.

Metrics were used to motivate people to react faster to problems. Number of
defects was shown in monitors in hallways which motivated developers to fix
the defects [S3]. Similarly, total reported defects and test failure and
success rate was also shown throughout the organization which motivated people
to avoid problems and also fix the problems fast. At Systematic, they measured
fix time of broken build and showed the time next to the coffee machine. It
provoked discussion on the reasons for long fix times and eventually
developers fixed the builds faster [S13]. The metric was later declared
mandatory for all projects. Also, the reasons for long fix times were
investigated. Similarly at Petrobras, build status was visible in minutes
after commits, which helped to create a culture where developers react with
high priority to broken builds [S4]. This helped to keep the main branch to be
closer to deployable state at all times. Build status was used to motivate
people to fix the build as fast as possible [S4]. Moreover, Violations of
static code analysis caused developers to immediately fix the issue because
the violations could cause a broken build status [S14]. Additionally,
developers could get faster feedback on their work. Furthermore, developers
could have more confidence in performing major refactorings with the safety
net the violations of static code analysis metric provides.

Metrics were used to change people's behavior. At Petrobras, they used a
technical debt board to discuss technical debt issues in their projects, see
\Cref{fig:debtBoard}. In the meetings, team members agreed which technical
debt issues they would focus in solving until the next meeting [S4].
Additionally, team members sought help from the architecture team for reducing
technical debt, e.g., by implementing automatic deployment systems and
improving source code unit testability.
At Mamdas, measuring the number of automated passing test steps changed teams
behaviour to write more unit tests [S5]. Metrics were also used to prevent
harmful behaviour such as cherry picking features that are most interesting to
the team [S17]. Measuring work in progress (WIP) and setting WIP limits
prevented cherry picking by enforcing only two features at a time and thus
preventing them from working on lower priority but more interesting features.
Finally, at Ericsson defect trend indicator created crisis awareness and
motivated the developers to take actions to avoid possible problems [S25].

\begin{figure}
	\includegraphics{images/DebtBoard}
	\centering
	\caption{Technical debt board was used to discuss technical debt issues [S4]}
	\label{fig:debtBoard}
\end{figure}

There can also be negative effects in using metrics. Using velocity metric had
negative effects such as cutting corners in implementing features to maintain
velocity with the cost of quality [S6]. For example, the managers excused the
developers from writing tests and the testers cut on the thoroughness of the
testing in hopes to maintain the velocity.

\subsection{Create improvement ideas}
\label{sec:createImprovementIdeas}
This section describes improvement ideas that were created based on metrics.

At Ericsson, lead time, processing time and queue time metric were used to
identify waste of extra process (a requirement would wait for long time before
full specification) requirement specification [S18]. Solution idea was created
where a quick high level proposal would be sent to customer without the need
for in-depth specification. Customer could then use the high level proposal to
evaluate if they want to pursue that requirement further.
% Similarly, two requirement specification phases could be combined when
% another waste of extra process was identified in requirements specification
% phase.
Furthermore, long processing times for solution proposal phase indicated waste
of motion, where requirements are clarified between marketing and development
unit. The solution idea was to increase close collaboration between marketing
unit and development unit at least for the more complex requirements.
Finally, there was a waste of waiting in design phase which could be
improved by starting real work only when the purchase order is received, not
when requests are received.

Lead time, processing time and queue time metric were used to identify waste
of waiting in testing phases [S18]. The improvement suggestion was to provide
earlier beta version and making testing phases parallel. Many of the
improvement ideas came from meetings where Value Stream Maps (VSM) were
used as a base for discussion.

Cost types, rate of requirements over phases and variance in handovers were
used to identify bottlenecks at Ericsson [S21]. They noticed that focusing on
deadlines caused a lot of requirements to be transferred to system test phase
close to the deadline. The improvement suggestion was to focus more on
continuous delivery instead of focusing on market driven deadlines.
Furthermore, Kanban was suggested as a development method to accomplish the
continuous delivery capabilities. Similarly at another case at Ericsson,
throughput and queue time metrics were used to identify bottleneck in network
integration test phase which led to using other testing practices in future
projects [S26].

Rate of requirements over time was used to identify problems in the
development process [S20]. One improvement suggestion was to change from push
to pull-approach so that team can adjust the workload to enable a more
continuous delivery. Another improvement suggestion was to add intermediate
release versions so that integration and testing would happen more often and
problems could be identified earlier than close to the actual release. Similar
solution was applied at Timberline Inc. where requirements inventory was kept
low which meant that design, implementation and testing could start earlier
and problems in requirements would get caught sooner [S17].

Citrix Online started measuring velocity for their Operations department as
well [S8]. This led to development departments trying to decrease their
products' Operations story points to enable faster releases. The reduction in
story points was possible by creating hot deployment strategies and providing
better documentation.

Mamdas, an Israel Air force IT department, were using burndown to follow their
progress [28]. However, when they noticed that work remaining was not
decreasing according to remaining resources they had to make changes. In their
iteration summary meeting they decided to pursue senior engineers to help them
create optimal development environments and continuous build systems. Also,
they decided to evaluate customer requests in more detail to avoid over
polishing features. A team working on automating workflows in criminal justice
system noticed that their velocity estimations were inaccurate which led to
dividing work items into smaller pieces to improve the accuracy of the
estimates [S10]. The division of work items meant that the team needed to
perform more analysis of the features during planning.

When story implementation flow metric showed a drop and project managers
complained about clarifications about features from customer were late, a root
cause analysis meeting was held [S13]. Also, after starting to use the
implementation flow metric new policies were stated to keep the flow
high: percentage of stories ready for sprint must be 100\% and implementation
flow must be at least 60\%. Moreover, both of the metrics need to be reported
monthly. Root cause analysis was also conducted to decrease the amount of
bounce backs at Timberline Inc [S17].

The reasons for the values of metrics (burndown, check-ins per day, number of
automated passing test steps, number of new and open defects) were discussed
in iteration summary meeting because it can be hard to analyze metrics without
understanding the context [S27]. Similarly at Ericsson, number of work items
per phase was used to ask development unit about the values of the metric and
the development unit confirmed that people felt overloaded as the metric
suggested [S20]. Furthermore in another Ericsson case, if the values of number
of work items were outside the control limits one could discuss with the
developers about the workload [S22].

At Systematic, after analyzing long fix times for broken builds the team added
automatic static code analysis checks to code check-in to catch defects
earlier [S13]. Similarly at T-Systems International, Quality manager could
change coding style guide and code standards based on the results of
violations to static code analysis metric [S14].

\section{Important metrics}
\label{sec:importantMetrics}
The following subsections describe first which metrics were described as
important, and then second, which metrics were described the most times in the
primary studies.

\subsection{Important metrics in terms of statements}
 
This section describes metrics that were described as important. Metrics were
considered important if the author of the primary study, or case employees
praised the metric. Also, metrics were considered important if there were
signs of continuous use of the metric. Furthermore, if metrics had positive
correlation to project success in surveys, they were considered important.

\begin{comment}
Capacity as number of features developed in release was considered better than
measuring speed, since speed is generally thought as a attribute of humans.
Capacity on the otherhand measures the capabilities of an organization.
\end{comment}

Progress as working code was considered as one of the cornerstones of agile
[S30]. Story flow percentage and velocity of elaborating features were
considered as key metrics for monitoring projects [S13]. Also, a minimum of
60\% for story flow percentage was identified as a key limit. Similarly,
velocity for elaborating features should be as fast as velocity of
implementing features. Also, they said that using both aforementioned metrics
\emph{``drive behaviors to let teams go twice as fast as they could before''}. 

Story percent complete metric was considered valuable since it embraces test
driven development - no progress is made before a test is written [S29]. Also,
story percent complete metric was considered more accurate than previously
used metric - however that metric was not mentioned. Moreover, story percent
complete metric gave normalized measure of progress compared to developer
comments about progress. Additionally, story percent complete metric leveraged
existing unit testing framework and thus requires only minimal overhead to
track progress. Furthermore, team members seemed to be extremely happy about
using the story percent complete metric. Practitioners at Ericsson valued the
transparency and the overview of progress that the metrics (cost types, rate
of requirements over phases and variance in handovers) were able to provide to
the complex product development with parallel activities [S21].

Effort estimates were considered important in release planning especially in
terms of prioritization [S9]. According to a survey [S7], top performing teams
at Adobe Systems estimated backlog items with relative effort estimates.
Similarly, pseudo-velocity, which was used by a Kanban team, was considered
essential for release planning [S23]. Moreover, burndown was valuable in meeting sprint commitments [S7].
Furthermore, managers said burndown was important in making decisions and
managing multiple teams [S5]. However, developers did not consider burndown
important [S5]. According to a survey [S1], project success had
significant positive relationship with the following metrics: team velocity, business
value delivered, running tested features, defect count after testing and
number of test cases. However, there were no detailed descriptions of these
metrics.

At another case at Ericsson, Value Stream Maps (VSM) were used to visualize
problem areas, and facilitate discussion for possible improvements [S18].
Practitioners valued how the maps were easy to understand. Metrics that were
used to build VSM were lead time, processing time and queue time. Similarly,
technical debt board, which visualized the status of technical debt, was
considered important because it gave a high level understanding of the
problems [S4]. Moreover, the technical debt board was then used to plan
actions to remove the technical debt. Furthermore, it was proven to be useful
in their context.

Net Promoter Score, which measures the probability of a customer recommending
the product to another potential customer, was said to be \emph{``one of the
purest measures of success''} [S7]. Similarly, projects that were said to be
definitely successful 77\% measured customer satisfaction often or always.
Also, the more often customer satisfaction would be measured the more likely
it would be that the project would have good code quality and the project
would succeed. Similarly, defects deferred metric was seen as a good predictor
of post-release quality because it correlated with issues found by the customers
[S7].

Defect prediction metrics \emph{predicted number of defects in backlog} and
\emph{defect trend indicator} were seen important to decision making, and
their use continued after the pilot period [S26]. Key attributes of the
metrics were sufficient accuracy and ease of use.

The following metrics were considered very useful in agile context: number of
unit tests, test coverage, test-growth ratio and build status [S14]. The
benefit for the number of unit tests was not well described except that it provided
\emph{``first insights''}. Test coverage provided info on how well the code
was tested. Test-growth ratio was useful in projects where old codebase was
used as basis for new features. Finally, fixing broken builds prevented
defects reaching customers. 

\subsection{Important metrics based on the amount of evidence}
This section describes the metrics that were discussed the most in the
primary studies. Metrics that were only mentioned by name without any
reasons of use, effect of use, or importance were not taken into account.

Effort estimate (x14) and velocity (x13) were clearly the most discussed
metrics among the studies. Open defects metric (x5) was discussed
the third most. The number of unit tests (x4) and lead time (x4) were both
mentioned four times. Work in progress (x3) and check-ins per day (x3) were
both described three times.
Finally, metrics that were mentioned two times were cycle time (x2), build
status (x2), number of work items per phase (x2) and customer satisfaction
(x2).

\chapter{Discussion}
\label{sec:Discussion}

\section{Implications for practice}
To provide implications to practice the findings are mapped to the principles
of agile software development \citetsrc{beck2001agile} categorized by
\citetsrc{1579312}. For each paragraph the naming by Patel et al. is used and
references to the principles of agile software development is provided by
numbers.

Communication and Collaboration (4th and 6th agile principles
\citepsrc{beck2001agile}) was reflected by metrics
providing basis for discussion. Value Stream Maps and number of bounce backs
initiated root cause analysis meetings [S3,S17]. Moreover, metrics were
analysed in reflection meeting where problem and improvement were identified
[S28]. Furthermore, technical debt board provided visibility on technical debt
issues and it helped to create discussion to decrease technical debt [S4].

Team involvement (5,8) was reflected in metrics that motivated team to act and
improve, see \cref{sec:motivatePeople}. Also, to promote sustainable
development metrics were targeted to balance the flow of work, see
\cref{sec:balanceWorkflow}.

Reflection (12) was visible in metrics that were used to identify problems,
see \cref{sec:identifyProblems}. Furthermore, metrics helped to find
improvement ideas, see \cref{sec:createImprovementIdeas}.

Frequent delivery of working software (1,3,7) was directly identified in one
of the studies, where the team measured progress by demonstrating the product
to the customer [S30]. Additionally, there were cases where, e.g.,
completed web-pages [S12] were the primary progress measure. Also, many
metrics focused on progress tracking, see \cref{sec:projectProgress}), and
timely completion of project goals, see \cref{sec:accomplishProjectGoals}. However,
some other measures from \cref{sec:projectProgress} show that instead of
working code agile teams followed completed tasks and velocity metrics.
%\juha{haluaisin jotain tämänkaltaista keskustelua suorista laatumittareista,
% mutta voiko näin sanoa tämän tutkimuksen perusteella. Eetu, etenkin tuo
% viimeinen virke, onko linjassa sinun mielestäsi??}\eetu{Nyt muokattuna voi
% sanoa.}

An integral part of the concept of working software is measuring post-release
quality, see  \cref{sec:understandQuality}. This was measured by customer
satisfaction, feedback, and customer defect reports. It was also common to use
pre-release data to predict post-release quality. Agile developers tend to
measure the end product quality with customer based metrics instead of the
traditional quality models, such as ISO/IEC 25010 \citepsrc{10951538}.

Managing Changing Requirements (2) was seen in the metrics that support
prioritization of features, see
\cref{sec:planning} and \cref{sec:planningActions}. This allowed the rapid
development of features important for the customer's business at a given time.
Also, metrics like technical debt board provided better a better codebase for
further development.

 %Additionally, different metrics helped keeping
% the internal quality of the product high throughout the development which
% then provided safe development of modifications from new ideas, see
% \cref{sec:PreQuality}.

Design (9,10,11) was directly seen in focus to measuring technical debt and
using metrics to enforce writing tests before actual code, see
\cref{sec:ensureTesting}. Additionally, the status of the build was continuously
monitored, see \cref{sec:increaseQuality}. However, the use of velocity
metric had a negative effect on technical quality, see
\cref{sec:motivatePeople}.
% was seen from different perspectives: on one hand metrics focused on
% problem/waste identification, see \cref{sec:ProblemIdentification},
Many metrics focused on making sure that the right features were selected for
implementation, see \cref{sec:planning}, thus avoiding unnecessary
work. Moreover, metrics were used to identify waste (processes where no value
is added to the product), see \cref{sec:identifyProblems}.



%\eetu{Mun mielestä seuraavat mittarit ei oo niin ketteriä, mut en osaa oikeen
%perustella miksi tai sanoa mitä periaatteita vastaan ne olisi: maintenance
%effort, cost types, defect amounts(?), defects deferred, revenue per
%customer(?), time to establish project foundation, test coverage, test growth
%ratio, cost performance index, schedule performance index. Näitä ei kaikkia
%ole myöskään kuvattu resultseissa, paitsi taulukossa mainittu.}

There were also metrics, or their usage, which were not agile in nature. For example,
maintaining velocity by cutting corners in quality instead of dropping
features from that iteration [S6]. Also, adding people to a project to reach a
certain date [S5, S17] does not seem that agile compared to removing tasks.
Furthermore, adding people can have a negative impact to progress, considering
the lack of knowledge and training time required.
% Moreover, the use of dates to plan interdependent tasks is not agile in
% nature [S12].Instead, interdependencies should be visible inchoosing the
% tasks to appropriate iterations.
Moreover, the use of number of defects to delay a release [S10] is against
agile thinking as one should rather decrease the scope to avoid such a
situation. Furthermore, developers at Avaya used effort estimates to predict
the iteration where a feature would be completed [S29], contradicting the
idea of completing a feature within an iteration.

%While the flow metrics Ericsson have a good target of balancing workflow,
% they seem  (or at least they are presented) complicated to use---meaning that one
%might need considerable effort to generate and analyse the metrics, which
%doesn't fit to the light-weightness of agile.

%\juha{muotoilisin tämän hieman toisin}
% Contradictory to fifth principle, Talby et al [viite] enforce writing
% automated test cases as a measure of progress - so in a way they didn't
% trust the developers to write the test on their own? Similarly, [viite]
% measured the status of the build to make developers fix the build faster -
% again, not trusting them to do it on their own.
Some agile metrics that work well for an agile team, such as tracking progress
by automated tests [S28], or measuring the status of the build
[S14] can turn against the agile principles if used as an external
controlling mechanism. The fifth agile principle requires trust in the team,
but if the metrics are enforced outside of the team, e.g., from upper
management there is a risk that the metrics turn into control mechanisms and
the benefits for the team itself suffer.

Based on the results for important metrics in \cref{sec:importantMetrics},
some characteristics of important metrics can be highlighted. It seems
industrial agile teams appreciated metrics that were easy to use, highly
visual and they had the ability to facilitate discussion on problems and
possible improvements. Also, it seems that customer satisfaction metrics are
important for the success of a project. Furthermore, effort estimation and
velocity metrics were described as important and they were also identified
from many primary studies.

Found metrics could be also categorized based on the temporal use the metric.
There were some clearly reactive metrics that were used to provide immediate
improvement, e.g., motivate people to fix the build faster. On the other hand
there were metrics that were aimed more towards long-term process improvement,
e.g., many of the cases from Ericsson. However, many of these cases seemed to
be very pilot-type, as in some metric based process improvement approach was
used. In many cases these metrics seemed heavy and not likely to be applied
after the pilot.

Based on the results of this study an average profile for metric use of an
agile team could be described as follows. They estimate effort for tasks
and they follow their progress using velocity. They measure pre-release
quality with number of defects and they are interested in customer
satisfaction with some metric. Additionally, they have some customized
metrics that are needed to solve immediate problems, e.g., they measure
the amount of time it takes to fix a build.

\begin{comment}
Löytyi selkeästi reaktiivisia, välittömiä mittareita joilla haluttiin
välittömiä tuloksia, esim. buildin tila ja buildin tilaan liittyvä
korjausaika. Sitten taas toisaalta oli pilottimaisia tutkimuksia, jossa
esitettiin pilottimaisia mittareita, jotka yleensä olivat tai näyttivät olleen
vaikeampi implementoida niiden monimutkaisuuden vuoksi (Ericsson paperit),
jotka yleensä muistuttivat LeanSD:tä.
Jonkin verran näkyin myös focusta asiakastyytyväisyyden mittaamiseen eri
tavoilla[luettele mittareita]. Velocity ja effort estimaattimittareita oli
paljon. Et tietyllä tavalla semmonen keskimääräinen projekti estimoi taskeja,
seuraa burndownia, mittaa jollain tavalla sisäistä laatua esim bugimäärien
avulla ja on kiinnostunut asiakastyytyväisyydestä jollai mittarilla. Sen
lisäksi joitain mittareita luodaan tiettyyn tarpeeseen tai ongelmaan, esim.
building korjausaika lyhyemmäksi, tai tekninen velka pienemmäksi, tai
implementoinnin flow tietyksi. Tietyllä tavalla mittarien räätälöimistä, tätä
ilmeisesti myös suositeltu dymondin artikkelissa.

Future research, mittareiden räätälöinti, agiili asiakastyytyväisyys, ?

\end{comment}

%\subsection{Implications for research}
%It was interesting to notice that there wasn't many code metrics, only the
%ones mentioned in \cite{S14} even though we feel there are many
%% studies regarding the benefits of code metrics. Maybe there are some
% practical
%problems implementing and analysing the data from code metrics?

%How to measure unmeasured agile principles...

%Effort estimates are prerequisite for velocity metrics, and since velocity
%metrics were vastly identified in our study, e

%In general, we think there were many metrics that were targeted for the team
% - instead of high focus on managerial or upper management reporting metrics.
%Making metrics visible for the team enables them to independently act and
%improve without the need of rapid supervision and telling people what to do.

The metric categorization by \citetsrc{fenton1998software} in
\Cref{tab:metricsCategorization} focuses on identifying the target entities
for measurement. However, the categorization presented in
\cref{sec:WhyMetricsUsed} and in \cref{sec:effects} focus on the reasons and
effects of metric use. Maybe it is better to understand why metrics are used
instead of knowing the target of measurement. Furthermore, it is important to
understand the consequences of using a metric because the effects of using
the metric could be dysfunctional as well.

\section{Comparison to prior studies}
Only few papers have broadly studied the reasons for software metrics use in
the context of agile software development. \citetsrc{1667571} highlight
process improvement as one of the reasons for measurement in their agile
metrics paper. Also, they emphasize that creation of value should be the
primary measure of progress - which was also seen in this study. Moreover,
they differentiate between organizational long-term 'metrics' from short-term
context driven 'diagnostics'. Both types of metrics were also seen in this
study. However, a key metric for business value a team should define with
business unit, proposed by \citetsrc{1667571}, was not seen in this study.
Furthermore, \citetsrc{1667571} do not provide any specific agile metrics but
rather describes how agile metrics should be chosen and how they should be introduced
to the organization. Also, they provide a set of heuristics for agile metrics.

\citetsrc{Korhonen2009} found in her study that traditional defect
metrics could be reused in agile context - if modified. Defect metrics were
also used in many of the primary studies.

Kitchenham's mapping study \citepsrc{kitchenham_whats_2010} identified several
code metrics in academic literature. However, in this study almost no evidence
of code metric use in the industrial agile context was found. Maybe agile
practitioners consider code metrics self-evident and do not report them, or
maybe code metrics are not widely used by agile industrial teams.

%Maybe it is time to
%re-evaluate the need for code metrics research if industry doesn't seem to
% use them.

%The reasons for the lack of code metric usage in agile
%contexts should be studied to evaluate the necessity of code metric research
% - or how code metric research could be modified to support agile development

%The lone case in our study where code metrics were used, the code
%metric usage was abstracted to a build tool, which would just indicate an
%error or broken build \cite{S14}. Maybe the use of code metrics should
%be heavily implemented through automated tools that handle the collection and
%analysing of code metric data?

\section{Limitations}

The large shares of specific application domains in the primary documents are
a threat to external validity. Seven out of 30 studies were from enterprise
information systems domain and especially strong was also the share of ten
telecom industry studies out of which eight were from the same company,
Ericsson. Also, Israeli Air Force was the case organization in three studies.

The threats to reliability in this research include mainly issues related to
the reliability of primary study selection and data extraction. The main
threat to reliability was having a single researcher performing the study
selection and data extraction. It is possible that researcher bias could have
had an effect on the results. This threat was mitigated by analysing the
reliability of both study selection and data extraction as described in
\cref{sec:Method}. Also, another threat to reliability is the chosen research
method, SLR. There is a great deal of industrial metric use in agile teams
that is not reported in scientific literature. So choosing another research
method, e.g., a survey targeted to companies practicing agile methods
could have produced different results.

%Sometimes it was hard to understand which metrics an author was referring
%when a ``why'' was described. Moreover, we had to sometimes assume that when
%author describes the reasons for using a tool, he would be actually talking
%about the metrics the tool shows.

Due to iterative nature of the coding process, it was challenging to make sure
that all previously coded primary documents would get the same treatment,
whenever new codes were discovered. In addition, the researcher's coding
'sense' developed over time, so it is possible that data extraction accuracy
improved during the analysis. In order to mitigate these risks a pilot study
was conducted to improve the coding scheme, get familiar with the research
method and refine the method and tools.

%First author has positive mindset towards agile methods, as well as towards
%certain metrics over others.

Some data from low scoring papers, e.g [S3], are not explained very
detailed, which could have caused incorrect interpretations. Also, only the
researcher conducted the quality evaluation, which could have an impact on the
actual quality scores.

Deciding which agile method was used in the cases was difficult. But on the
other hand it is quite natural that cases use many aspects from multiple agile
methods.

\chapter{Conclusions}%\juha{Kirjoitin tänne hieman lisää...} 
\label{sec:Conclusions}

This study presents the results from a systematic literature review of 30
primary studies. According to the researcher's knowledge there are no previous
systematic reviews of metric use in the context of industrial agile software
development. This study categorizes metrics found from empirical agile studies
and compares the found metrics to the metrics suggested by agile literature.
Moreover, this study provides descriptions of why metrics are used to support
agile software development. Similarly, this study describes the effects
metrics have for agile software development. Furthermore, this study
identifies important metrics based on statements and the amount of evidence.
This study also analyzed how the presented metrics support the twelve
principles of Agile Manifesto \citepsrc{beck2001agile}.

The results indicate that the reasons for the use metrics are focused on the
following areas:
\nameref{sec:planning}, \nameref{sec:progressTracking},
\nameref{sec:quality}, and \nameref{sec:identifyProblems}. Similarly, the
effects of metric use are focused on the following areas:
\nameref{sec:planningActions}, \nameref{sec:reactiveActions},
\nameref{sec:motivatePeople} and \nameref{sec:createImprovementIdeas}.

This paper provides researchers and practitioners with an overview of
the metric use in agile context and documented reasonings and effects behind
the proposed metrics. This study can be used as a source of relevant studies
regarding researchers' interests and contexts.

Finally, this study identified few propositions for future research on
measuring in agile software development. First, in the academia lot of
emphasis has been given to code metrics yet this study found little evidence
of their use in agile context. Second, the applicable quality metrics for
agile development and the relationship of pre-release quality metrics and
post-release quality are important directions of future research. Third, this
study found that planning and tracking metrics for iteration were often used
indicating a need to focus future research efforts on these areas. Fourth, use
of metrics for motivating and enforcing process improvements can be an
interesting future research topic. Fifth, the use of customized metrics and
the use of customer satisfaction metrics in industrial agile context can be an
interesting future research topic. Finally, the dysfunctional use of metrics
and the negative motivation from metric use can be interesting future research
topics.
