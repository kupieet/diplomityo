\chapter{Introduction}
\label{chapter:intro}
 % Software engineering is at a crossroads as there are new leaner and more
% agile software development methods appearing next to the traditional
% software development methods.
% \mika{Kappaleen pointti: No literature reviews of actual metric use}
Software metrics have been studied for decades and several literature reviews
have been published.
Yet, the literature reviews have been written from an academic viewpoint. For
example, \citetsrc{catal2009systematic} review fault prediction metrics,
\citetsrc{purao2003product} review metrics for object oriented systems and
\citetsrc{kitchenham_whats_2010} performs a mapping of most cited software
metrics papers. According to the researcher's knowledge there are no
systematic literature reviews on the actual use of software metrics in the
industry.

%\mika{Kappaleen pointti: Agile on tärkeää eikä metriikkoja tutkittu}

% \juha{Yritin konkretisoida trad vs. agile kontrastia}
Agile software development is becoming increasingly popular in the software
industry. The agile approach seems to be contradicting with the traditional
metrics approaches. For example, the agile emphasizes working software over
measuring progress in terms of intermediate products or documentation, and
embracing the change invalidates the traditional approach of tracking progress
against pre-made plan.
However, at the same time agile software development highlights some measures
that should be used, e.g., burndown graphs and 100\% automated unit testing
coverage. However, metric research in the context of agile methods
remains scarce.

The goal of this study is to review the literature of actual use of software
metrics in the context of agile software development. This study lays out the
current state of metrics usage in industrial agile software development based
on literature. Moreover, the study uncovers the reasons for metric usage as
well as highlights the effects of metric use. Furthermore, this study is part
of an EU funded project U-QASAR\footnote{http://www.uqasar.eu/}. The
goal in the U-QASAR project is to develop an analysis tool for software
development projects with heavy focus on metrics. This study aims to support
the tool development by providing understanding on metric usage in industrial
agile software development, and by providing suggestions for metrics used in
the tool.
% Due to our research goal, we focus this paper on case studies and actual
% empirical findings excluding theoretical discussion and models lacking
% empirical validation.
This study covers the following research questions:

\begin{itemize}
  \item \textbf{Research Question 1:} \emph{What metrics are used in
  industrial agile software development?}
  \item \textbf{Research Question 1.1:} \emph{What are the entities measured
  in industrial agile software development?}
 \item \textbf{Research Question 1.2:} \emph{How do the metrics found in
 industrial agile software development compare to those suggested by agile literature?}
  \item \textbf{Research Question 2:} \emph{Why are metrics used in industrial
  agile software development?} 
  \item \textbf{Research Question 3:} \emph{What are the effects of  metric
  use in industrial agile software development?} 
  \item \textbf{Research Question 4:} \emph{What metrics are important in
  industrial agile software development?}
\end{itemize} 

%\section{Problem statement}

\section{Structure of the thesis}
\label{section:structure} 

This thesis is structured as follows.\Cref{sec:background} provides background
information about related concepts regarding this study.
\Cref{sec:Method} describes how the systematic literature review was conducted. \Cref{sec:Results}
reports the results from the study. \Cref{sec:Discussion} discusses about the
findings and how they map to agile principles. \Cref{sec:Conclusions}
concludes the study.


\chapter{Background}
\label{sec:background}
This chapter describes the key concepts used in this study.
First, \cref{sec:agileSoftwareDevelopment} describes agile software
development. Second, \cref{sec:softwareMetrics} describes the benefits of
software measurement. Third, \cref{sec:systematicLiteratureReview} describes
the used research method, Systematic Literature Review.

\section{Agile software development}
\label{sec:agileSoftwareDevelopment}
%\eetu{tässä voisi olla historianäkökulma, voipi mennä pitkäks eikä niin
%mielenkiintoinen ehkä tässä paperissa} 

Agile development methods have emerged to the software world ruled by
traditional heavyweight methods. In agile methods the focus is in lightweight
working practices, constant deliveries and customer collaboration over long
planning periods, heavy documentation and inflexible development phases.

Agile manifesto created by agile enthusiasts \citepsrc{beck2001agile} lists
four core agile values:

\begin{itemize}
  \item Individuals and interactions over processes and tools
  \item Working software over comprehensive documentation
  \item Customer collaboration over contract negotiation
  \item Responding to change over following a plan
\end{itemize}

Furthermore, popular agile development methods include Scrum
\citepsrc{schwaber2002agile}, Extreme Programming \citepsrc{beck2004extreme},
Lean Software Development \citepsrc{poppendieck2003lean} and Kanban
\citepsrc{anderson2010kanban}. Scrum \citepsrc{schwaber2002agile} got its name
from rugby, where the team groups up and plans its next move. Similarly in software development, the
scrum team has daily meetings where they see what has been done and what
should be done next. On high level, the development is constructed from
multiple subsequent sprints, where an increment of the software is developed.
Sprints are planned by selecting items from a backlog and estimating the
effort needed to complete each item selected for the sprint. During sprints
the team groups up every day for the daily meeting where the status of the
tasks is tracked. At the end of the sprint a sprint review is organized where
each of the tasks are reviewed. Learning is emphasized every sprint with a
sprint retrospective meeting where issues related to work practices are
discussed and improvements are suggested.

Extreme Programming (XP) \citepsrc{beck2004extreme} got its name from a set of
principles and practices that were emphasized to the extreme. For example, if
you think testing is a good practice then test all the time with automated
unit tests. Similarly, if you think code reviews are beneficial, then do
constant code reviews by programming in pairs. Additionally, XP aims at
embracing change. This is achieved in XP by continuously refactoring code
base, creating and maintaining a comprehensive unit test suite and designing
simple. Changes in business requirements can then be flexibly developed.
Communication is also very important in XP. Communication is handled with unit
tests, pair-programming and having a customer available to answer business
questions.

Lean Software Development (LeanSD) \citepsrc{poppendieck2003lean} contains
many attributes from Scrum and XP, but it also utilises principles from Lean
manufacturing, which Toyota used to build their competitive edge
\citesrc{womack2007machine}. The modified lean principles used in LeanSD are:

\begin{enumerate}
  \item Eliminate Waste
  \item Amplify Learning
  \item Decide as Late as Possible
  \item Deliver as Fast as Possible
  \item Empower the Team
  \item Build Integrity In
  \item See the Whole 
\end{enumerate}

The aforementioned agile methods are often well defined and they require their
practices to be used comprehensively. Kanban \citepsrc{anderson2010kanban} on
the other hand is more of an evolutionary process model that allows each
Kanban implementation to be different, suited for each context. Some
principles, however, are the same in Kanban. For example, Kanban systems are
always pull-systems. Work is pulled to development only when there is
capacity, compared to some other systems where work is pushed to development
according to demand. The pull system is enabled by limiting work in progress
(WIP). For example, if the WIP limit is set to two for development and there
is already two work items under development then no more work can be started before
either of them is completed.

Multiple different agile software development methods conforming to similar
principles comprise agile software development. The agile methods emphasise
short development cycles, frequent face-to-face communication and continuous
learning. In this Systematic Literature Review, only studies that are of agile
software development are considered.

\section{Software measurement}
\label{sec:softwareMetrics}

``\emph{If you can not measure it, you can not improve it\ldots}'' said Lord
Kelvin, a mathematical physicist and engineer. Similarly, according to
\citetsrc{fenton1998software}, we use metrics every day to understand,
control and improve what we do and how we do it. According to
\citetsrc{jones2008applied}, who is a founder of Software Productivity
Research with a knowledge base of thousands of software projects, the top
performing software companies such as IBM and Microsoft extensively use
metrics in their business, while the lower performing teams do not.
Furthermore, measurement is a key technology for successful software
development and maintenance. \citetsrc{pulford1995quantitative} describe the
following benefits of metric use:

\begin{itemize}
  \item Improved project planning and management of projects.
  \item Alignment of software development to business objectives.
  \item Cost-effective improvement programmes.
  \item Improved project communication.
\end{itemize}

Similarly, \citetsrc{grady1992practical} lists reasons for metric use for
practitioners and managers:

\begin{itemize}
  \item A basis for estimates.
  \item To track project progress.
  \item To determine (relative) complexity.
  \item To help us to understand when we have achieved a desired state of
  quality.
  \item To analyze defects,
  \item and to experimentally validate best practices.
  \item In short: they help us to make better decisions.
\end{itemize}
  
\begin{comment}
First software metrics
were used to measure programmer productivity in terms of lines of code in
circa 1950 \citepsrc{jones2008applied}.
%\citepsrc{wolverton1974cost}.

 Software metrics eventually started to measure
other aspects of software development, e.g., the software process.

%Tää ehkä liian yleistä ja vain mun mielipiteitä.
Metrics can be used to level discussion. Instead of people discussing on
different levels, some common measure can focus the conversation and bring
people to a common understanding, at least about the current situation.
\end{comment}

There are many benefits to software metrics, for example being able to improve
many aspects of software projects, and in general, make better decisions. This
study tries to collect software metrics and understand the reasons and effects
for metric use. Additionally, this study tries to distinguish important
metrics.
 
\section{Systematic Literature Review}
\label{sec:systematicLiteratureReview}

Systematic Literature Review (SLR) is a research method originated from the
field of medicine \citepsrc{kitchenham2004procedures}. There are three main
reasons for conducting an SLR \citepsrc{keele2007guidelines}. First, to
aggregate and synthesise existing knowledge regarding a research topic.
Second, identify gaps in earlier research. Third, provide background
information to start investigating a new research topic.
% This rigorous and audible evaluation method can facilitate theory
% development and point out gaps in research \citepsrc{webster2002analyzing}.
Moreover, SLR provides a repeatable research method, a method that when
applied properly, should provide the exactly same results irrespective of the
person who is performing it. %Steps conducted in SLR are always described in
% detail, so that anyone could, at least in theory, perform the study on her
% own.
Furthermore, the detailed documentation of steps in SLR allows in-depth
evaluation of the rigorousness of the conducted study. However, SLR requires
considerably more effort than traditional reviews. Usually, SLRs are conducted
by researchers and PhD students.

SLR is an audible and trustworthy research method to for multiple purposes. In
this study, SLR is used to understand a why metrics are used and what are the
effects of metric use in agile software development. Furthermore, SLR is used
to collect the used metrics, and distinguish important metrics.


%\section{Software measurement}
%Measurements for software industry started from a technical point of view.

%\section{Related work}


%\fixme{tää jäi vähän laihaks. pitäiskö siirtää kamat discussionin alle}


%\section{Evidence based software engineering}

%\subsection{Measurement}
%According to Fenton et al.\cite{fenton1998software} ``Measurement is the
%process by which numbers of symbols are assigned to attributes of entities in
%the real world in such way as to describe them according to clearly defined
%rules.''


%\section{Previous metric research}

%There are a few mapping studies on software metrics(tähän vois lisätä ne
% kitch whats 2010:ssä olevat muut mut en tiiä mitä lisäarvoa ne tois).
%\cite{kitchenham_whats_2010} says there is a large body of research related
% to software metrics. However, she highlights that all evidence should be
%critically appraised so that further studies can be based on good quality
%evidence. She also reminds researchers to understand the context where
% metrics are taken from - failure to understand context will probably not provide
%answers to industry-related questions. 

%--EBSE
%--Mittaamisen SR:t
%--Agile mittaamisen muut tutkimukset vai enemmänkin tutkimuksessa käytettyjen
%termien ja käsitteiden selittäminen


%\section{Aims and research questions} 
%The aim of this paper is to provide
% preliminary results from a systematic review (SR) on agile metrics.
%Moreover, we are interested on the industrial use of metrics in agile
% context.

\chapter{Review method}
\label{sec:Method}

Systematic literature review (SLR) was chosen as research method because the
study is more about trying to understand a problem instead of trying to find a
solution to it. Also, there was already existing literature that could be
synthesized. 

\section{Protocol development}
A guide for SLRs by \citetsrc{kitchenham2004procedures} was used as a
basis for developing the review protocol. Additionally, an SLR on agile
development \citepsrc{dyba_empirical_2008} and an SLR on SLR
\citepsrc{kitchenham2013systematic} were used to further understand the
challenges and opportunities of SLRs. The protocol was also iterated in weekly
meetings with the instructors, as well as in a pilot study.

%otherguidelines \cite{webster2002analyzing}, a lessons learned from SRs
% \cite{brereton2007lessons},
\section{Search and selection process}

The strategy for finding primary studies was following:

\begin{itemize}
  \item Stage 1: Automated search
  \item Stage 2: Selection based on title and abstract
  \item Stage 3: Selection based on full text. Conduct data
  extraction and quality assessment.
\end{itemize}

%\includegraphics{SelectionFunnel.jpg}

\begin{table}
\centering
\caption{Paper selection funnel}
\begin{tabular}{lcr}
\toprule
Stage & Amount of papers \\
\midrule
Stage 1 & 774 \\  
Stage 2 & 163 \\ 
Stage 3 & 30\\
\bottomrule
\label{SelectionFunnel}
\end{tabular}
\end{table}

% \subsubsection{Search strings}
\Cref{SelectionFunnel} shows the selection funnel in terms of the number of
papers after each stage. Scopus database \footnote{http://www.scopus.com} was
used to find the primary studies with automated search. Keywords include
popular agile development methods and synonyms for the word metric. The search
was improved incrementally in three phases because some key papers and XP
conferences were not found initially. The search strings, hits and dates can
be found from \cref{app:Strings}.

%\juha{Laittaisin myös inclusion ja exclusion kriteerit appendixiin ja
% jättäisin vain tämänkaltaisen lyhyen kuvauksen tänne} 
The selection of the primary studies was based on inclusion criteria:
\emph{papers that present empirical findings on the industrial use and
experiences of metrics in agile context.} The papers were excluded based on
multiple criteria, mainly due to not conforming to requirements regarding
empirical findings and agile and industrial context. Full criteria are listed
in \cref{app:Criteria}.

%\juha{Inclusion criteria ja exclusion criteria siirretty -> appendix}
%\subsubsection{Inclusion criteria} \juha{->APPENDIX}
%
%\begin{itemize}
%  \item Papers that present the use and experiences of metrics in an agile
%  industry setting.
%\end{itemize}
%
%\subsubsection{Exclusion criteria} \juha{->APPENDIX}
%
%\begin{itemize}
%  \item Papers that don't contain empirical data from industry cases.
%  \item Papers that are not in English.
%  \item Papers that don't have agile context. There is evidence of
%  clearly non-agile practices or there is no agile method named. For example,
%  paper mentions agile but case company has only three releases per year.
%  \item Paper is only about one agile practice, which is not related to
%  measuring.
%  \item Papers that don't seem to have any data about metric usage. Similarly,
%  if there are only a few descriptions of metrics but no other info regarding
%  reasons or usage.
%  \item Papers that have serious issues with grammar or vocabulary and
%  therefore it takes considerable effort to understand sentences.
%  \item Papers that refer to another paper where the actual case is discussed.
%  \item Papers that are in academic or semi-academic setting - customer or
%  part of workers are from industry. The reason for this is that it doesn't
%  fully represent industry setting as software development methods are likely
%  enforced by academia.
%  \item Papers where results cannot be separated by setting, for example
%  surveys where there is data both from academia and industry. Similar
%  exclusion if results cannot be separated by software development method. 
%  \item Papers where the setting is not clear. For example only mention of
%  context is ``100 junior developers''.
%  \item Papers that are full conference proceedings. Individual papers should
%  be already listed separately.
%  \item Papers where the measurements are only used for the research. For
%  example author measures which agile practices correlate with success.
%  \item Papers that don't even show measurement usage in a pilot setting. For
%  example method or metric is used against static industrial data set.
%  %\item Papers that are about the same case, from the same author and same
%  %research focus, basically the paper format has just changed slightly.
%\end{itemize}


%\subsubsection{Stage 1 - Automatic search}

In stage 1, Scopus was used as the only search engine as it contained the most
relevant databases IEEE and ACM. Also, it was able to find Agile and XP conference
papers. Only XP Conference 2013 was searched manually because it could not be
found through Scopus.

%\subsubsection{Pilot}
%\label{pilot}
%We conducted a pilot study in order to refine the aim of the research and get
%familiar with the research method. Moreover, it was possible to modify the
%method and tools.

%15 papers were selected for the pilot; 5 by relevance, 5 by number of
%citations, and 5 by random selection.
% \begin{itemize} \item 5 by top relevance \item 5 by top citations \item 5 by
% random \end{itemize}
%\juha{tämä mahdollista poistaa (ainakin omana alilukuna) jos tila ei
% riitä}The pilot resulted in changing citation manager tool from Zotero to Jabref.
%Also, selection by title and selection by abstract steps were joined
% together.
%The quality assessment checklist was decided based on the pilot results.

%\subsubsection{Stage 2 - Selection by title and abstract}

In stage 2, papers were included and excluded by the researcher based on their
title and abstract. As the quality of abstracts can be poor in computer
science \citepsrc{kitchenham2004procedures}, full texts were also skimmed
through in case of unclear abstracts.
% - especially introduction, case description and conclusions were checked
% briefly.
Unclear cases were discussed with the instructors in weekly meetings and an
exclusion rule was documented if necessary. The validity of the selection process was analysed by performing the selection
for a random sample of 26 papers also by the second instructor. The level of
agreement was 'substantial' with Kappa 0.67
\citepsrc{landis_measurement_1977}.

%\subsubsection{Stage 3 - Selection by full text}

Stage 3 included multiple activities in one workflow. Selection by full text
was done, data was coded and quality assessment was done. Once again, if there
were unclear papers, they were discussed in the meetings. Also, selection of 7
papers was conducted by the second instructor with an 'almost perfect'
agreement, Kappa 1.0 \citepsrc{landis_measurement_1977}.

\section{Data extraction}
\label{sec:dataExtraction}

Integrated coding was selected for data extraction strategy
\citepsrc{6092576}. Integrated coding includes having a start list of codes as
well as creating new codes if necessary (ground-up). It provided focus to
research questions but flexibility regarding findings.
Deductive coding would have been too restraining and inductive coding might
have caused too much bias. Integrated coding made it possible to create a
sample list of code categories:

\begin{itemize}
  \item Why is the metric used?
  \item What is the effect of metric use?
  \item Metric
  \item Importance related to the metric
  \item Context
\end{itemize}

The coding started with the researcher reading the full text and marking
interesting quotes with a temporary code. After, reading the full text the
researcher checked each quote and coded again with an appropriate code based
on the built understanding. In weekly meetings with the instructors, a rule
set for collecting metrics was slowly built:

\begin{itemize}
  \item Collect metric if team or company uses it.
  \item Collect metric only if something is said about why it is used, what
  effects it causes or if it is described as important.
  \item Do not collect metrics that are only used for the comparison and
  selection of development methods. 
  \item Do not collect metrics that are primarily used to compare teams.
  (There were cases where a researcher or management uses a metric to compare teams. We wanted to find metrics a team could use.) 
\end{itemize}

Atlas.ti Visual QDA (Qualitative Data Analysis), version 7.1.x was used to
collect and synthesize the qualitative data. Amount of found quotes per code
can be seen in \Cref{tab:quotes}. To evaluate the repeatability of finding the
same metrics, second instructor coded metrics from three primary studies.
Capture-recapture method \citepsrc{seber2002estimation} was then used which
showed that 90\% of metrics were found.

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Amount of found quotes}
    \begin{tabular}{rr}
    \toprule
    Code  & Amount of quotations \\
    \midrule
    Why is the metric used? & 151 \\
    What is the effect of metric use? & 61 \\
    Metrics & 102 \\
    Importance related to the metric & 45 \\
    Context & 158 \\
    \bottomrule
    \end{tabular}%
  \label{tab:quotes}%
\end{table}%


A quality assessment form adopted from \citepsrc{dyba_empirical_2008} was used
to evaluate the quality of each primary study. Detailed list of quality
assessment questions can be found in \cref{app:quality}. Additionally, a
relevancy factor was added to the same assessment to describe how useful a
primary study was for this study. The scale for the relevancy factor is:

\begin{itemize}
  \item 0 = does not contain any information regarding metrics and should be
  already excluded
  \item 1 = only descriptions of metrics with no additional info
  \item 2 = some useful information related to metrics
  \item 3 = a good amount of relevant information regarding metrics and metric
  use
\end{itemize}

\section{Data synthesis}
Data synthesis followed the steps recommended by \citetsrc{6092576}.
Process started by going through all quotes within one code and giving each
quote a more descriptive code describing the quote in high level. Then the
descriptive codes were organized in groups based on their similarity, see
e.g., \Cref{fig:whyMapExample} These groups were then given high level codes
which are seen as categories in \Cref{tab:whyCategories} and
\Cref{tab:howCategories}.



Metrics were grouped based on similarity to enable categorization
(\Cref{tab:metricsCategorization} and \Cref{tab:importantMetrics}) and
comparison (\Cref{tab:comparisonOfMetrics}). For example, burndown is grouped
under velocity.


\begin{figure}
	\includegraphics{images/WhyMapExample.jpg}
	\centering
	\caption{Reason for metric use category named 'Progress tracking'
	was formed by organizing descriptive codes in to a group based on their similarity}
	\label{fig:whyMapExample}
\end{figure}


\chapter{Results} % - Why and how are metrics used
\label{sec:Results}

This chapter presents the results from the systematic literature review and
provides the answers to the research questions. \Cref{sec:overviewOfStudies}
describes the overview of studies. \Cref{sec:qualityEvaluationOfStudies}
describes the results from the quality evaluation of the primary studies.
\Cref{sec:metricsCategorization} presents the found metrics (RQ1),
categorizes them based on the entity that is measured (RQ1.1) and compares
them to metrics suggested by agile literature (RQ1.2).
\Cref{sec:WhyMetricsUsed} describes the reasons for using metrics (RQ2).
\Cref{sec:effects} describes the effects of metric use (RQ3).
\Cref{sec:importantMetrics} describes important metrics (RQ4) by statements
from the primary studies as well as by amount of evidence from primary studies.

\section{Overview of studies}
\label{sec:overviewOfStudies}

This section gives an overview of the primary studies.
\Cref{tab:PublicationDistribution} shows the distribution of primary studies
by publication channel. \Cref{tab:overviewOfPdocs} lists the primary studies
by context factors. 

The study identified 30 primary studies. Primary studies were published in 12
different journals, conferences or workshops, see
\Cref{tab:PublicationDistribution}. Large share of primary studies (43\%) were
published in Agile Conference. Rest of the studies were published in a wide
range of journals, conferences and workshops.

\begin{table}
\centering
\caption{Publication distribution of primary studies}
\label{tab:PublicationDistribution}
\begin{tabular}{llll}
\toprule Publication channel & Type & \# & \%\\
\midrule
	Agile Conference & Conference & 9    & 43 \\
    HICCS & Conference & 3     & 14 \\
    ICSE SDG & Workshop & 2     & 10 \\
    XP Conference & Conference & 2     & 10 \\
    Agile Development Conference & Conference & 1     & 5 \\
    APSEC & Conference & 1     & 5 \\
    ASWEC & Conference & 1     & 5 \\
    ECIS  & Conference & 1     & 5 \\
    Elektronika ir Elektrotechnika & Journal & 1     & 5 \\
    Empirical Software Engineering & Journal & 1     & 5 \\
    EUROMICRO & Conference & 1     & 5 \\
    ICSE  & Conference & 1     & 5 \\
    IST   & Journal & 1     & 5 \\
    IJPQM & Journal & 1     & 5 \\
    JSS   & Journal & 1     & 5 \\
    PROFES & Conference & 1     & 5 \\
    Software - Prac. and Exp. & Journal & 1     & 5 \\
    WETSoM & Workshop & 1     & 5 \\
\bottomrule
\end{tabular}
\end{table}

Primary studies and their context information can be seen in
\Cref{tab:overviewOfPdocs}. The earliest study is from 2002 and rest of the
studies are quite evenly distributed from 2002 to 2013.  Single-case was
the most used research method with 18 studies, then experience report with 8
studies, multi-case with 3 studies and finally survey with 2 studies. 

Agile method for the studies was identified based on the assessment of the
researcher. A specific method was chosen if it seemed to be a primary method
in the case. If it was not clear what agile method was used then 'NA' was set
as agile method. Based on the results, Scrum was the most used agile method
(35\%) in primary studies. XP was the second most used agile method (20\%),
while LeanSD and Kanban were used in 5\% of the cases. In 33\% of the cases
the used agile method was not clear.

Telecom was the most represented domain with 10 cases, enterprise information
systems was the second with 7 cases and web applications with 4 cases. There
were 15 cases that were other domains or cases without domain information.


\begin{comment}
% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Distribution of research methods}
    \begin{tabular}{rr}
    \toprule
    Research method & Amount \\
    \midrule
    Multicase & 2 \\
    Experience report & 7 \\
    Singlecase & 19 \\
    Survey & 1 \\
    \bottomrule
    \end{tabular}%
  \label{tab:ResearchMethods}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Distribution of agile methods}
    \begin{tabular}{rr}
    \toprule
    Agile method & Amount \\
    \midrule
    Scrum & 15 \\
    XP    & 7 \\
    Lean  & 5 \\
    Other & 5 \\
    \bottomrule
    \end{tabular}%
  \label{tab:AgileMethods}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Distribution of domains}
    \begin{tabular}{rr}
    \toprule
    Domain & Amount \\
    \midrule
    Telecom & 10 \\
    Enterprise information system & 7 \\
    Web application & 4 \\
    Other & 11 \\
    \bottomrule
    \end{tabular}%
  \label{tab:Domains}%
\end{table}%
\end{comment}
%


%\caption{Overview of primary studies}
\begin{longtable}{p{0,5cm}p{1cm}p{2,4cm}p{2cm}p{2cm}p{3cm}}
\caption{Overview of primary studies}\\
\toprule
ID & Year & Resear. meth. & Agile method & Team size & Domain\\
\midrule\\\relax
[S1] & 2010 & Survey & NA & NA & NA\\ \relax
[S2] & 2005 & Experience r. & MSF v4.0\footnote{Microsoft Solutions Framework v4.0} & NA & NA\\\relax
[S3] & 2009 & Multi-case &  NA/Scrum/ Scrum & 2-10/2-7/4-8
& ERP/Graphic design plug-in/Facility management\\\relax
[S4] & 2013 & Experience r. & Scrum & 25 teams & Software
for oil and gas industry\\\relax
[S5] & 2005 & Single-case & XP & 15 & Enterprise information
system\\\relax
[S6] & 2002 & Experience r. & XP & 50 & Enterprise
resource solution for the leasing industry\\\relax
[S7] & 2011 & Survey & Scrum & 26 teams & Desktop and SaaS
products\\\relax
[S8] & 2010 & Experience r. & Scrum & 5-9 & NA\\\relax
[S9] & 2006 & Single-case & XP & 15-20 & Broadband order
system\\\relax
[S10] & 2004 & Multi-case & XP/Scrum & 4-18/6-9 &
b-2-b e-commerce solutions/Criminal justice system development\\\relax
[S11] & 2007 & Single-case & Scrum & 500 & Security
services\\\relax
[S12] & 2010 & Single-case & Scrum & NA & E-commerce\\\relax
[S13] & 2011 & Single-case & LeanSD & 5$\pm$2 & Information and
communication software development\\\relax
[S14] & 2012 & Experience r. & XP & NA & Web application
development\\\relax
[S15] & 2006 & Multi-case & NA/NA/ NA/NA & 2-5/12-15/1-10/6-7 &
NA/NA/NA/NA\\\relax
[S16] & 2012 & Single-case & Scrum & 6-8 & Web page development\\\relax
[S17] & 2007 & Single-case & NA & Comp. 160 devs & Various\\\relax
[S18] & 2010 & Single-case & LeanSD & Dev site 600 &
Telecom\\\relax
[S19] & 2010 & Single-case & XP & 6-7 & Telecom\\\relax
[S20] & 2010 &  Single-case & NA & NA & Telecom\\\relax 
[S21] & 2011 & Single-case & Scrum & Dev site 500 &
Telecom\\\relax
[S22] & 2012 & Single-case & NA & NA & Telecom\\\relax
[S23] & 2011 & Experience r. & Scrum / Kanban & 9 and 6 & 
Casino games\\\relax
[S24] & 2011 & Single-case & Kanban & 6-8 & Telecom
maintenance\\\relax
%\cite{Shen200725} & 2007 & Singlecase & ScrumXPMix & 4 & Independent software
% developer & Adaptability, innovation, productivity, ROI\\
[S25] & 2010 & Single-case & NA & project size 100 &
Telecom\\\relax
[S26] & 2011 & Single-case & NA & project size 200 & Telecom\\\relax
[S27] & 2006 & Single-case & XP & 15 & Enterprise information
system\\\relax
[S28] & 2009 & Single-case & XP & 15 & Enterprise information
system\\\relax
[S29] & 2006 &  Experience r. & NA & NA & Telecom\\\relax
[S30] & 2013 & Single-case & NA & 5 & Space mission
control software\\
\bottomrule
%[S31] & 2006 & Experience r. & DSDM & NA & Library software\\\hline
\label{tab:overviewOfPdocs}
\end{longtable}


\begin{comment}
\begin{table*}
%\centering
\caption{Overview of primary studies}
\label{OverviewOfPdocs} 
\begin{tabular}{lllllp{3cm}p{8cm}} \hline
ID & Year & Resear. meth. & Agile method & Team size & Domain & Metrics\\
\hline 
\cite{S3} & 2009 & Multicase &  NA/Scrum/Scrum & 2-10/2-7/4-8
& ERP/Graphic design plug-in/Facility management & Team available hours, team
effective hours, critical defects sent by customer, open defects, test
failure rate\\
\cite{S4} & 2013 & Experience r. & Scrum & 25 teams & Software
for oil and gas industry & Technical debt in categories, build status,
technical debt in effort\\
\cite{S5} & 2005 & Singlecase & XP & 15 & Enterprise information
system & Burndown, check-ins per day, number of automated test steps, faults
per iteration\\
[S6] & 2002 & Experience r. & XP & 50 & Enterprise
resource solution for the leasing industry & Velocity\\
\cite{S7} & 2011 & Survey & Scrum & 26 teams & Desktop and SaaS
products & Burndown, story points, \# of open defects, defects found in
system test, defects deferred, net promoter score\\
\cite{S8} & 2010 & Experience r. & Scrum & 5-9 & NA & Story
points, task effort, velocity\\
\cite{S9} & 2006 & Singlecase & XPMix & 15-20 & Broadband order
system & Effort estimate, actual effort\\
\cite{S10} & 2004 & Multicase & XP/Scrum & 4-18/6-9 &
b-2-b e-commerce solutions/Criminal justice system development & \# of
defects/velocity\\
\cite{S11} & 2007 & Singlecase & ScrumMix & 500 & Security
services & Revenue per customer\\
\cite{S12} & 2010 & Singlecase & ScrumMix & NA & E-commerce & Task
expected start and end date, effort estimate, completed web pages, task done\\
\cite{S13} & 2011 & Singlecase &
LeanScrumFDD & 5$\pm$2 & Information and communication software development & Fix time of failed
build, story flow percentage, percentage of stories prepared for sprint,
time to establish project foundation, velocity of elaborating features,
velocity of implementing features\\
\cite{S14} & 2012 & Experience r. & XPMix & NA & Web application
development & Broken build, test coverage, test growth ratio, violations of
static code checks, \# of unit tests\\
\cite{S16} & 2012 & Singlecase & Scrum & 6-8 &Web page development
& Sprint velocity, release velocity, cost performance index, schedule
performance index, planned velocity\\
\cite{S17} & 2007 & Singlecase & Lean & Comp. 160 devs& Various
& Common tempo time, number of bounce backs, cycle time, work in progress\\
\cite{S18} & 2010 & Singlecase & ScrumXPMix & Dev site 600 &
Telecom & Lead time, processing time, queue time\\
\cite{S19} & 2010 & Singlecase & AgileMix & 6-7 & Telecom &
Change request per requirement, fault slips, implemented vs wasted
requirements, maintenance effort\\
\cite{S21} & 2011 & Singlecase & ScrumXPMix & Dev site 500 &
Telecom & Rate of requirements per phase, variance in handovers,
requirement's cost types\\
\cite{S22} & 2012 & Singlecase & LeanMix & NA & Telecom &
Cumulative flow of maintenance requests, lead time\\
\cite{S20} & 2010 &  Singlecase & LeanMix & NA & Telecom &
\# of faults, fault-slip-through, \# of requests from customer,
\# of requirements per phase\\
[S23] & 2011 & Experience r. & AgileMix & 9 and 6 & 
Casino games & Work in progress, average velocity, cycle time\\
\cite{S24} & 2011 & Singlecase & ScrumBan & 6-8 & Telecom
maintenance & Lead time, work in progress, \# of days in maintenance,
\# of days to overdue, reported hours on CSR\\
%\cite{Shen200725} & 2007 & Singlecase & ScrumXPMix & 4 & Independent software
% developer & Adaptability, innovation, productivity, ROI\\
\cite{S26} & 2011 & Singlecase & LeanMix & project size 200 & Telecom
& Throughput\\
\cite{S25} & 2010 & Singlecase & LeanMix & project size 100 &
Telecom & Defect trend indicator, \# of defects, predicted \# of
defects\\
\cite{S28} & 2009 & Singlecase & XP & 15 & Enterprise information
system & Burndown, check-ins per day, number of automated test steps\\
\cite{S27} & 2006 & Singlecase & XPMix & 15 & Enterprise information
system & Burndown, \# of new defects, number of written and passed tests,
task estimated vs actual time, time reported for overhead activities,
check-ins per day\\
\cite{S29} & 2006 &  Experience r. & ScrumXPMix & NA & Telecom
& Story estimate, story complete percentage\\
\cite{S30} & 2013 & Singlecase & ScrumMix & 5 & Space mission
control software & Progress as working code\\
\cite{S31} & 2006 & Experience r. & DSDM & NA & Library software &
Costs, schedule\\
\hline
\end{tabular}
\end{table*}
\end{comment}

\section{Quality evaluation of the primary studies}
\label{sec:qualityEvaluationOfStudies}

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Quality evaluation of primary studies}
    \begin{tabular}{p{0,3cm}p{0,5cm}p{0,4cm}p{0,4cm}p{0,5cm}p{0,6cm}p{0,5cm}p{0,6cm}p{0,5cm}p{0,4cm}p{0,6cm}p{0,5cm}p{0,5cm}p{0,5cm}}
\toprule
    Study & Res-earch & Aim   & Con-text & R.d-esign & Sam-pling & Ctrl. grp
    & Data coll. & Data anal. & Re-flex. & Find-ings & Val-ue & Tot-al &
    Rele-vancy
    \\
    \midrule
     {[}S1{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{2} \\
     {[}S2{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 2     & \multicolumn{1}{l}{2} \\
     {[}S3{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 3     & \multicolumn{1}{l}{2} \\
     {[}S4{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 2     & \multicolumn{1}{l}{3} \\
     {[}S5{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{3} \\
     {[}S6{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{2} \\
     {[}S7{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 5     & \multicolumn{1}{l}{2} \\
     {[}S8{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{3} \\
     {[}S9{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & 8     & \multicolumn{1}{l}{2} \\
     {[}S10{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 5     & \multicolumn{1}{l}{2} \\
     {[}S11{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{3} \\
     {[}S12{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 1     & \multicolumn{1}{l}{3} \\
     {[}S13{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{3} \\
     {[}S14{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 0     & \multicolumn{1}{l}{2} \\
     {[}S15{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{2} \\
     {[}S16{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 3     & \multicolumn{1}{l}{2} \\
     {[}S17{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 8     & \multicolumn{1}{l}{3} \\
     {[}S18{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{3} \\
     {[}S19{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{2} \\
     {[}S20{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & 4     & \multicolumn{1}{l}{2} \\
     {[}S21{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{2} \\
     {[}S22{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{2} \\
     {[}S23{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 4     & \multicolumn{1}{l}{2} \\
     {[}S24{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 4     & \multicolumn{1}{l}{2} \\
     {[}S25{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{3} \\
     {[}S26{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{2} \\
     {[}S27{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{2} \\
     {[}S28{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{3} \\
     {[}S29{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 1     & \multicolumn{1}{l}{3} \\
     {[}S30{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{2} \\
    \multicolumn{1}{l}{Total} & \multicolumn{1}{l}{16} & \multicolumn{1}{l}{15} & \multicolumn{1}{l}{20} & \multicolumn{1}{l}{15} & \multicolumn{1}{l}{18} & \multicolumn{1}{l}{6} & \multicolumn{1}{l}{14} & \multicolumn{1}{l}{13} & \multicolumn{1}{l}{7} & \multicolumn{1}{l}{21} & \multicolumn{1}{l}{24} &       &  \\
    \bottomrule
    \end{tabular}%
  \label{tab:qualityEvaluation}%
\end{table}%

The quality evaluation was done by the researcher after data extraction of
each primary study. Each category was evaluated with a scale from 0 to 1. The
evaluation form was adopted from \citepsrc{dyba_empirical_2008}. Detailed list
of quality evaluation questions can be found in \cref{app:quality}.
Additionally, a relevancy factor was assigned for each study describing its
relevancy for this study. The scale for the relevancy can be found from
\cref{sec:dataExtraction}.

The perceived quality of the studies varied a lot (from 0 to 10). Even though
there were many low scoring studies, they were included since they still
provided valuable insight. For example, in some cases an experience report
[S4] provided more valuable data than a high scoring research paper [S25].

According to the quality evaluation, control group and reflexivity had the
lowest total scores while value for research, context and findings scored the
highest. There were 13 primary studies with a total score of 8, 9 or 10, and
11 primary studies with a total score of 1, 2 or 3.

\section{Metrics}
\label{sec:metricsCategorization}

This section lists, categorizes and compares the found metrics from the SLR.
First, the found metrics are listed by primary study in \Cref{tab:Metrics}
answering \textbf{Research Question 1:} \emph{What metrics are used in
industrial agile software development?}. Second, the metrics are categorized according
  to the categorization by \citetsrc{fenton1998software} in
  \Cref{tab:metricsCategorization}, answering \textbf{Research Question 1.1:}
  \emph{What are the entities measured in industrial agile software
  development?}. Third, the found metrics are compared to the metrics
  suggested by agile methods in \Cref{tab:comparisonOfMetrics}, answering
  \textbf{Research Question 1.2:} \emph{How do the metrics found in industrial
  agile software development compare to those suggested by agile literature?}.
  Only metrics, which reason of use, effect of
use or importance was described, were collected. A total of 102 metrics were
found from the primary studies.


\begin{longtable}{lp{12cm}} 
\caption{RQ1: Metrics by primary studies}\\
\toprule\\
ID & Metrics\\
\midrule\\ \relax
[S1] & Business value delivered, customer satisfaction, defect count
after testing, number of test cases, running tested features\\ \relax
[S2] & Velocity\\ \relax
[S3] & Critical defects sent by customer, open defects, test
failure rate, test success rate, remaining task effort, team effectiveness\\
\relax
[S4] & Technical debt board, build status,
technical debt in effort\\\relax
[S5] & Burndown, check-ins per day, number of automated passing test steps,
faults per iteration\\\relax
[S6] & Velocity, story estimates\\\relax
[S7] & Burndown, story points, \# of open defects, \# of defects found
in system test, defects deferred, Net Promoter Score\\\relax
[S8] & Story points, task effort, velocity, operation's
velocity\\\relax 
[S9] & Effort estimate\\\relax
[S10] & \# of defects/velocity\\\relax
[S11] & Revenue per customer\\\relax
[S12] & Task's expected end date, effort estimate,
completed web pages, task done\\\relax
[S13] & Fix time of failed build, story flow percentage,
percentage of stories prepared for sprint, velocity of elaborating features,
velocity of implementing features\\\relax
[S14] & Build status, test coverage, test growth ratio, violations
of static code analysis, \# of unit tests\\ \relax
[S15] & Effort estimate / effort estimate / effort estimate / effort
estimate\\\relax
[S16] & Sprint burndown, release burndown, cost
performance index, schedule performance index, planned velocity\\\relax 
[S17] & Common tempo time, number of bounce backs, cycle
time, work in progress, customer satisfaction (Kano analysis), effort
estimate kits\\\relax
[S18] & Lead time, processing time, queue time\\\relax
[S19] & Change requests per requirement, fault slips,
implemented vs wasted requirements, maintenance effort, lead time\\\relax
[S20] & Number of requests from customers, inventory of requirements
over time\\\relax
[S21] & Rate of requirements per phase, variance in
handovers, requirement's cost types\\\relax
[S22] & \# of requirements per phase, lead time\\\relax
[S23] & Average velocity / work in progress, cycle time, pseudo
velocity\\\relax
[S24] & Lead time, work in progress%, \# of days in maintenance, \# of days to
% overdue, reported hours on CSR
\\\relax
[S25] & Defect trend indicator, \# of defects in backlog,
predicted \# of defects\\\relax
[S26] & Throughput, queue\\\relax
[S27] & Burndown, check-ins per day, number of automated passing test
steps, number of new and open defects\\\relax
[S28] & Burndown, number of automated passing test steps, check-ins per
day\\\relax 
[S29] & Story estimate, story complete percentage\\\relax
[S30] & Progress as working code\\
%[S31] & Costs, schedule\\\relax
\bottomrule
\label{tab:Metrics} 
\end{longtable}

%    \begin{tabular}{p{2cm}p{7cm}p{5,5cm}} fenton kategorisoinnin taulukonkoko

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{RQ1.1: Metric categorization based on
  \citetsrc{fenton1998software}}
 	\begin{tabular}{p{2,1cm}p{7cm}p{5,5cm}}
    \toprule
    Entities & Attributes &  \\
    \midrule
\textbf{Products} & \textbf{Internal} & \textbf{External} \\
    Products & Running tested features & Customer satisfaction x2, Net
    Promoter Score, number of requests from customers, progress as working code \\
    Test plans & Number of test cases &  \\
    Code  & Technical debt in categories, technical debt in effort, violations of static code analysis &  \\
    Builds & Build status x2, fix time of failed build &  \\
    Features & Remaining task effort, task's expected end date, task done, effort estimate x14, work in progress x3, number of days in maintenance, story complete percentage & Business value delivered, change requests per requirements \\
    Require-ments & Implemented vs wasted requirements, requirement's cost types, Percentage of stories prepared for sprint &  \\
    Defects &       & Defect trend indicator, predicted number of defects \\
    \textbf{Processes} &       &  \\
    Testing & Defect count after testing, critical defects sent by customer, open defects x5, test success rate, test failure rate, faults per iteration, defects found in system test, defects deferred, test coverage, test growth ratio & Number of bounce backs, fault slips \\
    Implementa-tion & Velocity x13, number of unit tests x4, completed web
    pages, cost performance index, schedule performance index, planned
    velocity, common tempo time, average velocity, check-ins per day x3 &
    Story flow percentage \\
    Requirements engineering & velocity of elaborating features &  \\
    Whole development cycle & cycle time x2, lead time x4, processing time, queue time, maintenance effort, number of work items per phase x2, variance in handovers, rate of requirements per phase, throughput, queue &  \\
    \textbf{Resources} &       &  \\
    Team  &  & Team effectiveness \\
    Customer & Revenue per customer &  \\
    \bottomrule
    \end{tabular}%
  \label{tab:metricsCategorization}%
\end{table}%

According to the categorization in \Cref{tab:metricsCategorization}, metrics
are used to measure products, test plans, code, builds, features, requirements
and defects. Most of the entities in Products class are measured internally,
except the products entity, which is measured mostly externally. Furthermore,
testing, implementation and whole development cycle
are measured mostly internally in the Processes class. Only two metrics are
related to measuring Resources class.

\fixme{Ylläolevaa voisi hieman muokata}

% Table generated by Excel2LaTeX from sheet 'Taul2'
%\begin{comment}
\begin{longtable}{p{3cm}p{1,4cm}p{1,5cm}p{1cm}p{1,5cm}p{1,5cm}p{1,5cm}}
  \caption{RQ1.2: Comparison of metrics found from agile literature compared
  to the metrics found from this study}\\
   
 \toprule
    Metrics suggested by literature & Method & Scrum & XP    & Kanban & LeanSD
    & NA
    \\
    \midrule
    Effort estimate & Scrum, XP, LeanSD & 2[S7]\footnote{Number prior to study
  reference is the index of the metric in \Cref{tab:Metrics}}, 1[S8], 2[S8], 3[S12] & 1[S9] &       &       & 1[S15], 2[S15], 3[S15], 4[S15], 6[S17], 1[S29] \\
    Velocity\footnote{Includes total work remaining from Scrum and effort left
    from Scrum and XP.} & Scrum, XP, LeanSD & 3[S8], 2[S10],  4[S8], 1[S16], 2[S16], 5[S16], 1[S23] & 1[S6], 1[S5], 1[S27], 1[S28] & 4[S23] & 5[S13] & 1[S2], 5[S3] \\
    Written and passed unit tests & XP, LeanSD &       & 3[S5], 3[S27], 2[S28], 5[S14] &       &       & 5[S1] \\
    Actual development time & XP    &       &       &       &       &  \\
    Load factor & XP    &       &       &       &       &  \\
    Work in progress & Kanban & 1[S21] &       & 2[S23], 2[S24] &       & 4[S17], 2[S20], 1[S22] \\
    Lead time & Kanban &       & 5[S19] & 1[S24] & 1[S18] & 2[S22] \\
    Due date performance & Kanban &       &       &       &       &  \\
    Throughput & Kanban &       &       &       &       & 1[S26] \\
    Issues and blocked work items & Kanban &       &       &       &       &  \\
    Flow effiency & Kanban &       &       &       &       &  \\
    Initial quality & Kanban, LeanSD & 3[S7], 4[S7] & 4[S5], 1[S10], 4[S27] &       &       & 3[S1],  2[S3], 2[S25] \\
    Failure load & Kanban &       &       &       &       & 2[S17] \\
    Cycle time & LeanSD &       &       & 3[S23] &       & 3[S17] \\
    Value Stream Maps (Work time, wait time) & LeanSD &       &       &       & 2[S18], 3[S18] &  \\
    Amount of written and passed acceptance tests per iteration & LeanSD &       &       &       &       & 4[S1], 3[S3], 4[S3] \\
    Not suggested &       & 1[S4], 2[S4],  3[S4], 4[S7],  5[S7],  1[S11], 1[S12],   3[S12], 4[S12], 3[S16], 4[S16], 2[S21], 3[S21] & 2[S5], 2[S27], 3[S28], 1[S14], 2[S14], 3[S14], 4[S14], 1[S19], 2[S19], 3[S19], 4[S19] &       & 1[S13],  2[S13],  3[S13], 4[S13] & 1[S1], 2[S1],  1[S3], 6[S3], 1[S17], 5[S17],  1[S20],  1[S25], 3[S25], 2[S26], 2[S29], 1[S30] \\
    \bottomrule
    \label{tab:comparisonOfMetrics}
    \end{longtable}%
%\end{comment}



Metrics found from the primary studies are compared to the metrics suggested
by agile literature in \Cref{tab:comparisonOfMetrics}. The metrics found
from literature are from Scrum \citepsrc{schwaber2011scrum}, XP
\citepsrc{beck2004extreme}, Kanban \citepsrc{anderson2010kanban} and LeanSD
\citepsrc{poppendieck2003lean}. The four rightmost columns in
\Cref{tab:comparisonOfMetrics} are describing the agile method used in the
primary studies. In some primary studies it was hard to identify a specific
agile method, thus 'NA' is used to describe those cases. The number before
primary study reference defines the index of the metric in \Cref{tab:Metrics}.

Actual development time (XP), load factor (XP), due date performance (Kanban),
issues and blocked work items (Kanban) and flow efficiency (Kanban) were not
described in primary studies. Many metrics (40 of 102, 39\%) found
from the primary studies were not suggested in agile literature.

\begin{comment}
Metrics could help in decreasing software project failures due to
prioritization and resource \& schedule issues in iteration tracking. Metrics
here are all reactive.

Iteration tracking metrics could help in monitoring issues that lead to
project failures. Metrics are mostly reactive.

Metrics that are used to motivate and improve people can be used to solve
issues related to values \& responsibilities and company policies that lead to
project failures. Metrics are both reactive and proactive.

Metrics that are used to point identify problems can be used to prevent Method
causes for failures. Metrics are almost all proactive.

Metrics that are used to improve or understand pre-release quality can be used
to prevent failures that are caused by value \& responsibility, task output
and existing product related issues. Metrics are mostly reactive.

Metrics that are used for post-release quality can be used to prevent failures
that are caused by customers' and users' opinions. Metrics are mostly
proactive.

In general, used metrics were more reactive than proactive.

In general, it seems that metrics are used the most to prevent project
failures that are caused by Methods, then by Environment and not so much about
People or Tasks.
\end{comment}

\section{Why are metrics used?}
\label{sec:WhyMetricsUsed}
\textbf{Research Question 2:} \emph{Why are metrics used in industrial
  agile software development?}\\*\\
The following sections describe the reasons for the use of metrics.
\Cref{tab:whyCategories} lists the categories for reasons of metric use by
sources. Some of the descriptions for the reasons of metric use might seem to
be incomplete. This is because the effects of metric use are described in the
following \cref{sec:effects}. Also, some reason categories might seem to be
describing more the effect than the reason, e.g.,
\nameref{sec:balanceWorkflow} in \cref{sec:balanceWorkflow}. However, the
reasons for metric use are described here for \nameref{sec:balanceWorkflow}
and then the actual actions and effects are described in the next
\cref{sec:effects}.

%    \begin{tabular}{rp{7cm}}
% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Reasons for the use of metrics by sources}
    \begin{tabular}{rp{7cm}}
    \toprule
    Categories & Sources \\
    \midrule
    Planning & [S6, S8, S9, S11, S12, S16, S17, S21, S23, S24, S25, S29] \\
    Progress tracking & [S2, S4, S5, S7, S8, S12, S13, S16, S17, S20, S21, S22, S23, S25, S27, S28] \\
    Understand and improve quality & [S3, S4, S5, S7, S14, S17, S19, S22, S28, S29] \\
    Identify problems & [S2, S13, S16, S18, S20, S21, S22, S25, S29] \\
    \bottomrule
    \end{tabular}%
  \label{tab:whyCategories}%
\end{table}%

\subsection{Planning}
\label{sec:planning}

Prioritization of tasks was one of the main activities metrics were used for.
At Objectnet, effort estimates were used to prioritize the features for next
release and as basis for resourcing [S9]. Teams at Adobe Systems used effort
estimates to prioritize activities based on relative value and relative effort
[S8]. At Verisign Managed Security Services, they used revenue per customer to
prioritize their backlog [S11]. At Timberline Inc, they used Kano analysis as
a 'voice of customer' so that prioritization decisions could be based on facts
instead of political power [S17]. Practitioners at Ericsson used cost types,
rate of requirements over phases and variance in handovers for short term
decisions related to requirements prioritization, staff allocation and
planning decisions [S21].

Metrics were used to estimate the size and amount of features that could be
taken under development. Velocity was used to improve effort estimates for
next the planning session, which helped to estimate the scope for the next
iteration [S16]. Scrum master and product owner at a Korean e-commerce company
used estimates to check if the planned scope would be possible to complete
during the next iteration [S12]. At WMS Gaming, they used pseudo-velocity and
average velocity to plan their releases [S23]. At Ericsson product maintenance
team, lead time was used to understand if all planned corrections can be
completed before release date [S24]. At Avaya Communications, they used story
estimates to predict the iteration where a feature would be completed
[S29].

Other planning uses for metrics were resourcing decisions and development
flexibility. At Timberline Inc, they broke down requirements into smaller
pieces that were estimated in effort to understand what skills are needed to complete
the work [S17]. At a Korean e-commerce company, they marked tasks done and
undone which made it possible to take undone tasks for the next iteration
[S12]. Also, they marked expected end date for tasks so the next person in
workflow could plan their own work as effectively as possible thus reducing
idle time. At ThoughtWorks, stories and their effort estimates were used as
the fundamental units of development for the iteration and as basis for
resourcing [S6]. At Ericsson, predicted number of defects was used to plan the
removal of defects [S25]. If the removal of defects would not be well planned
it could cause delays for the release and thus increase costs for the project.

\subsection{Progress tracking}
\label{sec:progressTracking}
Reasons for metrics use in progress tracking are divided into project
progress, increase visibility, accomplishing project goals and balance workflow.

\subsubsection{Project progress}
\label{sec:projectProgress}

Metrics were used to monitor the progress of the project. Completed web
pages metric was used as a measure of progress at Korean e-commerce company
[S12].
Number of automated passing test steps was used as a measure of progress in terms of
completed work at Mamdas [S5]. At Timberline Inc, breaking down tasks to
'kits' between 2 to 5 days enabled progress monitoring [S17]. Set of metrics
(burndown, check-ins per day, number of automated passing test steps, number
of new and open defects) was developed to manage risks and provide timely
progress monitoring [S27]. Developers at Avaya Communications used story
percent complete metric to give assessment of progress [S29]. However, a team
at NASA Ames Reserch Center did not want to spend resources on estimating features and
instead they focused on designing and developing their software solution
[S30]. Every six weeks they demonstrated their progress to
customer with working code.

Metrics were also used to give a higher level understanding of progress.
Release burndown showed project trends and could be used to predict completion
date [S16]. Also, release burndown could reflect addition or removal of
stories. At Ericsson, cost types, rate of requirements over phases and
variance in handovers were used to provide overview of progress [S21]. Metrics
(burndown, check-ins per day, number of automated passing test steps) were used to
communicate progress to upper management [S5] and ensure good progress to
external observers and ensure that key risks were under control [S5,S27,S28].

\subsubsection{Increase visibility}
\label{sec:increaseVisibility}
Metrics were used to simplify and understand complexity, and increase
visibility for all stakeholders. Cost types, rate of requirements over phases
and variance in handovers were used to increase the transparency of end-to-end
flow in a complex system [S21]. Similarly at Petrobras, technical debt board
was used to make technical debt issues visible and easier to manage [S4]. Metrics (burndown,
check-ins per day, number of automated passing test steps, number of open and
new defects) were used to replace individual perception with facts [S27].

Metrics were used to keep the team informed. At Ericsson, defect trend
indicator was used to monitor defect backlog and spread the information to
project members [S25]. At WMS Gaming, cycle time metric was used to let the
team track their performance [S23]. At Avaya Communications, story percent
complete metrics were generated automatically when tests were run and thus
kept everyone on the same page and eliminated schedule surprises [S29].
Additionally, the metric results were required to be reported periodically. At
Slovenian publishing company, release burndown made the correlation clear
between work remaining and team's progress in reducing it [S16].
 
\subsubsection{Accomplishing project goals}
\label{sec:accomplishProjectGoals}
Metrics were used to understand if project goals can be achieved. At
Timberline Inc, there was a need for simple indicator that would quickly tell
if project is under control [S17]. They used common tempo time to understand if
project was in target for delivery. At Microsoft Corporation, they monitored
work in progress to predict lead time which in turn would predict project
schedule [S2].
At Adobe Systems, sprint burndown was used to tell the team if they were on
track regarding the sprint commitments [S7]. Similarly at Mamdas, burndown was
used to see if the team could meet their goals, and if not, what could be done
[S5]. Burndown was also used to mitigate the risk where developers spend too
much time perfecting features over finishing all tasks of the iteration [S28].
Story flow percentage was used so that a developer could finish a story in a
steady flow [S13]. Story implementation flow metric describes how efficiently
a developer has been able to complete a story compared to the estimate.

\subsubsection{Balance workflow}
\label{sec:balanceWorkflow}
Metrics were used to balance workflow to prevent overloading people. At
Ericsson, inventory of requirements over time was used to identify large
handovers of requirements that would cause overloading situations to employees
[S20]. The aim was to have steady flow of requirements.
Similarly at Citrix Online, operations department was overloaded so they
decided to start evaluating incoming work with Ops story points to level the
workload [S8]. Moreover, people should be respected by having balanced
workload to avoid overload situations [S22]. This could be achieved by
measuring the number of requirements per phase which would notice peaks of
workload. Timberline Inc tried to pace work according to customer demand [S17]. However, too much
work was pushed to development, which caused many problems, including developers feeling
overworked. They started using common tempo time to make sure there would be
balance of workflow.
% Component level burndown was used to decide on resource mobility [S5].

At Ericsson, variance in handovers was used to guarantee that requirements
would flow evenly [S21]. Mamdas was measuring check-ins per day metric, which
measured how often code was committed to main trunk [S5]. The point was to
avoid people from committing only at the end of the iteration, and instead integrate
early and often. At WMS Gaming, they had problems with large tasks
blocking other work, so they set a rule that only certain size of tasks (8
story points) can be taken for development [S23].

\subsection{Understand and improve quality}
\label{sec:quality}
The following sections describe how metrics were used to understand the
quality of the product both before and after release. Also, the sections will
describe that metrics were used to improve the quality of the product and
ensure that the product will be tested thoroughly.

\subsubsection{Understand level of quality}
\label{sec:understandQuality}
Metrics were used to understand the level of quality after the release.
Number of change requests from customer was used as an indicator of customer
satisfaction [S19]. Maintenance effort was used as an indicator of overall
quality of the released product [S19]. Number of maintenance requests was used
as an indicator of built-in quality [S22].

Metrics were also used to understand the level of quality before the release.
At Adobe Systems, they measured pre-release quality with Net Promoter Score
which was measured from pre-release customer surveys [S7]. Net Promoter Score
measures how willing a customer is to recommend the product to another
potential customer. They also measured defects found in system test which was
used to measure the quality of software delivered to system test process.
Additionally, they measured defects deferred, which was used to predict the
quality customers would experience. Defects deferred were defined as the
defects that are known but are not fixed for a release, usually due to time
constraints. At Mamdas, faults per iteration were used to measure the
quality of the product [S5].

\subsubsection{Increase quality}
\label{sec:increaseQuality}
Metrics were used to increase the level of quality. Governance mechanisms,
which included a set of metrics (burndown, check-ins per day and number of
automated passing test steps), were used to increase product quality [S28].
At T-Systems International, they used a set of metrics(build status, number of
unit tests, test coverage, test growth ratio, violations of static code
analysis) to improve the internal software quality of the project [S14]. Build
status was measured to prevent defects reaching production environment.
Similarly, violations of static code analysis metric was used to prevent the
existence of critical violations. Furthermore, critical defects sent by
customers were tracked and fixed to prevent losing customers [S3].
Finally, technical debt board was used to reduce technical debt [S4].

\subsubsection{Ensure level of testing}
\label{sec:ensureTesting}
Metrics were used to make sure the product is tested thoroughly.
At T-Systems International, test coverage was used to evaluate how well the
code was tested [S14]. However, in Brown-field (legacy) projects it's better
to measure test-growth-ratio since there might not be many tests in the
existing code base. At Timberline Inc, work in progress was measured so it
could be minimized [S17]. Large amount of work in progress would contain many
unidentified defects which would need to be eventually discovered. At Mamdas,
using number of automated passing test steps decreased the risk
that the product would be unthoroughly tested [S5]. Similarly, number of
automated passing test steps was used to make sure regression tests are ran
and passed every iteration. Finally, story percent complete metric supported
test driven development by requiring unit tests to be written for progress
tracking [S29].

\subsection{Identify problems}
\label{sec:identifyProblems}
Metrics were used to identify problems, bottlenecks and waste in the process.
Cumulative number of work items over time metric was used to identify
bottlenecks in the development process [S22]. Similarly, monitoring work in
progress was used to identify blocked work items and also the development
phase where the blockage occurred [S2]. Cost types, rate of requirements over
phases and variance in handovers were used for process improvement by spotting
bottlenecks and uneven requirement flows [S21]. For example, a lot of
requirements were transferred to System Test phase, but only a small amount of
requirements were transferred to Ready for Release phase, see
\Cref{fig:handOvers}.

\begin{figure}
	\includegraphics{images/HandoverPet2011.jpg}
	\centering
	\caption{Rate of requirements identified a bottleneck in System Test phase
	[S21]}
	\label{fig:handOvers}
\end{figure}

Metrics were able to identify waste, as in development phases were no value is
added, in software processes. At Ericsson, Value Stream Maps (VSM) were used
to spot waste in the development process [S18]. In another case at Ericsson,
long lead times led to identification of waste of waiting [S22].
Similarly, measuring story flow percentage allowed identification of waste related to context
shifts [S13].

Metrics were used to identify problems and find improvement opportunities.
Defect trend indicator was used to provide the project manager an ISO/IEC
15939:2007 compatible indicator for problems with the defect backlog [S25].
Basically, the indicator showed if the defect backlog will increase, stay the
same or decrease in the coming week. The project manager could then use the
info to take necessary actions to avoid possible problems. At a Slovenian
publishing company, schedule performance index and cost performance index were
used to monitor for deviances in the project's progress and providing early
signs if something goes wrong [S16]. Developers at Avaya had issues with the
80/20 rule, where the last 20\% of iteration takes the longest [S29]. With the
metrics that their tool T3 provided (e.g story percent complete) they were
able to see the early symptoms of various problems that can cause delays, and
thus react early.

Metrics were also used to find improvement opportunities. Number of work items
per phase and lead time was used to spot instabilities in the process [S20].
They had set control limits to the metrics and if a measured value was outside
the control limits it meant that there was some kind of instability.

%Similarly, monitoring schedule and costs with a dashboard allowed to spot for
% improvement opportunities at OCLC [S31].

\section{What are the effects of metric use?}
\label{sec:effects}
\textbf{Research Question 3:} \emph{What are the effects of  metric use in
  industrial agile software development?}\\*\\
The following subsections describe the effects of metric use.
\Cref{tab:howCategories} lists the effect categories by sources. Some of the
categories for the reasons and the effects of metrics are close to each other
so sometimes it can be hard to understand why some categories are listed under
the reasons and not under the effects. For example \cref{sec:balanceWorkflow}
describes that the purpose for metrics was the balancing of workflow and then
the actual actions and effects are described under \cref{sec:effects}.

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Effects for metric usage by sources}
    \begin{tabular}{rp{7cm}}
    \toprule
    Categories & Sources \\
    \midrule
    Planning actions & [S2, S11, S12, S23] \\
    Reactive actions & [S3, S5, S10, S14, S16, S17, S28] \\
    Motivate people & [S3, S4, S6, S13, S14, S17, S25] \\
    Create improvement ideas & [S8, S10, S13, S14, S17, S18, S20, S21, S22, S26, S27, S28] \\
    \bottomrule
    \end{tabular}%
  \label{tab:howCategories}%
\end{table}%


\subsection{Planning actions}
\label{sec:planningActions}
This section describes the planning actions that happened due to the
use of metrics.

Product owners at WMS Gaming used lead time to schedule high priority features
and plan demo dates with customers [S23]. Similarly, at Verisign
Managed Security Services they used revenue per customer metric to allow
higher valued features to be prioritized higher in the backlog [S11].

Velocity / 2 metric was used as scoping tool for a release [S23]. The team had
enough work not to sit idle, but there was still enough time to fix high
priority defects. Similarly, effort estimates were used to scope the iteration and if
there were tasks that could not be completed before a release date then they
were excluded from the backlog [S12]. Furthermore, velocity was used to define a
minimum delivery level for the iteration where 'must have' requirements are
assigned, and a stretch goal where lower priority requirements are assigned
[S2].

Expected date of task completion was used so that other team members could
plan their own work [S12]. For example, a developer could know when she can
start implementation because the designer had informed the expected date of
completion for the design.

\subsection{Reactive actions}
\label{sec:reactiveActions}
This section describes reactive actions that occurred due to the use of
metrics.

Metrics were used to cut down the scope of an iteration or to add more
resources if it seemed not all tasks could be completed with current pace.
When component level burndown was used to notice that a component was behind
schedule at Mamdas, resources were added and scope was reduced for the release
[S5]. Similarly, release burndown showed that work remaining was not
decreasing fast enough so the scope of the release was decreased [S16].
Furthermore, if common tempo time would indicate too much planned work, then
the tasks would be cut or more resources would be added [S17]. Similarly,
employees were trained with multiple skills, e.g., customer support did
testing and documentation engineers were taught how to input their material into the
system, so in case of imbalanced workload the work could be reorganized to
achieve more balanced workflow.  Similarly, If team effectiveness is not high
enough to complete tasks, resources from other teams can be used [S3]. Other actions
that were suggested in case of low team effectiveness were reduction of tasks
and working overtime.

Metrics were also used to react to quality information. At Timberline Inc,
monitoring cycle times revealed high time consumption on manual testing [S17].
The cause was an unmotivated person who was then moved to writing automated
test scripts which he preferred over manual testing. At Escrow.com, number of
defects was used to delay a release when too many defects were noticed in a QA cycle [S10].
At T-Systems International, quality manager interpreted results of static code
analysis from the build tool and he would then make plans for necessary
refactorings [S14]. When amount of written and passed unit tests was not
increasing, an alarm was raised at Mamdas [S28]. The issue was discussed in a
reflection meeting where they understood that too much work was put to a
single tester writing the tests and once she was doing work for another
project, no tests were written. The team then started to learn to write tests
themselves, and later, a dedicated tester was assigned to write the
tests.

\subsection{Motivate people}
\label{sec:motivatePeople}
This section describes the motivating effects that the metrics had on people.

Metrics were used to motivate people to react faster to problems. Number of
defects was shown in monitors in hallways which motivated developers to fix
the defects [S3]. Similarly, total reported defects, test failure rate and
test success rate were also shown throughout the organization which motivated
people to avoid problems and also fix the problems fast. At Systematic, they measured
fix time of broken build and showed the time next to the coffee machine. It
provoked discussion on the reasons for long fix times, and eventually,
the developers fixed the builds faster [S13]. The metric was later declared
mandatory for all projects. Also, the reasons for long fix times were
investigated. Similarly at Petrobras, build status was visible in minutes
after commits, which helped to create a culture where developers react with
high priority to broken builds [S4]. This helped to keep the main branch to be
closer to deployable state at all times. Build status was used to motivate
people to fix the build as fast as possible [S4]. Moreover, violations of
static code analysis caused developers to immediately fix the issue because
the violations could cause a broken build status [S14]. Additionally,
developers could get faster feedback on their work. Furthermore, developers
could have more confidence in performing major refactorings with the safety
net the violations of static code analysis metric provided.

Metrics were used to change the behavior of people. At Petrobras, they used a
technical debt board to discuss technical debt issues in their projects, see
\Cref{fig:debtBoard}. In the meetings, team members agreed which technical
debt issues they would focus in solving until the next meeting [S4].
Additionally, team members sought help from the architecture team for reducing
technical debt, e.g., by implementing automatic deployment systems and
improving source code unit testability. At Mamdas, measuring the number of
automated passing test steps changed the team's behaviour to write more unit
tests [S5]. Metrics were also used to prevent harmful behaviour such as cherry
picking features that were the most interesting to the team [S17]. Measuring
work in progress (WIP) and setting WIP limits prevented cherry picking by enforcing
working on only two features at a time, and thus preventing them from working
on lower priority, but more interesting, features.
Finally, at Ericsson defect trend indicator created crisis awareness and
motivated the developers to take actions to avoid possible problems [S25].

%\begin{comment}
\begin{figure}
	\includegraphics{images/DebtBoard.jpg}
	\centering
	\caption{Technical debt board was used to discuss technical debt issues
	[S4]}
	\label{fig:debtBoard}
\end{figure}

%\end{comment}

There can also be negative effects in using metrics. Using velocity metric had
negative effects such as cutting corners in implementing features to maintain
velocity with the cost of quality [S6]. For example, the managers excused the
developers from writing tests and the testers cut on the thoroughness of the
testing in hopes to maintain the velocity.

\subsection{Create improvement ideas}
\label{sec:createImprovementIdeas}
This section describes improvement ideas that were created based on metrics.

At Ericsson, lead time, processing time and queue time metric were used to
identify waste of extra process - (a requirement would wait for long time
before full specification) requirement specification [S18]. Solution idea was created
where a quick high level proposal would be sent to the customer without the
need for in-depth specification. The customer could then use the high level
proposal to evaluate if they want to pursue that requirement further.
% Similarly, two requirement specification phases could be combined when
% another waste of extra process was identified in requirements specification
% phase.
Furthermore, long processing times for solution proposal phase indicated waste
of motion, where requirements are clarified between marketing and development
unit. The solution idea was to increase close collaboration between marketing
unit and development unit at least for the more complex requirements.
Finally, there was a waste of waiting in design phase which could be
improved by starting real work only when the purchase order is received, not
when requests are received.

Lead time, processing time and queue time metric were used to identify waste
of waiting in testing phases [S18]. The improvement suggestion was to provide
earlier beta version and making testing phases parallel. Many of the
improvement ideas came from meetings where Value Stream Maps (VSM) were
used as a base for discussion.

Cost types, rate of requirements over phases and variance in handovers were
used to identify bottlenecks at Ericsson [S21]. They noticed that focusing on
deadlines caused a lot of requirements to be transferred to system test phase
close to the deadline. The improvement suggestion was to focus more on
continuous delivery instead of focusing on market driven deadlines.
Furthermore, Kanban was suggested as a development method to accomplish the
continuous delivery capabilities. Similarly at another case at Ericsson,
throughput and queue time metrics were used to identify a bottleneck in
network integration test phase which led to using other testing practices in future
projects [S26].

Rate of requirements over time was used to identify problems in the
development process [S20]. One improvement suggestion was to change from push
to pull-approach so that the team could adjust the workload to enable a more
continuous delivery. Another improvement suggestion was to add intermediate
release versions so that integration and testing would happen more often and
problems could be identified earlier than close to the actual release. Similar
solution was applied at Timberline Inc. where requirements inventory was kept
low which meant that design, implementation and testing could start earlier
and problems in requirements would get caught sooner [S17].

Citrix Online started measuring velocity for their Operations department as
well [S8]. This led to development departments trying to decrease their
products' Operations story points to enable faster releases. The reduction in
story points was possible by creating hot deployment strategies and providing
better documentation.

At an Israel Air force IT department, Mamdas, they were using burndown to
follow their progress [28]. However, when they noticed that work remaining was not
decreasing according to remaining resources they had to make changes. In their
iteration summary meeting they decided to pursue senior engineers to help them
create optimal development environments and continuous build systems. Also,
they decided to evaluate customer requests in more detail to avoid over
polishing features. 

A team working on automating workflows in criminal justice
system noticed that their velocity estimations were inaccurate which led to
dividing work items into smaller pieces to improve the accuracy of the
estimates [S10]. The division of work items meant that the team needed to
perform more analysis of the features during planning.

When story implementation flow metric showed a drop and project managers
complained about clarifications about features from customer were late, a root
cause analysis meeting was held [S13]. Also, after starting to use the
implementation flow metric new policies were stated to keep the flow
high: percentage of stories ready for sprint must be 100\% and implementation
flow must be at least 60\%. Moreover, both of the metrics need to be reported
monthly. Root cause analysis was also conducted at Timberline Inc to decrease
the amount of bounce backs [S17].

The reasons for the values of metrics (burndown, check-ins per day, number of
automated passing test steps, number of new and open defects) were discussed
in iteration summary meeting because it can be hard to analyze metrics without
understanding the context [S27]. Similarly at Ericsson, number of work items
per phase was used to ask development unit about the values of the metric and
the development unit confirmed that people felt overloaded as the metric
suggested [S20]. Furthermore in another case at Ericsson, if the values
of number of work items were outside the control limits one could discuss with the
developers about the workload [S22].

At Systematic, after analyzing long fix times for broken builds the team added
automatic static code analysis checks to code check-in to catch defects
earlier [S13]. Similarly at T-Systems International, Quality manager could
change coding style guide and code standards based on the results of
\emph{violations to static code analysis} metric [S14].

\section{Important metrics}
\label{sec:importantMetrics}
\textbf{Research Question 4:} \emph{What metrics are important in industrial
  agile software development?}\\*\\

The following subsections describe first which metrics were described as
important, and then second, which metrics were described the most times in the
primary studies.

\subsection{Important metrics in terms of statements}
 
This section describes metrics that were described as important. Metrics were
considered important if the author of the primary study, or case employees
praised the metric. Also, metrics were considered important if there were
signs of continuous use of the metric. Furthermore, if metrics had positive
correlation to project success in surveys, they were considered important.

\begin{comment}
Capacity as number of features developed in release was considered better than
measuring speed, since speed is generally thought as a attribute of humans.
Capacity on the otherhand measures the capabilities of an organization.
\end{comment}

Progress as working code was considered as one of the cornerstones of agile
[S30]. Story flow percentage and velocity of elaborating features were
considered as key metrics for monitoring projects [S13]. Also, a minimum of
60\% for story flow percentage was identified as a key limit. Similarly,
velocity for elaborating features should be as fast as velocity of
implementing features. Also, they said that using both aforementioned metrics
\emph{``drive behaviors to let teams go twice as fast as they could before''}. 

Story percent complete metric was considered valuable since it embraces test
driven development - no progress is made before a test is written [S29]. Also,
story percent complete metric was considered more accurate than previously
used metric - however that metric was not mentioned. Moreover, story percent
complete metric gave normalized measure of progress compared to developer
comments about progress. Additionally, story percent complete metric leveraged
existing unit testing framework and thus requires only minimal overhead to
track progress. Furthermore, team members seemed to be extremely happy about
using the story percent complete metric. Practitioners at Ericsson valued the
transparency and the overview of progress that the metrics (cost types, rate
of requirements over phases and variance in handovers) were able to provide to
the complex product development with parallel activities [S21].

Effort estimates were considered important in release planning especially in
terms of prioritization [S9]. According to a survey [S7], top performing teams
at Adobe Systems estimated backlog items with relative effort estimates.
Similarly, pseudo-velocity, which was used by a Kanban team, was considered
essential for release planning [S23]. Moreover, burndown was valuable in meeting sprint commitments [S7].
Furthermore, managers said burndown was important in making decisions and
managing multiple teams [S5]. However, developers did not consider burndown
important [S5]. According to a survey [S1], project success had
significant positive relationship with the following metrics: team velocity, business
value delivered, running tested features, defect count after testing and
number of test cases. However, there were no detailed descriptions of these
metrics.

At another case at Ericsson, Value Stream Maps (VSM) were used to visualize
problem areas, and facilitate discussion for possible improvements [S18].
Practitioners valued how the maps were easy to understand. Metrics that were
used to build VSM were lead time, processing time and queue time. Similarly,
technical debt board, which visualized the status of technical debt, was
considered important because it gave a high level understanding of the
problems [S4]. Moreover, the technical debt board was then used to plan
actions to remove the technical debt. Furthermore, it was proven to be useful
in their context.

Net Promoter Score, which measures the probability of a customer recommending
the product to another potential customer, was said to be \emph{``one of the
purest measures of success''} [S7]. Similarly, projects that were said to be
definitely successful 77\% measured customer satisfaction often or always.
Also, the more often customer satisfaction would be measured the more likely
it would be that the project would have good code quality and the project
would succeed. Similarly, defects deferred metric was seen as a good predictor
of post-release quality because it correlated with issues found by the customers
[S7].

Defect prediction metrics \emph{predicted number of defects in backlog} and
\emph{defect trend indicator} were seen important to decision making, and
their use continued after the pilot period [S26]. Key attributes of the
metrics were sufficient accuracy and ease of use.

The following metrics were considered very useful in agile context: number of
unit tests, test coverage, test-growth ratio and build status [S14]. The
benefit for the number of unit tests was not well described except that it provided
\emph{``first insights''}. Test coverage provided info on how well the code
was tested. Test-growth ratio was useful in projects where old codebase was
used as basis for new features. Finally, fixing broken builds prevented
defects reaching customers. 



% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{RQ4: Important metrics based on amount of evidence and an
  importance factor}
    \begin{tabular}{p{5,5cm}rr}
    \toprule
    Metric & Amount of evidence & Importance factor \\
\midrule
    Effort estimate [S7, S8, S8, S9, S12, S15, S15, S15, S15, S17, S29] & 14    & 3 \\
    Velocity [S2, S3, S5, S6, S8, S8, S10, S13, S16, S16, S16, S23, S27, S28] & 13    & 3 \\
    Work in progress [S17, S20, S21, S22, S23, S24] & 6     & 1 \\
    Defect count [S1, S3, S5, S10, S25, S27] & 5     & 2 \\
    Number of unit tests [S1, S5, S14, S27, S28] & 4     & 1 \\
    Lead time [S18, S19, S22, S24] & 4     &  \\
    Check-ins per day [S5, S27, S28] & 3     &  \\
    Build status [S4, S14] & 2     & 3 \\
    Cycle time [S17, S23] & 2     &  \\
    Customer satisfaction [S1, S17] & 2     &  \\
    Progress as working code [S30] & 1     & 3 \\
    Technical debt board [S4] & 1     & 3 \\
    Net Promoter Score [S7] & 1     & 3 \\
    Story flow percentage [S13] & 1     & 2 \\
    Velocity of elaborating features [S13] & 1     & 2 \\
    Story percent complete [S29] & 1     & 2 \\
    Running tested features [S1] & 1     & 2 \\
    Number of test cases [S1] & 1     & 2 \\
    Value Stream Maps (lead time, processing time, queue time) [S18] & 1     & 2 \\
    Defect trend indicator [S25] & 1     & 2 \\
    Cost types [S21] & 1     & 1 \\
    Variance in handovers [S21] & 1     & 1 \\
    Deferred defects [S7] & 1     & 1 \\
    Predicted number  of defects in backlog [S25] & 1     & 1 \\
    Test coverage [S14] & 1     & 1 \\
    Test-growth ratio [S14] & 1     & 1 \\
    \bottomrule
    \end{tabular}%
  \label{tab:importantMetrics}%
\end{table}%

\fixme{tästä pitäis jauhaa jotain}



\subsection{Important metrics based on the amount of evidence}
This section describes the metrics that were discussed the most in the
primary studies. Metrics that were only mentioned by name without any
reasons of use, effect of use, or importance were not taken into account.

\fixme{consider removing since the table}
Effort estimate (x14) and velocity (x13) were clearly the most discussed
metrics among the studies. Open defects metric (x5) was discussed
the third most. The number of unit tests (x4) and lead time (x4) were both
discussed four times. Work in progress (x6) and check-ins per day (x3) were
both described three times.
Finally, metrics that were mentioned two times were cycle time (x2), build
status (x2),  and customer satisfaction (x2).
\fixme{consider removing}

\chapter{Discussion}
\label{sec:Discussion}
\fixme{Lisää summary resulteista.}

\begin{comment}
Use of metrics can have negative effects. (Mut tää ei oo uutta tietoa. Joku
sanoi että ``Tell me how you will measure so I know how to act'' tjms.
Toisaalta sekin on tulos että vahvistaa olemassaolevaa tietoa. Vähän niinkuin
että ketterässäkin tapahtuu tätä. Ketterä ei suojaa vääränlaiselta
mittarointikulttuurilta. Toisaalta tällaisista negatiivisista vaikutuksista ei
ollut oikein kuin yksi tapaus. Mutta toisaalta se oli vahvaa näyttöä, ja
uskoisin että negatiivista käyttöä harvoin raportoidaan tutkimuksissa,
päinvastoin.) Ydinmittarilla voi olla negatiivisia vaikutuksia. Software
metrics ethiquette gradykirja
\end{comment}


Finding 1: Use of metrics can have negative effects, see
linkkiVelocitykappaleeseen. [S14] also hints towards dysfunctional use of
metrics, for example developers causing broken build if broken build is used
as a KPI \fixme{tää pitäis laittaa tuloksiin}. ``Tell me how you measure me,
and I will tell you how I will behave" said \citetsrc{goldratt2006haystack}.
It seems agile methods don't give any special protection from dysfunctional
use of metrics, even when using one of the core metrics of Scrum. Even though
there weren't a lot of evidence for this, the one case showed strong evidence,
and also it is presumable that dysfunctional use of metrics would rarely be
reported.



\begin{comment}
Tilannesidonnaisten mittareiden merkitys. Havaittu muutamia
mittareita mitkä luodaan jotain ongelmaa varten. Tällainen malli on
mielenkiintoinen. Esimerkkien kautta kuvattu, mikä tarve jne. Kokemuksen
kautta räätälöidään myös mittareita vähän niinku retrospektiin. Samaa kuin
Finding 10.
\end{comment}

Finding 2: This study describes the reasons and effects of metric use. There
was evidence on use of situative metrics. Situative metrics are created and
used based on a need, for example as a solution to a problem. At Systematic,
they had issues with build's long fix times. They started measuring fix time
of broken build and showed the time next to the coffee machine. It provoked
discussion on the reasons for long fix times, and eventually, the developers
fixed the builds faster [S13]. Similarly, they had issues with preparations to
sprints. They started then measuring \emph{percentage of stories prepared for
sprint} supported with a checklist. Furthermore, they set an organizational
objective of at least 100\% for that metric. At Petrobras, they had problems
with customers related to rework and delays. They started measuring technical
debt with a technical debt board that visualised the state of different
technical debt categories in their projects. This helped to create
awareness and address various technical debt issues [S4]. 

In a way metrics are a common tool for process improvement (viitteet), but
maybe this type of creation of custom metrics is something new? (En ainakaan
löytäny äkkiä mitään artikkeleita)


\begin{comment}
Tärkeiden mittareiden ominaisuuksia havaittu: helppokäyttöinen,
hyödyntää olemassaolevia työkaluja. Ja ehkä why osion perusteella visualisoi
tilannetta ja herättää keskustelua (tää ei vaan oikeen seuraa important
metrics osuudesta). Jos why:sta saa perusteltua tärkeyden. Tärkeyttä voi
tarkastella monelta kantilta. Monet samat syyt voi perustella tärkeyden. Tutki
tärkeiden mittareiden listaa jos siitä näkisi ominaisuuksia.

Tärkeitä mittareita nimetty. Kahdelta kantilta arvioitu. jeejee.
Onko tämä kuitenkaan löydös? Toisaalta, tämä ``tärkeä mittari'' on jo
aikaisemmin jossain minun ja jonkun muun kanssa käydyissä keskusteluissa
todettu huonoksi termiksi. Koska tärkeä miltä kannalta? Ei voi olla vain
'tärkeä' ilman kontekstia. Vai pitäisikö sanoa ``mittareita, jotka nähtiin
tärkeiksi''? Sama asia kai. Liittyy edellisiin, niputa yhteen. Kato listaa, ja
tuleeko siitä jotain mieleen. onko niillä mittareilla jotain yhteistä? Mitkä
ei anna arvoa? Minkä asian mittaaminen on tärkeä? Mitä asioita tärkeät
mittarit mittaa? Mitä tästä voi päätellä? Finding 3 ja 5 samanolosia, yhdistä.
Capers: ``Most successful companies measure software productivity and quality
accurately. They plan and estimate software projects accurately''

\end{comment}

Finding 3: This study describes important metrics. There are some
characteristics of important metrics that can be pointed out: such as ease of
use and that it utilises existing tools. Also, based on the effects of metric
use, it seems that the ability to provoke discussion is a characteristic for important
metrics.\fixme{varmaan esimerkkejä tähänkin}


According to \citetsrc{jones2008applied}, software
productivity and quality are measured accurately by most successful software
companies. Also, they plan and estimate software projects accurately.





\begin{comment}
Mittaroinnin syyt vs mitä muualla sanottu mittaamisen syyksi.
Nopeasti arvioitunu on paljon yhtäläistä. Pitäisi ehkä tutkia tarkemmin onko jotain
syitä mitä kirjallisuudessa ei ole esitetty. Se, että joka tapauksessa on
paljon samaa, tarkoittaisi sitä, että mittaroinnin tarkoitus ei juuri muutu,
vaikka kehitysmetodi muuttuisikin. Tutki nuo erot.
\end{comment}

Finding 4: The categories for reasons and effects for metric use can be seen
in \fixme{taulukot}. This answers the question why metrics are used in industrial
agile development. If we then compare how these found reasons compare to
reasons mentioned by others. Found category 'Planning' is found in other
literature as Project estimation (grady), Improved project planning (ami).
'Progress tracking' is found as Progress monitoring (grady), improved project
communication and management of projects (grady). 'Understand and improve
quality' is found as ``Measurement is necessary for quality control and
assurance (Fenton, horst) and ``Measurement is important for prediction. We
want to predict products and processes at the stages of the software
life-cycle. (Fenton, horst)'' in terms of predicting post-release quality
based on pre-release quality (defects deferred). ``Identify problems is found
as ``Process improvement through failure analysis'' (grady).

As a conclusion with respect to the findings
is a similar quote from Capers: ``The goal of applied software measurement is
to give software managers and professionals a set of useful, tangible data
points for sizing, estimating, managing, and controlling software projects
with rigor and precision. Estimating could be seen as ``Planning actions'',
managing and controlling could be seen as ``Reactive actions''. Researcher was
not able to find similar reasons or effects as ``Motivate people'' would
suggest. Maybe this is something more unique to agile development. Maybe the
development is more centered on the people.

There were also reasons that were not found in this study, such as
``Experimental validation of best practices'' (grady), Alignment of software
development to business objectives'' (ami), and `` The software development
process should be designed by measurable objectives, which leads to a precise
definition of software quality attributes.'' (Fenton, horst). These pointers
seemed to point out from the core activities in agile development,
implementation and testing.



\begin{comment}
UNOHDA TÄÄ FOR NOW: Finding 5:The use of code metrics: quite low, but there
are some:
tech debt, violations. Kitchenham:n tutkimuksessa näkyi paljon koodimittareita, näissä ei
hirveesti. Onko niiden käyttö kuitenkin niin hankalaa ettei niitä siksi
käytetä? vai eikö niiden käyttöä vain raportoida? Toisaalta se yks icse
revieweri sano, että kyllä niitä käytetään koska myynnissä on sellaisia
työkaluja tjms. Laske prioriteettia, koska ei voi hirveesti sanoa. Voi olla
että mittarit on käytössä koodareilla mutta ei tietoa mihin käytetään.
\end{comment}


Finding 5: Prosesseja mittaavat mittarit mittaavat pääsiassa vain
implementointia, testausta ja koko devausprosessia. Muita vaiheita ei
juurikaan mitata. Miksi ei? Ehkä agilessa keskitytään enemmän itse tekemiseen?
Ei kuulosta vakuuttavalta. Ehkä kuulostaakin. Keskitytään agile principlesin
mukaisesti tekemiseen, määrittelyn mittaamisen. Muut prosessit ei ehkä niin
näkyviä? Voi olla että muita prosessivaiheita ei ole erotettu agiilissa. Voi
reflektoida prinsiippejä.


Finding 6: Lähes kaikki mittarit mittaavat asioita 'sisäisesti', vain valmista
tuotetta mitataan ulkoisesti, eli sen käyttäytymisen perusteella. (Fentonin
mukaan ulkoinen mittaus haastavaa, yleensä voidaan tehdä vain devauksen
loppuvaiheessa. Sisäisellä mittauksella ennustetaan ulkoista. Managerit
arvostavat enemmän ulkoisia mittareita.) 2005 agiili testaus Itkonen.
Agiilissa uskotaan että sisäiset käytännöt tuottavat suoraan ulkoista laatua.
Tää sisäinen ja ulkoinen ei olekaan niin simppelijuttu. Mitäs ne nyt
tarkottaa? Pitää pystyä määrittelemään selkeästi että voi mitään sanoa.
(Fundamental Aspects of Software Measurement, selittää jotain external ja
internalista))


Finding 7: Resurssiluokan entiteettejä ei juurikaan mitata. Miksi ei?
Ketterässä keskitytään tuotteeseen, eikä sen tekijöihin? Ehkä oletetaan, että
on pystyvä joukko tekijöitä? Agiilissa ajatellaan että on yksinkertainen
projekti, eikä muut. Beam ja turner, 200234, software. Balancing agile and
discipline. ``Mitä enemmän porukkaa joka osaa soveltaa agiilia?'' Ihmisten
mittaamista ei nähdä tärkeänä. 


Finding 8: Mittareiden sijoittumisesta menetelmiin: velocityä käytettiin
scrum -ja xp keisseissä. Unit testeja mitattu lähes ainoastaan XP:ssä. Mitä nää
meinaa? Suht odotettua. Eli näyttää, että teollisuus käyttää
kirjallisuudessa ehdotettuja mittareita. Kanban ja LeanSD:ssä
esitettyjä mittareita ei ollut juurikaan, mutta ehkä sitä selittää niiden
pieni osuus setissä. Toisaalta oli iso osa pala sellasia joita ei oltu
ehdotettu. Näitä täydennetään sitten muilla mittareilla.




\textbf{Research Question 1:} \emph{What metrics are used in
  industrial agile software development?}
  
Code metrics were not described alot. Compared to Kitchenham's study that
found many of those. Cause might be that they were not industrial studies.



  
 \textbf{Research Question 1.1:} \emph{What are the entities measured
  in industrial agile software development?}
  
  Entäs mittauksen kohde? Fentonin kategorisoinnista sai siis selville mitä
kohdetta mitataan. Pitäisikö löytää kirjallisuudesta joku vastaava
tietopläjäys, vai voinko sanoa et tää on nyt uutta tietoa siitä et mitä
kohteita mitataan ketterässä. Entä mitä tämä tarkoittaa? pitäis ehkä kattoa
uudestaan sitä taulukkoa ja mun kirjoittamia tuloksia, jos se herättäis jotain
lisää ajatuksia.



The metric categorization by \citetsrc{fenton1998software} in
\Cref{tab:metricsCategorization} focuses on identifying the target entities
for measurement. However, the categorization presented in
\cref{sec:WhyMetricsUsed} and in \cref{sec:effects} focus on the reasons and
effects of metric use. Maybe it is better to understand why metrics are used
instead of knowing the target of measurement. Furthermore, it is important to
understand the consequences of using a metric because the effects of using
the metric could be dysfunctional as well.
  
 \textbf{Research Question 1.2:} \emph{How do the metrics found in
 industrial agile software development compare to those suggested by agile literature?}
 
 Entäs se vertailu? Se että ~40 prossaa mittareista ei näy ketterässä
kirjallisuudessa vois viitata joko siihen, että ne projektit ei ollutkaan
ketteriä, tai et ketterät projektit käyttää silti vielä muitakin mittareita.
En usko et tän tarkemmalle tasolla tarvitsee mennä tässä aiheessa.
 
\textbf{Research Question 2:} \emph{Why are metrics used in industrial
  agile software development?} 
  
  Tässä vielä vähän omaa ajatuksenjuoksua: motivationia en oo nähny muissa
papereissa. Paljon oli samoja syitä mittareiden käytölle kuin 'jo muinaisissa'
mittarikirjoissa. Vertaa vaikka introssa vs mun kategoriat, vois yrittää vielä
lisää tunnistaa mun kategorioista sellasia mitä ei muualla ole löytynyt. Jos
on paljon samaa, se viittaisi siihen että mittaamisen tarkoitus ei hirveästi
ole muuttunut menetelmän vaihtumisen vuoksi. Ja jos on muuttunut, niin mihin
suuntaan?
  
  
  Found metrics could be also categorized based on the temporal use the metric.
There were some clearly reactive metrics that were used to provide immediate
improvement, e.g., motivate people to fix the build faster. On the other hand
there were metrics that were aimed more towards long-term process improvement,
e.g., many of the cases from Ericsson. However, many of these cases seemed to
be very pilot-like, as in some metric based process improvement approach was
piloted in the organization. In many cases these metrics seemed heavy and not
likely to be applied after the pilot.
  
 \textbf{Research Question 3:} \emph{What are the effects of  metric
  use in industrial agile software development?} 
  
  
 \textbf{Research Question 4:} \emph{What metrics are important in
  industrial agile software development?}

Based on the results for important metrics in \cref{sec:importantMetrics},
some characteristics of important metrics can be highlighted. It seems
industrial agile teams appreciated metrics that were easy to use, highly
visual and they had the ability to facilitate discussion on problems and
possible improvements. Also, it seems that customer satisfaction metrics are
important for the success of a project. Furthermore, effort estimation and
velocity metrics were described as important and they were also identified
from many primary studies.

Based on the results of this study an average profile for metric use of an
industrial agile team could be described as follows. They estimate effort for
tasks and they follow their progress using velocity. They measure pre-release
quality with number of defects and they are interested in customer
satisfaction with some metric. Additionally, they have some customized
metrics that are needed to solve immediate problems, e.g., they measure
the amount of time it takes to fix a build.





Mitähän nuo muut tutkimuskyssärit sit antais? Tuntuu et monet vaatis sitä et
lukis sikana muita papereita et vois sit verrata tuloksia niihin, koska nyt
mulla on vaan tulokset. Vaikea verrata kun ei tiedä mitään muuta\ldots Kai
tää nimenomaan osoittaa sitä tutkimusalueen hallintaa. prkl.





Vois myös motivoida lisää et miks oon tehny agile mappingin. Siitä on kauan
kun tähän ryhdyin. Mut muistaakseni huomasin, että jotkut syistä ja
vaikutuksista tuntui noudattavan agile prinsiippejä. Sitten Mika tai Juha sano
et pitäis kaikki prinsiipit sit käydä läpi. Mut mitä tämmönen mapping sit
oikeen antaa ulos? Ilmeisesti mäppäyksellä voisi mitata kuinka agiilia
mittarin käyttö on - eli tietynlainen instrumentti. Näitä ``Kuinka agiili
projektisi on?'' mittareitahan on vaikka kuinka, olisko tämä yksi lisää
siihen? TArvitseeko niitä olla lisää?

Tärkeistä mittareista mulla on jotain. Ehkä noi discussioinit pitäis vaan
jakaa tutkimuskysymyksien mukaan niin, ehkä se jäsentäis tätä paremmin.




%\section{Implications for practice}
\section{Mapping metric use to agile principles}
To provide implications to practice the findings are mapped to the principles
of agile software development \citetsrc{beck2001agile} categorized by
\citetsrc{1579312}. For each paragraph the naming by Patel et al. is used and
references to the principles of agile software development is provided by
numbers.

Communication and Collaboration (4th and 6th agile principles
\citepsrc{beck2001agile}) was reflected by metrics
providing basis for discussion. Value Stream Maps and number of bounce backs
initiated root cause analysis meetings [S3,S17]. Moreover, metrics were
analysed in reflection meeting where problem and improvement were identified
[S28]. Furthermore, technical debt board provided visibility on technical debt
issues and it helped to create discussion to decrease technical debt [S4].

Team involvement (5,8) was reflected in metrics that motivated team to act and
improve, see \cref{sec:motivatePeople}. Also, to promote sustainable
development metrics were targeted to balance the flow of work, see
\cref{sec:balanceWorkflow}.

Reflection (12) was visible in metrics that were used to identify problems,
see \cref{sec:identifyProblems}. Furthermore, metrics helped to find
improvement ideas, see \cref{sec:createImprovementIdeas}.

Frequent delivery of working software (1,3,7) was directly identified in one
of the studies, where the team measured progress by demonstrating the product
to the customer [S30]. Additionally, there were cases where, e.g.,
completed web-pages [S12] were the primary progress measure. Also, many
metrics focused on progress tracking, see \cref{sec:projectProgress}), and
timely completion of project goals, see \cref{sec:accomplishProjectGoals}. However,
some other measures from \cref{sec:projectProgress} show that instead of
working code agile teams followed completed tasks and velocity metrics.
%\juha{haluaisin jotain tämänkaltaista keskustelua suorista laatumittareista,
% mutta voiko näin sanoa tämän tutkimuksen perusteella. Eetu, etenkin tuo
% viimeinen virke, onko linjassa sinun mielestäsi??}\eetu{Nyt muokattuna voi
% sanoa.}

An integral part of the concept of working software is measuring post-release
quality, see  \cref{sec:understandQuality}. This was measured by customer
satisfaction, feedback, and customer defect reports. It was also common to use
pre-release data to predict post-release quality. Agile developers tend to
measure the end product quality with customer based metrics instead of the
traditional quality models, such as ISO/IEC 25010 \citepsrc{10951538}.

Managing Changing Requirements (2) was seen in the metrics that support
prioritization of features, see
\cref{sec:planning} and \cref{sec:planningActions}. This allowed the rapid
development of features important for the customer's business at a given time.
Also, metrics like technical debt board provided better a better codebase for
further development.

 %Additionally, different metrics helped keeping
% the internal quality of the product high throughout the development which
% then provided safe development of modifications from new ideas, see
% \cref{sec:PreQuality}.

Design (9,10,11) was directly seen in focus to measuring technical debt and
using metrics to enforce writing tests before actual code, see
\cref{sec:ensureTesting}. Additionally, the status of the build was continuously
monitored, see \cref{sec:increaseQuality}. However, the use of velocity
metric had a negative effect on technical quality, see
\cref{sec:motivatePeople}.
% was seen from different perspectives: on one hand metrics focused on
% problem/waste identification, see \cref{sec:ProblemIdentification},
Many metrics focused on making sure that the right features were selected for
implementation, see \cref{sec:planning}, thus avoiding unnecessary
work. Moreover, metrics were used to identify waste (processes where no value
is added to the product), see \cref{sec:identifyProblems}.



%\eetu{Mun mielestä seuraavat mittarit ei oo niin ketteriä, mut en osaa oikeen
%perustella miksi tai sanoa mitä periaatteita vastaan ne olisi: maintenance
%effort, cost types, defect amounts(?), defects deferred, revenue per
%customer(?), time to establish project foundation, test coverage, test growth
%ratio, cost performance index, schedule performance index. Näitä ei kaikkia
%ole myöskään kuvattu resultseissa, paitsi taulukossa mainittu.}

There were also metrics, or their usage, which were not agile in nature. For example,
maintaining velocity by cutting corners in quality instead of dropping
features from that iteration [S6]. Also, adding people to a project to reach a
certain date [S5, S17] does not seem that agile compared to removing tasks.
Furthermore, adding people can have a negative impact to progress, considering
the lack of knowledge and training time required.
% Moreover, the use of dates to plan interdependent tasks is not agile in
% nature [S12].Instead, interdependencies should be visible inchoosing the
% tasks to appropriate iterations.
Moreover, the use of number of defects to delay a release [S10] is against
agile thinking as one should rather decrease the scope to avoid such a
situation. Furthermore, developers at Avaya used effort estimates to predict
the iteration where a feature would be completed [S29], contradicting the
idea of completing a feature within an iteration.

%While the flow metrics Ericsson have a good target of balancing workflow,
% they seem  (or at least they are presented) complicated to use---meaning that one
%might need considerable effort to generate and analyse the metrics, which
%doesn't fit to the light-weightness of agile.

%\juha{muotoilisin tämän hieman toisin}
% Contradictory to fifth principle, Talby et al [viite] enforce writing
% automated test cases as a measure of progress - so in a way they didn't
% trust the developers to write the test on their own? Similarly, [viite]
% measured the status of the build to make developers fix the build faster -
% again, not trusting them to do it on their own.
Some agile metrics that work well for an agile team, such as tracking progress
by automated tests [S28], or measuring the status of the build
[S14] can turn against the agile principles if used as an external
controlling mechanism. The fifth agile principle requires trust in the team,
but if the metrics are enforced outside of the team, e.g., from upper
management there is a risk that the metrics turn into control mechanisms and
the benefits for the team itself suffer.






\begin{comment}
Löytyi selkeästi reaktiivisia, välittömiä mittareita joilla haluttiin
välittömiä tuloksia, esim. buildin tila ja buildin tilaan liittyvä
korjausaika. Sitten taas toisaalta oli pilottimaisia tutkimuksia, jossa
esitettiin pilottimaisia mittareita, jotka yleensä olivat tai näyttivät olleen
vaikeampi implementoida niiden monimutkaisuuden vuoksi (Ericsson paperit),
jotka yleensä muistuttivat LeanSD:tä.
Jonkin verran näkyin myös focusta asiakastyytyväisyyden mittaamiseen eri
tavoilla[luettele mittareita]. Velocity ja effort estimaattimittareita oli
paljon. Et tietyllä tavalla semmonen keskimääräinen projekti estimoi taskeja,
seuraa burndownia, mittaa jollain tavalla sisäistä laatua esim bugimäärien
avulla ja on kiinnostunut asiakastyytyväisyydestä jollai mittarilla. Sen
lisäksi joitain mittareita luodaan tiettyyn tarpeeseen tai ongelmaan, esim.
building korjausaika lyhyemmäksi, tai tekninen velka pienemmäksi, tai
implementoinnin flow tietyksi. Tietyllä tavalla mittarien räätälöimistä, tätä
ilmeisesti myös suositeltu dymondin artikkelissa.

Future research, mittareiden räätälöinti, agiili asiakastyytyväisyys, ?

\end{comment}

%\subsection{Implications for research}
%It was interesting to notice that there wasn't many code metrics, only the
%ones mentioned in \cite{S14} even though we feel there are many
%% studies regarding the benefits of code metrics. Maybe there are some
% practical
%problems implementing and analysing the data from code metrics?

%How to measure unmeasured agile principles...

%Effort estimates are prerequisite for velocity metrics, and since velocity
%metrics were vastly identified in our study, e

%In general, we think there were many metrics that were targeted for the team
% - instead of high focus on managerial or upper management reporting metrics.
%Making metrics visible for the team enables them to independently act and
%improve without the need of rapid supervision and telling people what to do.



\section{Comparison to prior studies}
Only few papers have broadly studied the reasons for software metrics use in
the context of agile software development. \citetsrc{1667571} highlight
process improvement as one of the reasons for measurement in their agile
metrics paper. Also, they emphasize that creation of value should be the
primary measure of progress - which was also seen in this study. Moreover,
they differentiate between organizational long-term 'metrics' from short-term
context driven 'diagnostics'. Both types of metrics were also seen in this
study. However, a key metric for business value a team should define with
business unit, proposed by \citetsrc{1667571}, was not seen in this study.
Furthermore, \citetsrc{1667571} do not provide any specific agile metrics but
rather describes how agile metrics should be chosen and how they should be introduced
to the organization. Also, they provide a set of heuristics for agile metrics.

\citetsrc{Korhonen2009} found in her study that traditional defect
metrics could be reused in agile context - if modified. Defect metrics were
also used in many of the primary studies.

Kitchenham's mapping study \citepsrc{kitchenham_whats_2010} identified several
code metrics in academic literature. However, in this study almost no evidence
of code metric use in the industrial agile context was found. Maybe agile
practitioners consider code metrics self-evident and do not report them, or
maybe code metrics are not widely used by agile industrial teams.

%Maybe it is time to
%re-evaluate the need for code metrics research if industry doesn't seem to
% use them.

%The reasons for the lack of code metric usage in agile
%contexts should be studied to evaluate the necessity of code metric research
% - or how code metric research could be modified to support agile development

%The lone case in our study where code metrics were used, the code
%metric usage was abstracted to a build tool, which would just indicate an
%error or broken build \cite{S14}. Maybe the use of code metrics should
%be heavily implemented through automated tools that handle the collection and
%analysing of code metric data?

\section{Limitations}

The large shares of specific application domains in the primary documents are
a threat to external validity. Seven out of 30 studies were from enterprise
information systems domain and especially strong was also the share of ten
telecom industry studies out of which eight were from the same company,
Ericsson. Also, Israeli Air Force was the case organization in three studies.

The threats to reliability in this research include mainly issues related to
the reliability of primary study selection and data extraction. The main
threat to reliability was having a single researcher performing the study
selection and data extraction. It is possible that researcher bias could have
had an effect on the results. This threat was mitigated by analysing the
reliability of both study selection and data extraction as described in
\cref{sec:Method}. Also, another threat to reliability is the chosen research
method, SLR. There is a great deal of industrial metric use in agile teams
that is not reported in scientific literature. So choosing another research
method, e.g., a survey targeted to companies practicing agile methods
could have produced different results.

%Sometimes it was hard to understand which metrics an author was referring
%when a ``why'' was described. Moreover, we had to sometimes assume that when
%author describes the reasons for using a tool, he would be actually talking
%about the metrics the tool shows.

Due to iterative nature of the coding process, it was challenging to make sure
that all previously coded primary documents would get the same treatment,
whenever new codes were discovered. In addition, the researcher's coding
'sense' developed over time, so it is possible that data extraction accuracy
improved during the analysis. In order to mitigate these risks a pilot study
was conducted to improve the coding scheme, get familiar with the research
method and refine the method and tools.

%First author has positive mindset towards agile methods, as well as towards
%certain metrics over others.

Some data from low scoring papers, e.g [S3], are not explained very
detailed, which could have caused incorrect interpretations. Also, only the
researcher conducted the quality evaluation, which could have an impact on the
actual quality scores.

Deciding which agile method was used in the cases was difficult. But on the
other hand it is quite natural that cases use many aspects from multiple agile
methods.

\chapter{Conclusions}%\juha{Kirjoitin tänne hieman lisää...} 
\label{sec:Conclusions}

This study presents the results from a systematic literature review of 30
primary studies. According to the researcher's knowledge there are no previous
systematic reviews of metric use in the context of industrial agile software
development. This study categorizes metrics found from empirical agile studies
and compares the found metrics to the metrics suggested by agile literature.
Moreover, this study provides descriptions of why metrics are used to support
agile software development. Similarly, this study describes the effects
metrics have for agile software development. Furthermore, this study
identifies important metrics based on statements and amount of evidence.
This study also analyzed how the presented metrics support the twelve
principles of Agile Manifesto \citepsrc{beck2001agile}.

The results indicate that the reasons for the use metrics are focused on the
following areas:
\nameref{sec:planning}, \nameref{sec:progressTracking},
\nameref{sec:quality}, and \nameref{sec:identifyProblems}. Similarly, the
effects of metric use are focused on the following areas:
\nameref{sec:planningActions}, \nameref{sec:reactiveActions},
\nameref{sec:motivatePeople} and \nameref{sec:createImprovementIdeas}.

This paper provides researchers and practitioners with an overview of
the metric use in agile context and documented reasonings and effects behind
the proposed metrics. This study can be used as a source of relevant studies
regarding researchers' interests and contexts.

Finally, this study identified few propositions for future research on
measuring in agile software development. First, in the academia lot of
emphasis has been given to code metrics yet this study found little evidence
of their use in agile context. Second, the applicable quality metrics for
agile development and the relationship of pre-release quality metrics and
post-release quality are important directions of future research. Third, this
study found that planning and tracking metrics for iteration were often used
indicating a need to focus future research efforts on these areas. Fourth, use
of metrics for motivating and enforcing process improvements can be an
interesting future research topic. Fifth, the use of customized metrics and
the use of customer satisfaction metrics in industrial agile context can be an
interesting future research topic. Finally, the dysfunctional use of metrics
and the negative motivation from metric use can be interesting future research
topics.
