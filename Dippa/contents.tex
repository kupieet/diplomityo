\chapter{Introduction}
\label{chapter:intro}
 % Software engineering is at a crossroads as there are new leaner and more
% agile software development methods appearing next to the traditional
% software development methods.
% \mika{Kappaleen pointti: No literature reviews of actual metric use}
Software metrics have been studied for decades and several literature reviews
have been published.
Yet, the literature reviews have been written from an academic viewpoint that
typically focuses on the effectiveness of a single metric. For example, Catal
et al. review fault prediction metrics \citepsrc{catal2009systematic},
\citetsrc{purao2003product} review metrics for object oriented systems and
\citetsrc{kitchenham_whats_2010} performs a mapping of most cited software
metrics papers. According to the researcher's knowledge there are no
systematic literature reviews on the actual use of software metrics in the
industry.

%\mika{Kappaleen pointti: Agile on tärkeää eikä metriikkoja tutkittu}

% \juha{Yritin konkretisoida trad vs. agile kontrastia}
Agile software development is becoming increasingly popular in the software
industry. The agile approach seems to be contradicting with the traditional
metrics approaches. For example, the agile emphasizes working software over
measuring progress in terms of intermediate products or documentation, and
embracing the change invalidates the traditional approach of tracking progress
against pre-made plan.
However, at the same time agile software development highlights some measures
that should be used, e.g., burndown graphs and 100\% automated unit testing
coverage. However, measurement research in the context of agile methods
remains scarce.

The goal of this paper is to review the literature of actual use of software
metrics in the context of agile software development. This study will lay out
the current state of metrics usage in industrial agile software development
based on literature.
Moreover, the study uncovers the reasons for metric usage as well as
highlights actions that the use of metrics can trigger.
%Due to our research
%goal, we focus this paper on case studies and actual empirical findings
%excluding theoretical discussion and models lacking empirical validation.

This study covers the following research questions: 

\begin{itemize}
  \item RQ1: Why are metrics used?
  \item RQ2: What actions do the use of metrics trigger?
  \item RQ3: Which metrics are used?
  \item RQ4: Which metrics are important?
\end{itemize} 

%\section{Problem statement}

\section{Structure of the Thesis}
\label{section:structure} 

This thesis is structured as follows. \Cref{sec:Method} describes how
the SR was conducted. \Cref{sec:Results} reports the results from the study.
\Cref{sec:Discussion} discusses about the findings and how they map to agile
principles. \Cref{sec:Conclusions} concludes the paper. \fixme{Final
structure}

\chapter{Background}

\section{Agile software development}
%\eetu{tässä voisi olla historianäkökulma, voipi mennä pitkäks eikä niin
%mielenkiintoinen ehkä tässä paperissa} 

Agile development methods have emerged to the software world ruled by
traditional heavyweight methods. In agile methods the focus is in lightweight
working practices, constant deliveries and customer collaboration over long
planning periods, heavy documentation and inflexible development phases.

Agile manifesto created by agile enthusiasts \citepsrc{beck2001agile} lists
agile principles that give an idea what is agile development about. Popular agile
development methods include Scrum \citepsrc{schwaber2002agile}, Extreme
Programming \citepsrc{beck2004extreme} and Kanban
\citepsrc{anderson2010kanban}.

%\section{Software measurement}
%Measurements for software industry started from a technical point of view.

\section{Related work}

\citetsrc{1667571} don't provide any specific agile metrics but rather
describes how agile metrics should be chosen and how they should be introduced
to the organization. Also, they provide a set of heuristics for agile metrics.


\fixme{tää jäi vähän laihaks}

\section{Systematic literature review}

Systematic literature review is a research method originated from the field of
medicine \fixme{viite}. The overarching idea is to aggregate and synthetisize
existing knowledge regarding a research topic. This rigorous and audible
evaluation method can facilitate theory development and point out gaps in
research \citepsrc{webster2002analyzing}.

\section{Causes for software project failures}


\section{Evidence based software engineering}

%\subsection{Measurement}
%According to Fenton et al.\cite{fenton1998software} ``Measurement is the
%process by which numbers of symbols are assigned to attributes of entities in
%the real world in such way as to describe them according to clearly defined
%rules.''


\section{Previous metric research}

%There are a few mapping studies on software metrics(tähän vois lisätä ne
% kitch whats 2010:ssä olevat muut mut en tiiä mitä lisäarvoa ne tois).
%\cite{kitchenham_whats_2010} says there is a large body of research related
% to software metrics. However, she highlights that all evidence should be
%critically appraised so that further studies can be based on good quality
%evidence. She also reminds researchers to understand the context where
% metrics are taken from - failure to understand context will probably not provide
%answers to industry-related questions. 

%--EBSE
%--Mittaamisen SR:t
%--Agile mittaamisen muut tutkimukset vai enemmänkin tutkimuksessa käytettyjen
%termien ja käsitteiden selittäminen


%\section{Aims and research questions} 
%The aim of this paper is to provide
% preliminary results from a systematic review (SR) on agile metrics.
%Moreover, we are interested on the industrial use of metrics in agile
% context.

\chapter{Review method}
\label{sec:Method}
Systematic review (SR) was chosen as research method because the study is more
about trying to understand a problem instead of trying to find a solution to
it. Also, there was already existing literature that could be synthesized.

\section{Protocol development}
Kitchenham's guide for SRs \citepsrc{kitchenham2004procedures} was used as a
basis for developing the review protocol. Additionally, a SR on agile
development \citepsrc{dyba_empirical_2008} and a SR on SR
\citepsrc{kitchenham2013systematic} were used to further understand the
challenges and opportunities of SRs. The protocol was also iterated in weekly
meetings with the instructors, as well as in a pilot study.

%otherguidelines \cite{webster2002analyzing}, a lessons learned from SRs
% \cite{brereton2007lessons},
\section{Search and selection process}

The strategy for finding primary studies was following:

\begin{itemize}
  \item Stage 1: Automated search
  \item Stage 2: Selection based on title and abstract
  \item Stage 3: Selection based on full text. Conduct data
  extraction and quality assessment.
\end{itemize}

\Cref{SelectionFunnel} shows the selection funnel in terms of the number
of papers after each stage.

%\includegraphics{SelectionFunnel.jpg}

\begin{table}
\centering
\caption{Paper selection funnel}
\begin{tabular}{lcr} \hline
\label{SelectionFunnel}
Stage & Amount of papers \\ \hline
Stage 1 & 774 \\  
Stage 2 & 163 \\ 
Stage 3 & 31\\
\hline
\end{tabular}
\end{table}

% \subsubsection{Search strings}
Scopus database \footnote{http://www.scopus.com} was used to find the primary
documents with automated search. Keywords include popular agile development
methods and synonyms for the word metric. The search was improved
incrementally in three phases because some key papers and XP
conferences were not found initially. The search strings, hits and dates can
be found from \cref{app:Strings}.

%\juha{Laittaisin myös inclusion ja exclusion kriteerit appendixiin ja
% jättäisin vain tämänkaltaisen lyhyen kuvauksen tänne} 
The selection of the primary documents was based on inclusion criteria:
\emph{papers that present empirical findings on the industrial use and
experiences of metrics in agile context.} The papers were excluded based on
multiple criteria, mainly due to not conforming to requirements regarding
empirical findings and agile and industrial context. Full criteria are listed
in \cref{app:Criteria}.

%\juha{Inclusion criteria ja exclusion criteria siirretty -> appendix}
%\subsubsection{Inclusion criteria} \juha{->APPENDIX}
%
%\begin{itemize}
%  \item Papers that present the use and experiences of metrics in an agile
%  industry setting.
%\end{itemize}
%
%\subsubsection{Exclusion criteria} \juha{->APPENDIX}
%
%\begin{itemize}
%  \item Papers that don't contain empirical data from industry cases.
%  \item Papers that are not in English.
%  \item Papers that don't have agile context. There is evidence of
%  clearly non-agile practices or there is no agile method named. For example,
%  paper mentions agile but case company has only three releases per year.
%  \item Paper is only about one agile practice, which is not related to
%  measuring.
%  \item Papers that don't seem to have any data about metric usage. Similarly,
%  if there are only a few descriptions of metrics but no other info regarding
%  reasons or usage.
%  \item Papers that have serious issues with grammar or vocabulary and
%  therefore it takes considerable effort to understand sentences.
%  \item Papers that refer to another paper where the actual case is discussed.
%  \item Papers that are in academic or semi-academic setting - customer or
%  part of workers are from industry. The reason for this is that it doesn't
%  fully represent industry setting as software development methods are likely
%  enforced by academia.
%  \item Papers where results cannot be separated by setting, for example
%  surveys where there is data both from academia and industry. Similar
%  exclusion if results cannot be separated by software development method. 
%  \item Papers where the setting is not clear. For example only mention of
%  context is ``100 junior developers''.
%  \item Papers that are full conference proceedings. Individual papers should
%  be already listed separately.
%  \item Papers where the measurements are only used for the research. For
%  example author measures which agile practices correlate with success.
%  \item Papers that don't even show measurement usage in a pilot setting. For
%  example method or metric is used against static industrial data set.
%  %\item Papers that are about the same case, from the same author and same
%  %research focus, basically the paper format has just changed slightly.
%\end{itemize}


%\subsubsection{Stage 1 - Automatic search}

In stage 1, Scopus was used as the only search engine as it contained the most
relevant databases IEEE and ACM. Also, it was able to find Agile and XP conference
papers. Only XP Conference 2013 was searched manually because it couldn't be
found through Scopus.

%\subsubsection{Pilot}
%\label{pilot}
%We conducted a pilot study in order to refine the aim of the research and get
%familiar with the research method. Moreover, it was possible to modify the
%method and tools.

%15 papers were selected for the pilot; 5 by relevance, 5 by number of
%citations, and 5 by random selection.
% \begin{itemize} \item 5 by top relevance \item 5 by top citations \item 5 by
% random \end{itemize}
%\juha{tämä mahdollista poistaa (ainakin omana alilukuna) jos tila ei
% riitä}The pilot resulted in changing citation manager tool from Zotero to Jabref.
%Also, selection by title and selection by abstract steps were joined
% together.
%The quality assessment checklist was decided based on the pilot results.

%\subsubsection{Stage 2 - Selection by title and abstract}

In stage 2, papers were included and excluded by the researcher based on their
title and abstract. As the quality of abstracts can be poor in computer
science \citepsrc{kitchenham2004procedures}, full texts were also skimmed
through in case of unclear abstracts.
% - especially introduction, case description and conclusions were checked
% briefly.
Unclear cases were discussed with the instructors in weekly meetings and an
exclusion rule was documented if necessary.

The validity of the selection process was analysed by performing the selection
for a random sample of 26 papers also by the second instructor. The level of
agreement was 'substantial' with Kappa 0.67
\citepsrc{landis_measurement_1977}.

%\subsubsection{Stage 3 - Selection by full text}

Stage 3 included multiple activities in one work flow. Selection by full text
was done, data was coded and quality assessment was done. Once again, if there
were unclear papers, they were discussed in meetings. Also, selection of 7
papers was conducted by the second instructor with an 'almost perfect'
agreement, Kappa 1.0 \citepsrc{landis_measurement_1977}.

\section{Data extraction}

Integrated coding was selected for data extraction strategy
\citepsrc{6092576}.
It provided focus to research questions but flexibility regarding findings.
Deductive coding would have been too restraining and inductive coding might
have caused too much bias. Integrated coding made it possible to create a
sample list of code categories:

\begin{itemize}
  \item Why is measurement used?
  \item How is measurement used?
  \item Metric
  \item Importance related to metric
  \item Context
\end{itemize}

The coding started with the researcher reading the full text and marking
interesting quotes with a temporary code. After, reading the full text the
researcher checked each quote and coded again with an appropriate code based
on the built understanding. In weekly meetings with the instructors, a rule
set for collecting metrics was slowly built:

\begin{itemize}
  \item Collect metric if team or company uses it.
  \item Collect metric only if something is said about why it is used, what
  actions it causes or if it is described as important.
  \item Don't collect metrics that are only used for the comparison and
  selection of development methods. 
  \item Don't collect metrics that are primarily used to compare teams. 
\end{itemize}

Atlas.ti Visual QDA (Qualitative Data Analysis), version 7.1.x was used to
collect and synthesize the qualitative data. Amount of found quotes per code
can be seen in \Cref{tab:quotes}.

To evaluate the repeatability of finding the same metrics, second instructor
coded metrics from three papers. Capture-recapture method
\citepsrc{seber2002estimation} was then used which showed that 90\% of metrics
were found.

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \label{tab:quotes}
  \caption{Amount of found quotes}
    \begin{tabular}{rr}
    \toprule
    Code  & Amount of quotations \\
    \midrule
    Why use this metric? & 151 \\
    How is measurement used? & 61 \\
    Metrics & 108 \\
    Importance related to metric & 45 \\
    Context & 158 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

A quality assessment form adopted from \citepsrc{dyba_empirical_2008} was used
to evaluate the quality of each primary study. Detailed list of quality
assessment questions can be found in \cref{app:quality}. Additionally, a
relevancy factor was added to the same assessment to describe how useful the
paper was for this study. The scale for the relevancy factor is:

\begin{itemize}
  \item 0 = doesn't contain any information regarding metrics and should be
  already excluded
  \item 1 = only descriptions of metrics with no additional info
  \item 2 = some useful information related to metrics
  \item 3 = a good amount of relevant information regarding metrics and metric
  usage
\end{itemize}

\section{Data synthesis}
Data synthesis followed the steps recommended by \citetsrc{6092576}.
Process started by going through all quotes within one code and giving each quote a
more descriptive code describing the quote in high level. Then the descriptive
codes were organized in groups based on their similarity. These groups were
then given high level codes which are seen as categories in
\Cref{tab:whyCategories} and \Cref{tab:howCategories}.

\chapter{Results} % - Why and how are metrics used
\label{sec:Results}

This chapter presents the results from the systematic literature 
review. \Cref{sec:overviewOfStudies} describes the
overview of studies and results from the quality evaluation.
\Cref{sec:metricsCategorization} categorizes found metrics according to
categorization by \citetsrc{fenton1998software}. \Cref{sec:WhyMetricsUsed}
describes the reasons for using metrics. \Cref{sec:effects} describes the
effects the use of metrics have. \Cref{sec:importantMetrics} describes
important metrics by statements from the primary studies as well as by amount
of evidence from primary studies.

\section{Overview of studies}
\label{sec:overviewOfStudies}

\Cref{tab:PublicationDistribution} shows the distribution of primary
studies by publication channels. \Cref{tab:overviewOfPdocs} lists the primary
studies by context factors.

Most studies were clearly from Agile
Conference as can be seen from \Cref{tab:PublicationDistribution}. Rest of the
studies have been published in a wide range of journals, conferences and
workshops. 

\begin{table}
\centering
\caption{Publication distribution of primary studies}
\label{tab:PublicationDistribution}
\begin{tabular}{llll}
\hline Publication channel & Type & \# & \%\\
\hline
	Agile Conference & Conference & 10    & 48 \\
    HICCS & Conference & 3     & 14 \\
    ICSE SDG & Workshop & 2     & 10 \\
    XP Conference & Conference & 2     & 10 \\
    Agile Development Conference & Conference & 1     & 5 \\
    APSEC & Conference & 1     & 5 \\
    ASWEC & Conference & 1     & 5 \\
    ECIS  & Conference & 1     & 5 \\
    Elektronika ir Elektrotechnika & Journal & 1     & 5 \\
    Empirical Software Engineering & Journal & 1     & 5 \\
    EUROMICRO & Conference & 1     & 5 \\
    ICSE  & Conference & 1     & 5 \\
    IST   & Journal & 1     & 5 \\
    IJPQM & Journal & 1     & 5 \\
    JSS   & Journal & 1     & 5 \\
    PROFES & Conference & 1     & 5 \\
    Software - Prac. and Exp. & Journal & 1     & 5 \\
    WETSoM & Workshop & 1     & 5 \\
\hline
\end{tabular}
\end{table}

Primary studies and their context information can be seen in
\Cref{tab:overviewOfPdocs}. The earliest study is from 2002 and rest of the
studies are quite evenly distributed from 2002 to 2013. \fixme{agile method
lean sotku -casper} Singlecase was most used research method with 18 studies,
then experience report 8 studies, multicase with 3 studies and finally survey
with 2 studies. Telecom was the most represented domain with 10 cases,
enterprise information systems was the second with 7 cases and web
applications with 4 cases. Other domains, or cases without domain information,
were 16.

\begin{comment}
% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Distribution of research methods}
    \begin{tabular}{rr}
    \toprule
    Research method & Amount \\
    \midrule
    Multicase & 2 \\
    Experience report & 7 \\
    Singlecase & 19 \\
    Survey & 1 \\
    \bottomrule
    \end{tabular}%
  \label{tab:ResearchMethods}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Distribution of agile methods}
    \begin{tabular}{rr}
    \toprule
    Agile method & Amount \\
    \midrule
    Scrum & 15 \\
    XP    & 7 \\
    Lean  & 5 \\
    Other & 5 \\
    \bottomrule
    \end{tabular}%
  \label{tab:AgileMethods}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Distribution of domains}
    \begin{tabular}{rr}
    \toprule
    Domain & Amount \\
    \midrule
    Telecom & 10 \\
    Enterprise information system & 7 \\
    Web application & 4 \\
    Other & 11 \\
    \bottomrule
    \end{tabular}%
  \label{tab:Domains}%
\end{table}%
\end{comment}
%


%\caption{Overview of primary studies}
\begin{longtable}{p{0,5cm}p{1cm}lp{2cm}p{2cm}p{3cm}}
\caption{Overview of primary studies}\\
\hline
\label{tab:overviewOfPdocs}
ID & Year & Resear. meth. & Agile method & Team size & Domain\\
\hline
[S1] & 2010 & Survey & NA & NA & NA\\ \relax
[S2] & 2005 & Experience r. & MSF v4.0\footnote{Microsoft Solutions Framework v4.0} & NA & NA\\\relax
[S3] & 2009 & Multicase &  NA/Scrum/ Scrum & 2-10/2-7/4-8
& ERP/Graphic design plug-in/Facility management\\\relax
[S4] & 2013 & Experience r. & Scrum & 25 teams & Software
for oil and gas industry\\\relax
[S5] & 2005 & Singlecase & XP & 15 & Enterprise information
system\\\relax
[S6] & 2002 & Experience r. & XP & 50 & Enterprise
resource solution for the leasing industry\\\relax
[S7] & 2011 & Survey & Scrum & 26 teams & Desktop and SaaS
products\\\relax
[S8] & 2010 & Experience r. & Scrum & 5-9 & NA\\\relax
[S9] & 2006 & Singlecase & XPMix & 15-20 & Broadband order
system\\\relax
[S10] & 2004 & Multicase & XP/Scrum & 4-18/6-9 &
b-2-b e-commerce solutions/Criminal justice system development\\\relax
[S11] & 2007 & Singlecase & ScrumMix & 500 & Security
services\\\relax
[S12] & 2010 & Singlecase & ScrumMix & NA & E-commerce\\\relax
[S13] & 2011 & Singlecase & LeanScrum FDD & 5$\pm$2 & Information and
communication software development\\\relax
[S14] & 2012 & Experience r. & XPMix & NA & Web application
development\\\relax
[S15] & 2006 & Multicase & NA/NA/ NA/NA & 2-5/12-15/1-10/6-7 &
NA/NA/NA/NA\\\relax
[S16] & 2012 & Singlecase & Scrum & 6-8 & Web page development\\\relax
[S17] & 2007 & Singlecase & Lean & Comp. 160 devs & Various\\\relax
[S18] & 2010 & Singlecase & ScrumXP Mix & Dev site 600 &
Telecom\\\relax
[S19] & 2010 & Singlecase & AgileMix & 6-7 & Telecom\\\relax
[S20] & 2010 &  Singlecase & LeanMix & NA & Telecom\\\relax 
[S21] & 2011 & Singlecase & ScrumXP Mix & Dev site 500 &
Telecom\\\relax
[S22] & 2012 & Singlecase & LeanMix & NA & Telecom\\\relax
[S23] & 2011 & Experience r. & AgileMix & 9 and 6 & 
Casino games\\\relax
[S24] & 2011 & Singlecase & ScrumBan & 6-8 & Telecom
maintenance\\\relax
%\cite{Shen200725} & 2007 & Singlecase & ScrumXPMix & 4 & Independent software
% developer & Adaptability, innovation, productivity, ROI\\
[S25] & 2010 & Singlecase & LeanMix & project size 100 &
Telecom\\\relax
[S26] & 2011 & Singlecase & LeanMix & project size 200 & Telecom\\\relax
[S27] & 2006 & Singlecase & XPMix & 15 & Enterprise information
system\\\relax
[S28] & 2009 & Singlecase & XP & 15 & Enterprise information
system\\\relax
[S29] & 2006 &  Experience r. & ScrumXP Mix & NA & Telecom\\\relax
[S30] & 2013 & Singlecase & ScrumMix & 5 & Space mission
control software\\\relax
[S31] & 2006 & Experience r. & DSDM & NA & Library software\\\hline
\end{longtable}

\fixme{Korjaa Lean viittaukset metodeihin taulukossa(overview of studies)}

\begin{comment}
\begin{table*}
%\centering
\caption{Overview of primary studies}
\label{OverviewOfPdocs} 
\begin{tabular}{lllllp{3cm}p{8cm}} \hline
ID & Year & Resear. meth. & Agile method & Team size & Domain & Metrics\\
\hline 
\cite{S3} & 2009 & Multicase &  NA/Scrum/Scrum & 2-10/2-7/4-8
& ERP/Graphic design plug-in/Facility management & Team available hours, team
effective hours, critical defects sent by customer, open defects, test
failure rate\\
\cite{S4} & 2013 & Experience r. & Scrum & 25 teams & Software
for oil and gas industry & Technical debt in categories, build status,
technical debt in effort\\
\cite{S5} & 2005 & Singlecase & XP & 15 & Enterprise information
system & Burndown, check-ins per day, number of automated test steps, faults
per iteration\\
[S6] & 2002 & Experience r. & XP & 50 & Enterprise
resource solution for the leasing industry & Velocity\\
\cite{S7} & 2011 & Survey & Scrum & 26 teams & Desktop and SaaS
products & Burndown, story points, \# of open defects, defects found in
system test, defects deferred, net promoter score\\
\cite{S8} & 2010 & Experience r. & Scrum & 5-9 & NA & Story
points, task effort, velocity\\
\cite{S9} & 2006 & Singlecase & XPMix & 15-20 & Broadband order
system & Effort estimate, actual effort\\
\cite{S10} & 2004 & Multicase & XP/Scrum & 4-18/6-9 &
b-2-b e-commerce solutions/Criminal justice system development & \# of
defects/velocity\\
\cite{S11} & 2007 & Singlecase & ScrumMix & 500 & Security
services & Revenue per customer\\
\cite{S12} & 2010 & Singlecase & ScrumMix & NA & E-commerce & Task
expected start and end date, effort estimate, completed web pages, task done\\
\cite{S13} & 2011 & Singlecase &
LeanScrumFDD & 5$\pm$2 & Information and communication software development & Fix time of failed
build, story flow percentage, percentage of stories prepared for sprint,
time to establish project foundation, velocity of elaborating features,
velocity of implementing features\\
\cite{S14} & 2012 & Experience r. & XPMix & NA & Web application
development & Broken build, test coverage, test growth ratio, violations of
static code checks, \# of unit tests\\
\cite{S16} & 2012 & Singlecase & Scrum & 6-8 &Web page development
& Sprint velocity, release velocity, cost performance index, schedule
performance index, planned velocity\\
\cite{S17} & 2007 & Singlecase & Lean & Comp. 160 devs& Various
& Common tempo time, number of bounce backs, cycle time, work in progress\\
\cite{S18} & 2010 & Singlecase & ScrumXPMix & Dev site 600 &
Telecom & Lead time, processing time, queue time\\
\cite{S19} & 2010 & Singlecase & AgileMix & 6-7 & Telecom &
Change request per requirement, fault slips, implemented vs wasted
requirements, maintenance effort\\
\cite{S21} & 2011 & Singlecase & ScrumXPMix & Dev site 500 &
Telecom & Rate of requirements per phase, variance in handovers,
requirement's cost types\\
\cite{S22} & 2012 & Singlecase & LeanMix & NA & Telecom &
Cumulative flow of maintenance requests, lead time\\
\cite{S20} & 2010 &  Singlecase & LeanMix & NA & Telecom &
\# of faults, fault-slip-through, \# of requests from customer,
\# of requirements per phase\\
[S23] & 2011 & Experience r. & AgileMix & 9 and 6 & 
Casino games & Work in progress, average velocity, cycle time\\
\cite{S24} & 2011 & Singlecase & ScrumBan & 6-8 & Telecom
maintenance & Lead time, work in progress, \# of days in maintenance,
\# of days to overdue, reported hours on CSR\\
%\cite{Shen200725} & 2007 & Singlecase & ScrumXPMix & 4 & Independent software
% developer & Adaptability, innovation, productivity, ROI\\
\cite{S26} & 2011 & Singlecase & LeanMix & project size 200 & Telecom
& Throughput\\
\cite{S25} & 2010 & Singlecase & LeanMix & project size 100 &
Telecom & Defect trend indicator, \# of defects, predicted \# of
defects\\
\cite{S28} & 2009 & Singlecase & XP & 15 & Enterprise information
system & Burndown, check-ins per day, number of automated test steps\\
\cite{S27} & 2006 & Singlecase & XPMix & 15 & Enterprise information
system & Burndown, \# of new defects, number of written and passed tests,
task estimated vs actual time, time reported for overhead activities,
check-ins per day\\
\cite{S29} & 2006 &  Experience r. & ScrumXPMix & NA & Telecom
& Story estimate, story complete percentage\\
\cite{S30} & 2013 & Singlecase & ScrumMix & 5 & Space mission
control software & Progress as working code\\
\cite{S31} & 2006 & Experience r. & DSDM & NA & Library software &
Costs, schedule\\
\hline
\end{tabular}
\end{table*}
\end{comment}

\section{Quality evaluation of studies}

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Quality evaluation of primary studies}
    \begin{tabular}{rp{0,5cm}p{0,5cm}p{0,5cm}p{0,5cm}p{0,5cm}p{0,5cm}p{0,5cm}p{0,5cm}p{0,5cm}p{0,5cm}p{0,5cm}p{0,5cm}p{0,5cm}}
    \toprule
    Study & Rese arch & Aim   & Con text & R.de sign & Samp ling & Ctrl. Grp &
    Data coll. & Data anal & Reflex ivity & Find ings & Value & To tal & Rele
    vancy \\
    \midrule
     {[}S1{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{2} \\
     {[}S2{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 2     & \multicolumn{1}{l}{2} \\
     {[}S3{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 3     & \multicolumn{1}{l}{2} \\
     {[}S4{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 2     & \multicolumn{1}{l}{3} \\
     {[}S5{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{3} \\
     {[}S6{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{2} \\
     {[}S7{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 5     & \multicolumn{1}{l}{2} \\
     {[}S8{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{3} \\
     {[}S9{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & 8     & \multicolumn{1}{l}{2} \\
     {[}S10{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 5     & \multicolumn{1}{l}{2} \\
     {[}S11{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{3} \\
     {[}S12{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 1     & \multicolumn{1}{l}{3} \\
     {[}S13{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{3} \\
     {[}S14{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 0     & \multicolumn{1}{l}{2} \\
     {[}S15{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{2} \\
     {[}S16{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 3     & \multicolumn{1}{l}{2} \\
     {[}S17{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 8     & \multicolumn{1}{l}{3} \\
     {[}S18{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{3} \\
     {[}S19{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & 4     & \multicolumn{1}{l}{2} \\
     {[}S20{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{2} \\
     {[}S21{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{2} \\
     {[}S22{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{2} \\
     {[}S23{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 4     & \multicolumn{1}{l}{2} \\
     {[}S24{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 4     & \multicolumn{1}{l}{2} \\
     {[}S25{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 10    & \multicolumn{1}{l}{3} \\
     {[}S26{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{2} \\
     {[}S27{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{2} \\
     {[}S28{]} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & 9     & \multicolumn{1}{l}{3} \\
     {[}S29{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 1     & \multicolumn{1}{l}{3} \\
     {[}S30{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & 3     & \multicolumn{1}{l}{2} \\
     {[}S31{]} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{0} & 2     & \multicolumn{1}{l}{2} \\
    \multicolumn{1}{l}{Total} & \multicolumn{1}{l}{16} & \multicolumn{1}{l}{15} & \multicolumn{1}{l}{20} & \multicolumn{1}{l}{15} & \multicolumn{1}{l}{19} & \multicolumn{1}{l}{7} & \multicolumn{1}{l}{14} & \multicolumn{1}{l}{13} & \multicolumn{1}{l}{7} & \multicolumn{1}{l}{21} & \multicolumn{1}{l}{24} &       &  \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%



The perceived quality of the studies varied a lot (from 0 to 10). Even with
many low quality studies they were included since they still provided valuable
insight. For example in some cases experience reports can provide more
valuable data than a high scoring research papers.

According to the evaluation control group and reflexivity had the lowest total
scores while value for research, context and findings scored the highest.

\fixme{Tähän voisi satuilla dybåån tyyliin, eli toistaa miten tämä tehtiin,
mitä tarkoittaa 0,1,. Sen lisäksi voisi vielä avata itsestäänselvyyksiä mitä
taulukossa lukee. Voisi toistaa tuon relevancy factorinkin. Saatana.}


\begin{comment}
\section{Why were metrics used and how were they used?}

Categories for the reasons and the use of measurements are listed in
\cref{WhyHowCategories}. The following chapters will describe each category in
more detail.

\begin{table}
\centering 
\caption{Categories for measurement usage}
\label{WhyHowCategories}
\begin{tabular}{c p{2,5cm}} \hline
Categories & Sources\\ \hline
\nameref{sec:IterationPlanning}&
 {[}S6,S23,S3,S8,S12{]}
{[}S16,S9,S11{]}\\
\nameref{sec:IterationTracking} &
{[}S21,S27,S16,S5,S12{]}
{[}S4,S7,S6,S17,S29{]}
{[}S30,S10,S25,S24,S23{]}
[S8,S20]\\
\nameref{sec:Motivate} &
[S29,S28,S13,S4,S3]
[S23,S25,S27]\\
\nameref{sec:ProblemIdentification} &
[S21,S29,S17,S13,S25]
[S16,S19,S18,S31,S22]\\
\nameref{sec:PreQuality} &
[S14,S5,S29]\\
\nameref{sec:PostQuality} &
[S19,S3,S7,S25]\\
\nameref{sec:ChangesInProcesses} &
[S13,S21,S18,S4,S25,S14,S26,S20]\\
\hline
\end{tabular}
\end{table}


\subsection{Iteration planning}
\label{sec:IterationPlanning}
Many metrics were used to support iteration planning. The metrics were used
for task prioritization and scoping of the iteration.

Many metrics were focused to help in the prioritization of the tasks for the
next iteration [S8,S9,S11].
Prioritization of features was affected by a metric that measured the amount
of revenue a customer is willing to pay for a feature [S11].

Effort estimation metrics were used to measure the size of the features
[S6].
Furthermore, velocity metrics were used to calculate how many features is the
team able to complete in an iteration [S23]. Knowing the teams'
effective available hours was found useful when selecting tasks for an
iteration [S3].
Velocity metrics were also used to improve the next iteration estimates
[S16].
In one case, task's start and end date metric was used to point out
interdependent tasks in the planning phase [S12].


\subsection{Iteration tracking} %completion}
\label{sec:IterationTracking}
% Timely manner of completing iteration is one of the cornerstones of agile
% development !ISKEVIITE.
Purpose of iteration tracking was to track how the tasks selected for the
iteration were performed and that necessary modifications were done to the
plan to complete the iteration according to schedule.

Metrics helped in monitoring, identifying problems, and predicting the end
result by making it transparent to the stakeholders how the iteration is
progressing [S21,S27,S16,S5,S12,S7,S29,S30].

Progress metrics included number of completed web pages [S12], story
completion percentage [S29] and velocity metrics [S5].
% \juha{velocity metriikan käyttäminen etenemisenseurantaan on
% epäagiilia->ongelmia - tästä asiaa discussioniin} \eetu{lisätty}
However, using velocity metrics had also negative effects such as cutting
corners in implementing features to maintain velocity with the cost of quality
[S6]. One qualitative progress metric was product demonstrations with customer
[S30]. Measuring the completion of tasks enabled selecting incomplete tasks to
the next iteration [S12].

%Risk management was also mentioned often, for example a metric was added
%where it seemed valuable in decreasing a risk \cite{S5},  
%\cite{S27}\cite{S29}\cite{S5}\cite{S28}.

When the metrics indicated, during an iteration, that all planned tasks could
not be completed, the iteration was rescoped by cutting tasks [S16,S5,S17] or
adding extra resources [S5,S17].
 
 % Almost anyone could walk into a team's room and with a quick glance
 % understand what is the status of the iteration.
When there were problems that needed to be fixed, whether they were short or
long term, the metrics helped in making decisions to fix them
[S25,S5,S21,S4].
It was possible to base decisions on data, not only use common sense and
experience [S28]. Balance of work flow was mentioned as a reason
for using metrics in multiple papers
[S23,S19,S20,S8,S21,S5,S13].
% Crosstraining people to work on multiple disciplines was used to balance the
% work flow\cite{S17}\cite{S28}.  - Tämä jäi aika
% irralliseksi
Progress metrics were used to focus work on tasks that matter the most
[S28], avoid partially done work [S24], avoid
task switching [S24] and polishing of features
[S28].Finally, open defects metric was used to delay a release
[S10].

%\subsubsection{Project progress}
%While iteration completion is important - so is the whole project's completion.
%Metrics were also used to monitor the whole project: assess risks, follow work
%completion and 

\subsection{Motivating and improving}
\label{sec:Motivate}
% (Agile methods emphasize self-empowerment - this was also visible in the
% used metrics.)
This section describes metrics that were used to motivate people and support team level
improvement of working practices and performance.

Metrics were used to communicate different data about the project or product
to the team members
[S29,S28,S23,S25,S27].
Measurement data motivated teams to act and improve their
performance [S28,S23,S3,S4,S13].
Some examples included fixing the build faster by visualizing build status
[S13,S4], fixing bugs faster by showing amount of
defects in monitors [S3] and increasing testing by measuring
product size by automated tests that motivated team to write more tests
[S28].


\subsection{Identifying process problems}
\label{sec:ProblemIdentification}
Metrics were often used to identify or avoid problems in processes and work
flows. This chapter describes how metrics were used to spot problems.

There were multiple cases highlighting how metrics are used to identify or
predict problems in order to solve or avoid them
[S21,S29,S16,S19,S18,S31].

Sometimes there were work phases where no value was added, e.g.,
``waiting for finalization''. This type of activity was called waste and was
identified by using lead time. [S22]

% \juha{Tämä mittari ei ole selkeä, täytyy avata hieman enemmän mikä se on}
% \eetu{Avattu nyt hieman lisää.}
Story implementation flow metric describes how efficiently a developer has
been able to complete a story compared to the estimate. This metric helped to
identify a problem with receiving customer requirement clarifications.
[S13]

Creating awareness with defect trend indicator helped to take actions to avoid
problems [S25]. One common solution to problems was to find
the root cause [S13,S17].

\subsection{Pre-release quality}
\label{sec:PreQuality}
Metrics in the pre-release quality category were used to prevent defects
reaching customers and to understand what was the current quality of the
product.

Integration fails was a problem to avoid with static code check metrics
[S14]. Moreover, metrics were used to make sure that the product
is sufficiently tested before the next step in the release path
[S14,S5]. Additionally, making sure that the
product is ready for further development was mentioned [S7].

%\juha{Tämä ei oikeastaan sano mitään, oliko tuossa joku selkeämpi ajatus,
%jotain muuta kuin nuo mainitut staattiset mittarit ja testikattavuus?}Also,
%using metrics to improve pre-release quality was a goal in one case
%\cite{S14}.

Some metrics forced writing tests before the actual code [S29].
Technical debt was measured with a technical debt board that was used to
facilitate discussion on technical debt issues, see \cref{fig:debtBoard}
[S4].

\begin{figure}
	\includegraphics{images/DebtBoard}
	\centering
	\caption{Technical debt.}
	\label{fig:debtBoard}
\end{figure}


\subsection{Post-release quality}
\label{sec:PostQuality}
Metrics in post-release quality deal with evaluating the quality of the
product after it has been released.

Customer satisfaction, customer responsiveness, and quality indicators were
seen as attributes of post-release quality. Some metrics included customer
input to determine post-release quality
[S19,S7,S3] while other metrics
used pre-release data as predictors of post-release quality
[S25,S19,S7]. Customer related
metrics included, e.g., defects sent by customers [S3],
change requests from customers [S19] and customer's
willingness to recommend product to other potential customers
[S7].
Quality prediction metrics included defect counts [S19],
maintenance effort [S25] and deferred defect counts
[S7].

\subsection{Changes in processes or tools}
\label{sec:ChangesInProcesses}
This chapter describes the reported changes that applying metrics had for processes and tools. The changes include changes in measurement practices, development policies, and the whole development process.

The successful usage of sprint readiness metric and story flow metric changed
company policy to have target values for both metrics as well as monthly
reporting of both metrics by all projects [S13].

At Ericsson by monitoring the flow of requirements metric they decided to
change their implementation flow from push to pull to help them deliver in a
more continuous manner. Also, based on the metric they added an intermediate
release version to have release quality earlier in the development cycle.
[S20]

Changes to requirements management were also made based on lead time in other
case at Ericsson. Analysing lead time contributed to delaying technical design
after purchase order was received, providing customer a rough estimate quickly
and merging the step to create solution proposal and technical design.
[S18]

Problem with broken build, and the long times to fix the build, led to
measurements that monitor and visualize the state of the build and the time it
takes to fix it [S4,S13,S14].

Also, additional code style rules were added to code check-in and build tools
so that builds would fail more often and defects would get caught before
release [S13,S14]. 

Similarly, testing approaches were changed based on flow metrics. Using lead
time led to that integration testing could be started parallel to system
testing [S18]. Also, throughput of a test process showed
insufficient capability to handle the incoming features, which led to changing
the test approach [S26].

\begin{figure}
	\includegraphics{images/HandoverPet2011}
	\centering
	\caption{Handovers [21]}
	\label{fig:handOvers}
\end{figure}

\end{comment}

\section{Metrics \& categorization}
\label{sec:metricsCategorization}
Metrics are listed by primary study in appendix, see \Cref{tab:Metrics}.

Found metrics are categorized in \Cref{tab:metricsCategorization} using categorization by
\citetsrc{fenton1998software}.

%    \begin{tabular}{p{2cm}p{7cm}p{5,5cm}} fenton kategorisoinnin taulukonkoko

% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Metric categorization based on \citetsrc{fenton1998software}}
 	\begin{tabular}{p{2,1cm}p{7cm}p{5,5cm}}
    \toprule
    Entities & Attributes &  \\
    \midrule
\textbf{Products} & \textbf{Internal} & \textbf{External} \\
    Product & Running tested features & Customer satisfaction x2, Net Promoter Score, number of requests from customers, progress as working code \\
    Test plan & Number of test cases &  \\
    Code  & Technical debt in categories, technical debt in effort, violations of static code analysis &  \\
    Build & Build status x2, fix time of failed build &  \\
    Features & Remaining task effort, task's expected end date, task done, effort estimate x14, work in progress x3, number of days in maintenance, story complete percentage & Business value delivered, change requests per requirements \\
    Require-ments & Implemented vs wasted requirements, requirement's cost types, Percentage of stories prepared for sprint &  \\
    Defects &       & Defect trend indicator, predicted number of defects \\
    \textbf{Processes} &       &  \\
    Testing & Defect count after testing, critical defects sent by customer, open defects x5, test success rate, test failure rate, faults per iteration, defects found in system test, defects deferred, test coverage, test growth ratio & Number of bounce backs, fault slips \\
    Implementa-tion & Velocity x13, number of unit tests x4, completed web
    pages, cost performance index, schedule performance index, planned
    velocity, common tempo time, average velocity, check-ins per day x3 &
    Story flow percentage \\
    Requirements engineering & velocity of elaborating features &  \\
    Whole development cycle & cycle time x2, lead time x4, processing time, queue time, maintenance effort, number of work items per phase x2, variance in handovers, rate of requirements per phase, throughput, queue, costs, schedule &  \\
    \textbf{Resources} &       &  \\
    Team  & Actual effort & Team effectiveness \\
    Customer & Revenue per customer &  \\
    \bottomrule
    \end{tabular}%
  \label{tab:metricsCategorization}%
\end{table}%

According to the categorization metrics are focused to externally measure the
product while features are measured internally. Measurement in process
entities is focused on internally measuring implementation, testing and
development cycle as a whole.


\begin{comment}
Metrics could help in decreasing software project failures due to
prioritization and resource \& schedule issues in iteration tracking. Metrics
here are all reactive.

Iteration tracking metrics could help in monitoring issues that lead to
project failures. Metrics are mostly reactive.

Metrics that are used to motivate and improve people can be used to solve
issues related to values \& responsibilities and company policies that lead to
project failures. Metrics are both reactive and proactive.

Metrics that are used to point identify problems can be used to prevent Method
causes for failures. Metrics are almost all proactive.

Metrics that are used to improve or understand pre-release quality can be used
to prevent failures that are caused by value \& responsibility, task output
and existing product related issues. Metrics are mostly reactive.

Metrics that are used for post-release quality can be used to prevent failures
that are caused by customers' and users' opinions. Metrics are mostly
proactive.

In general, used metrics were more reactive than proactive.

In general, it seems that metrics are used the most to prevent project
failures that are caused by Methods, then by Environment and not so much about
People or Tasks.
\end{comment}

\section{Why use metrics?}
\label{sec:WhyMetricsUsed}
The following sections describe the reasons for the use of metrics.
\Cref{tab:whyCategories} lists the categories for reasons by sources.
\fixme{kerro että nämä ``Miksi'' jutut jää vähän puolittaisiksi koska toinen
puoli on effectissä. Myös kerro siitä että joskus whyt ja effectit on
samankuuloisia.}

%    \begin{tabular}{rp{7cm}}
% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Reasons for the use of metrics by sources}
    \begin{tabular}{rp{7cm}}
    \toprule
    Categories & Sources \\
    \midrule
    Planning & [S6, S8, S9, S11, S12, S16, S17, S21, S23, S24, S25, S29] \\
    Progress tracking & [S2, S4, S5, S7, S8, S12, S13, S16, S17, S20, S21, S22, S23, S25, S27, S28] \\
    Understand and improve quality & [S3, S4, S5, S7, S14, S17, S19, S22, S28, S29] \\
    Identify problems & [S2, S13, S16, S18, S20, S21, S22, S25, S29, S31] \\
    \bottomrule
    \end{tabular}%
  \label{tab:whyCategories}%
\end{table}%

\subsection{Planning}

Prioritization of tasks was one of the main activities metrics were used for.
At Objectnet, effort estimates were used to prioritize the features for next
release and as basis for resourcing [S9]. Teams at Adobe Systems used effort
estimates to prioritize activities based on relative value and relative effort
[S8]. At Verisign Managed Security Services, they used revenue per customer to
prioritize their backlog [S11]. At Timberline, they used Kano analysis as a
'voice of customer' so that prioritization decisions could be based on facts
instead of political power [S17]. Practitioners at Ericsson used cost types,
rate of requirements over phases and variance in handovers for short term
decisions related to requirements prioritization, staff allocation and
planning decisions [S21].

Metrics were used to estimate the size and amount of features that could be
taken under development. Velocity was used to improve effort estimates for
next planning session, which helped to understand how large the scope can be
for the next iteration [S16]. Scrum master and product owner at a Korean
e-commerce company used estimates to check the if the planned scope would be
possible to complete during the next iteration [S12]. At WMS Gaming, they used
pseudo-velocity and average velocity to plan their releases [S23]. At Ericsson
product maintenance team, lead time was used to understand if all planned
corrections can be completed before release date [S24]. At Avaya
Communications, they used story estimates to understand the iteration where a
feature would be completed [S29]. \fixme{epäagiili käyttö}

Other planning uses for metrics were resourcing decisions and development
flexibility. At Timberline, they broke down requirements into smaller pieces
that were estimated in effort to understand what skills are needed to complete
the work [S17]. At a Korean e-commerce company, they marked tasks done and
undone which made it possible to take undone tasks for the next iteration
[S12]. Also, they marked expected end date for tasks so the next person in
workflow could plan their own work as effectively as possible thus reducing
idle time. At ThoughtWorks, stories and their effort estimates were used as
the fundamental units of development for the iteration and as basis for
resourcing [S6]. At Ericsson, predicted number of defects was used to plan the
removal of defects [S25]. If the removal of defects would not be well planned
it could cause delays for the release and thus increase costs for the project.

\subsection{Progress tracking}
Reasons for metrics usage in progress tracking are divided into project
progress, increase visibility, accomplishing project goals and balance workflow.

\subsubsection{Project progress}

Metrics were used to monitor the progress of the project. Completed web pages
was used as a measure of progress at Korean e-commerce company [S12]. Number
of automated passing test steps was used as a measure of progress in terms of
completed work at Mamdas [S5]. At Timberline, breaking down tasks to 'kits'
between 2 to 5 days enabled progress monitoring [S17]. Set of metrics
(burndown, check-ins per day, number of automated passing test steps, number
of new and open defects) was developed to manage risks and provide timely
progress monitoring [S27]. Developers at Avaya Communications used story
percent complete metric to give assessment of progress [S29]. A team at NASA
Ames Reserch Center didn't want to spend resources on estimating features and
instead they focused on designing and developing their software solution
[S30]. Every six weeks they demonstrated their progress to
customer with working code.

Metrics were also used to give higher level understanding of progress.
Release burndown showed project trends and could be used to predict completion
date [S16]. Also, release burndown could reflect addition or removal of
stories. At Ericsson, cost types, rate of requirements over phases and
variance in handovers were used to provide overview of progress [S21]. Metrics
(burndown, check-ins per day, number of automated passing test steps) were used to
communicate progress to upper management [S5] and ensure good progress to
external observers and ensure that key risks were under control [S5,S27,S28].

\subsubsection{Increase visibility}

Metrics were used to simplify and understand complexity, and increase
visibility for all stakeholders. Cost types, rate of requirements over phases
and variance in handovers were used to increase the transparency of end-to-end
flow in a complex system [S21]. At Petrobras, technical debt board was used to
make technical debt issues visible and easier to manage [S4]. Metrics (burndown,
check-ins per day, number of automated passing test steps, number of open and
new defects) were used to replace individual perception with facts [S27].

Metrics were used to keep the team informed. At Ericsson, defect trend
indicator was used to monitor defect backlog and spread the information to
project members [S25]. At WMS Gaming, cycle time metric was used to let the
team track their own performance [S23]. At Avaya Communications, story percent
complete metrics were generated automatically when tests were run and thus
kept everyone on the same page and eliminated schedule surprises [S29].
Additionally, the metric results were required to be reported periodically as
well. At Slovenian publishing company, release burndown made the correlation
clear between work remaining and team's progress in reducing it [S16].
 
\subsubsection{Accomplishing project goals}

Metrics were used to understand if project goals can be achieved. At
Timberline, there was a need for simple indicator that would quickly tell if
project is under control [S17]. They used common tempo time to understand if
project was in target for delivery. At Microsoft Corporation, they monitored
work in progress to predict lead time which in turn would predict project
schedule [S2].
At Adobe Systems, sprint burndown was used to tell the team if they were on
track regarding the sprint commitments [S7]. Similarly at Mamdas, burndown was
used to see if the team could meet their goals, and if not what could be done
[S5]. Burndown was also used to mitigate the risk where developers spend too
much time perfecting features over finishing all tasks of the iteration [S28].
Story flow percentage was used so that a developer could finish a story in a
steady flow [S13]. Story implementation flow metric describes how efficiently
a developer has been able to complete a story compared to the estimate.

\subsubsection{Balance workflow}
\label{sec:balanceWorkflow}
Metrics were used to balance workflow to prevent overloading people. At
Ericsson, inventory of requirements over time was used to identify large
handovers of requirements that would cause overloading situations to employees
[S20]. The aim was to have steady flow of requirements.
Similarly at Citrix Online, operations department was overloaded so they
decided to start evaluating incoming work with Ops story points to level the
workload [S8]. People should be respected by having balanced workload to avoid
overload situations [S22]. This could be achieved by measuring number of
requirements per phase which would notice peaks of workload. Timberline tried
to pace work according to customer demand [S17]. However, too much work was
pushed to development, which caused many problems, including developers feeling
overworked. They started using common tempo time to make sure there would be
balance of workflow.
% Component level burndown was used to decide on resource mobility [S5].

At Ericsson, variance in handovers was used to guarantee that requirements
would flow evenly [S21]. Mamdas was measuring check-ins per day metric, which
measured how often code was committed to main trunk [S5]. The point was to
avoid people from committing only at the end of the iteration, and instead integrate
early and often. At WMS Gaming, they had problems with large tasks
blocking other work, so they set a rule that only certain size of tasks(8
story points) can be taken for development [S23].

\subsection{Understand and improve quality}
The following sections describe how metrics were used to understand the
quality of the product both before and after release. Also, the sections will
describe that metrics were used to improve the quality of the product and
ensure that the product will be tested thoroughly.

\subsubsection{Understand level of quality}

Metrics were used to understand the level of quality after the release.
Number of change requests from customer was used as indicator customer
satisfaction [S19]. Maintenance effort was used as an indicator of overall
quality of the release product [S19]. Number of maintenance requests was used
as an indicator of built in quality [S22].

Metrics were also used to understand the level of quality before the release.
At Adobe Systems, they measured pre-release quality with Net Promoter Score
which was measured from pre-release customer surveys [S7]. Net Promoter Score
measures how willing a customer is to recommend the product to another
potential customer. They also measured defects found in system test which was
used to measure the quality of software delivered to system test process.
Additionally, they measured defects deferred that was used to predict the
quality customers would experience. Defects deferred were defined as the
defects that are known but are not fixed for a release, usually due to time
constraints. At Mamdas, faults per iteration were used to measure the
product's quality [S5].

\subsubsection{Increase quality}

Metrics were used to increase the level of quality. Governance mechanisms,
which included a set of metrics (burndown, check-ins per day and number of
automated passing test steps), were used to increase product quality [S28].
At T-Systems International, they used a set of metrics(build status, number of
unit tests, test coverage, test growth ratio, violations of static code
analysis) to improve project's internal software quality [S14]. Build status
was measured to prevent defects reaching production environment.
Similarly, violations of static code analysis was used to prevent the
existence of critical violations. Furthermroe, critical defects sent by
customers were tracked and fixed to prevent losing customers [S3].
Finally, technical debt board was used to reduce technical debt [S4].

\subsubsection{Ensure level of testing}

Metrics were used to make sure the product is tested well enough.
At T-Systems International, test coverage was used to evaluate how well the
code was tested [S14]. However, in Brown-field (legacy) projects it's better
to measure test-growth-ratio since there might not be many tests in the
existing code base. At Timberline, work in progress was measured so it could
be minimized [S17]. Large amount of work in progress would contain many
unidentified defects which would need to be eventually discovered. At Mamdas,
using number of automated passing test steps dealt with decreasing the risk
that the product would be unthoroughly tested [S5]. Similarly, number of
automated passing test steps was used to make sure regression tests are ran
and passed every iteration. Finally, story percent complete metric supports
test driven development by requiring unit tests to be written for progress
tracking [S29].

\subsection{Identify problems}

Metrics were used to identify problems, bottlenecks and waste in the process.
Cumulative number of work items over time metric was used to identify
bottlenecks in the development process [S22]. Similarly, monitoring work in
progress was used to identify blocked work items and also the development
phase where the blockage occurred [S2]. Cost types, rate of requirements over phases
and variance in handovers were used for process improvement by spotting
bottlenecks and uneven requirement flows [S21].

Metrics were able to identify waste, as in development phases were no value is
added, in software processes. At Ericsson, value stream maps (VSM) were used
to spot waste in the development process [S18]. In another case at Ericsson,
long lead times led to identification of waste of waiting [S22].
Similarly, measuring story flow percentage allowed identification of waste related to context
shifts [S13].

Metrics were used to identify problems and find improvement opportunities.
Defect trend indicator was used to provide the project manager an ISO/IEC
15939:2007 compatible indicator for problems with the defect backlog [S25].
Basically, the indicator showed if the defect backlog will increase, stay the
same or decrease in the coming week. The project manager could then use the
info to take necessary actions to avoid possible problems. At the Slovenian
publishing company, schedule performance index and cost performance index were
used to monitor for deviances in the project's progress and providing early
signs if something goes wrong [S16]. Developers at Avaya had issues with the
80/20 rule, where the last 20\% of iteration takes the longest [S29]. With the
metrics that their tool T3 provided (e.g Story percent complete) they were
able to see the early symptoms of various problems that can cause delays, and
thus react early.

Metrics were also used to find improvement opportunities. Number of work items
per phase and lead time was used to spot instabilities in the process [S20].
They had set control limits to the metrics and if a measured value was outside
the control limits it meant that there was some kind of instability.
Similarly, monitoring schedule and costs with a dashboard allowed to spot for
improvement opportunities at OCLC [S31].

\section{What kind of effects did the use of metrics have?}
\label{sec:effects}
The following subsections describe the effects that the use of metrics had.
\Cref{tab:howCategories} lists the effect categories by sources.

Some of the categories for the reasons and the effects of metrics are close to
each other so sometimes it can be hard to understand why some categories are listed
under the reasons and not under the effects. For example
\cref{sec:balanceWorkflow} describes that the purpose for metrics was the
balancing of workflow and then the actual actions and effects are described
under \cref{sec:effects}.


% Table generated by Excel2LaTeX from sheet 'Taul1'
\begin{table}[htbp]
  \centering
  \caption{Effects for metric usage by sources}
    \begin{tabular}{rp{7cm}}
    \toprule
    Categories & Sources \\
    \midrule
    Planning actions & [S2, S11, S12, S23] \\
    Reactive actions & [S3, S5, S10, S14, S16, S17, S28] \\
    Motivate people & [S3, S4, S6, S13, S14, S17, S25] \\
    Create improvement ideas & [S8, S10, S13, S14, S17, S18, S20, S21, S22, S26, S27, S28] \\
    \bottomrule
    \end{tabular}%
  \label{tab:howCategories}%
\end{table}%


\subsection{Planning actions}
This section describes the planning actions that happened due to the
use of metrics.

Product owners at WMS Gaming used lead time to schedule high priority features
and planned demo dates with customers [S23]. Similarly, at Verisign
Managed Security Services they used revenue per customer metric to allow
higher valued features to be prioritized higher in the backlog [S11].

Velocity / 2 metric was used as scoping tool for a release [S23]. The team had
enough work not to sit idle, but there was still enough time fix high priority
defects. Similarly, effort estimates were used to scope the iteration and if
there were tasks that cannot be completed before release date then they were
excluded from the backlog [S12]. Furthermore, velocity was used to define a
minimum delivery level for the iteration where 'must have' requirements are
assigned, and a stretch goal where lower priority requirements are assigned
[S2].

Expected date of completion was used so that other team members could plan
their own work [S12]. For example, developer could know when she can start
implementation because designer had informed the expected date of completion
for the design.

\subsection{Reactive actions}
This section describes reactive actions that occurred due to the use of
metrics.

Metrics were used to cut down the scope of an iteration or to add more
resources if it seemed not all tasks could be completed with current pace.
When component level burndown was used to notice that a component was behind
schedule at Mamdas, resources were added and scope was reduced for the release
[S5]. Similarly, release burndown showed that work remaining was not
decreasing fast enough so the scope of the release was decreased [S16].
Furthermore, if common tempo time would indicate too much planned work, then
the tasks would be cut or more resources would be added [S17]. Similarly,
employees were trained with multiple skills e.g customer support did testing
and documentation engineers were taught how to input their material into the
system, so in case of imbalanced workload the work could be reorganized to
achieve more balanced workflow.  If team effectiveness is not high enough to
complete tasks, resources from other teams can be used [S3]. Other actions
that were suggested were reduction of tasks and working overtime.

Metrics were also used to react to quality information. At Timberline,
monitoring cycle times revealed high time consumption on manual testing [S17].
The cause was an unmotivated person who was moved to writing automated test
scripts which he preferred. At Escrow.com, number of defects were used to
delay a release when too many defects were noticed in a QA cycle [S10].
At T-Systems International, quality manager interpreted results of static code
analysis from the build tool and he would then make plans for necessary
refactorings [S14]. When amount of written and passed unit tests was not
increasing an alarm was raised at Mamdas [S28]. The issue was discussed in a
reflection meeting where they understood that too much work was put to a
single tester writing the tests and once she was doing work for another
project no tests were written. The team then started to learn to write tests
themselves, and later a dedicated tester was assigned to write the
tests.

\subsection{Motivate people}
This section describes the motivating effects that the metrics had on people.

Metrics were used to motivate people to react faster to problems. Number of
defects was shown in monitors in hallways which motivated developers to fix
the defects [S3]. Similarly, total reported defects and test failure and
success rate was also shown throughout the organization which motivated people
to avoid problems and also fix the problems fast. At Systematic, they measured
fix time of broken build and showed the time next to the coffee machine. It
provoked discussion on the reasons for long fix times and eventually
developers fixed the builds faster [S13]. The metric was later declared
mandatory for all projects. Also, the reasons for long fix times were
investigated. Similarly at Petrobras, build status was visible in minutes
after commits, which helped to create a culture where developers react with
high priority to broken builds [S4]. This helped to keep the main branch to be
closer to deployable state at all times. Build status was used to motivate
people to fix the build as fast as possible [S4]. Moreover, Violations of
static code analysis caused developers to immediately fix the issue because
the violations could cause a broken build status [S14]. Additionally,
developers could get faster feedback on their work. Furthermore, developers
could have more confidence in performing major refactorings with the safety
net the violations of static code analysis metric provides.

Metrics were used to change people's behavior. At Petrobras, they used a
technical debt board to discuss technical debt issues in their projects. In
the meetings, team members agreed which techincal debt issues they would focus
in solving until the next meeting [S4]. Additionally, team members sought help
from the architecture team for reducing technical debt, e.g by implementing
automatic deployment systems and improving source code unit testability.
At Mamdas, measuring the number of automated passing test steps changed teams
behaviour to write more unit tests [S5]. Metrics were also used to prevent
harmful behaviour such as cherry picking features that are most interesting to
the team [S17]. Measuring work in progress (WIP) and setting WIP limits
prevented cherry picking by enforcing only two features at a time and thus
preventing them from working on lower priority but more interesting features.
Finally, at Ericsson defect trend indicator created crisis awareness and
motivated the developers to take actions to avoid possible problems [S25].

There can also be negative effects in using metrics. Using velocity metric had
negative effects such as cutting corners in implementing features to maintain
velocity with the cost of quality [S6]. For example, the managers excused the
developers from writing tests and the testers cut on the thoroughness of the
testing in hopes to maintain the velocity.

\subsection{Create improvement ideas}

This sections describes improvement ideas that were created based on metrics.

When a waste of extra process(requirement would wait for long time before full
specification) was identified in requirement specification with the help of
lead time, processing time and queue time metrics, a solution idea was created
where a quick high level proposal would be sent to customer without the need
for in-depth specification [S18]. Customer could then use the high level
proposal to evaluate if they want to pursue that requirement further. Similarly, two
requirement specification phases could be combined when another waste of extra
process was identified in requirements specification phase [S18].
Additionally, lead time could be decrease by increasing the collaboration with the market
unit and the development unit [S18]. Similarly, there was a waste of waiting
in design which could be improved by starting real work only when the
purchase order is received, not when requests are received [S18]. 

Lead time, processing time and queue time metric were used to identify waste
of waiting in testing phases [S18]. The improvement suggestion was to provide
earlier beta version and making testing phases parallel. Many of the
improvement ideas came from meetings where the value stream maps (VSM) were
used as a base for discussion.

Cost types, rate of requirements over phases and variance in handovers were
used to identify bottlenecks at Ericsson [S21]. They noticed that focusing on
deadlines caused a lot of requirements to be transferred to system test phase
close to the deadline. The improvement suggestion was to focus more on
continuous delivery instead of focusing on market driven deadlines.
Furthermore, Kanban was suggested as a development method to accomplish the
continuous delivery capabilities.

Throughput and queue time metrics were used to identify bottleneck in network
integration test phase which lead to using other testing practices in future
projects [S26].

Rate of requirements over time was used to identify problems in the
development process [S20]. One improvement suggestion was to change from push
to pull-approach so that team can adjust the workload to enable continuous
delivery. Another improvement suggestion was to add intermediate release
versions so that integration and testing would happen more often and problems
could be identified earlier than close to the actual release. Similar solution
was applied at Timberline inc. where requirements inventory was kept low which
meant that design, implementation and testing could start earlier and problems
in requirements would get caught sooner [S17].

Citrix online started measuring velocity for their Operations department
as well [S8]. This led to other departments trying to decrease their products'
Ops story points to enable faster releases. The reduction in story points was
possible by creating hot deployment strategies and providing better
documentation. 

Mamdas, an Israel Air force IT department, were using burndown to follow their
progress [28]. However, when they noticed that work remaining wasn't
decreasing according to remaining resources they had to make changes. In their iteration
summary meeting they decided to pursue senior engineers to help them create
optimal development environments and continuous build systems. Also, they
decided to evaluate customer requests in more detail to avoid over polishing
features. 

When story implementation flow metric showed a drop and project managers
complained about clarifications about features from customer were late, a root
cause analysis meeting was held [S13]. Also, after starting to use the
implementation flow metric new policies were stated to keep the flow
high: percentage of stories ready for sprint must be 100\% and implementation
flow must be at least 60\%, and both of the metrics need to be reported
montly. Root cause analysis was also conducted to decrease the amount of
bounce backs at Timberline inc. [S17].

A team noticed their velocity estimations were inaccurate which led to
dividing work items into smaller pieces to improve the accuracy of the
estimates [S10].

The reasons for the values of metrics(burndown, check-ins per day, number of
automated passing test steps, number of new and open defects) were discussed
in iteration summary meeting because it can be hard to analyze metrics without understanding the context
[S27]. Similarly, number of work items per phase was used to ask development
unit about the values of the metric and the development unit confirmed that
people felt overloaded as the metric suggested [S20]. Furthermore, if the
values of number of work items were outside the control limits one could
discuss with the developers about the workload [S22].

After analyzing long fix times for broken builds the team added
automatic static code analysis checks to code check-in to catch defects
earlier [S13]. Quality manager can change coding style guide and code
standards based on the results of violations to static code analysis metric
[S14].

\section{Important metrics}
\label{sec:importantMetrics}
The following subsections describe first which metrics were described as
important and then second which metrics were described the most times in the
primary studies.

\subsection{Important metrics in terms of statements}
 
This section describes metrics that were described as important.

\begin{comment}
Capacity as number of features developed in release was considered better than
measuring speed, since speed is generally thought as a attribute of humans.
Capacity on the otherhand measures the capabilities of an organization.
\end{comment}

Progress as working code was considered as one of the cornerstones of agile
[S30]. Story flow percentage and velocity of elaborating features were
considered as key metrics for monitoring projects [S13]. Also, a minimum 60\%
value for flow was identified. Similarly, velocity for elaborating features
should be as fast as velocity of implementing features. Also, they said using
both metrics \emph{``drive behaviors to let teams go twice as fast as they
could before''}. Story percent complete metric was considered valuable since
it embraces test driven development - no progress is made before test is
written [S29]. Also, percent complete metric is considered more accurate than
previously used metric. Moreover, it gives normalized measure of progress
compared to developer comments about progress. Additionally, story percent
complete metric leverages existing unit testing framework and thus requires
only minimal overhead to track progress. Team members seemed to be extremely
happy about using the metric. Practitioners at Ericsson valued transparency and overview of progress that
the metrics (cost types, rate of requirements over phases and
variance in handovers) were able to provide to the complex product development with
parallel activities [S21].

In an agile survey [S1], project success had significant positive relationship
with the following metrics: team velocity, business value delivered, running
testing features, defect count after testing and number of test cases.
However, there were no detailed descriptions of these metrics.

Effort estimates were consider important in release planning especially in
terms of prioritization [S9]. Top performing teams at Adobe Systems estimated
backlog items with relative effort estimates [S7]. Pseudo-velocity was considered
essential for release planning [S23]. Burndown was valuable in meeting sprint
commitments [S7]. Similary, managers said burndown was important in making
decisions and managing multiple teams [S5]. However, developers didn't
consider burndown important [S5].

At another case at Ericsson Value Stream Maps (VSM) were used to visualize
problem areas and possible improvements [S18]. Practitioners valued how the
maps were easy to understand. Metrics that were used to build VSM were lead
time, processing time and queue time. Technical debt board that visualized the
status of technical debt in categories was considered important because it
gave a high level understanding of the problems and it was then used to plan
actions to remove technical debt [S4]. It was proven to be useful in their
context.

Net Promoter Score was said to be \emph{``one of the purest measures of
success''} [S7]. According to a survey [S1], projects that were said to be
definitely successful 77\% measured customer satisfaction often or always.
Also, the more often customer satisfaction would be measured the more likely
it would be that the project would have good code quality and the project
would succeed. Defects deferred was seen as a good predictor of post-release
quality because it correlated with issues found by the customers [S7].

Defect prediction metrics predicted number of defects in backlog and defect
trend indicator were seen important to decision making, and their use
continued after the pilot period [S26]. Key attributes of the metrics were
sufficient accuracy and ease of use.

The following metrics were considered very useful in agile context: number of
unit tests, test coverage, test-growth ratio and build status [S14]. The
benefit for the number of unit tests was not well described except that it provided
\emph{``first insights''}. Test coverage provided info on how well the code
was tested. Test-growth ratio was useful in projects where old codebase was
used as basis for new features. Fixing broken builds prevented defects
reaching customers. 

\subsection{Important metrics based on the amount of evidence}
This section describes metrics that were discussed the most times in the
primary studies.

Effort estimate (x14) and velocity (x13) were clearly the most discussed
metrics among the studies. Open defects metric (x5) were were discussed
the third most. Next comes number of unit tests (x4) and lead time (x4). Work
in progress (x3) and check-ins per day (x3) are both describe three times.
Finally, metrics that were mentioned two times were cycle time (x2), build
status (x2), number of work items per phase (x2) and customer satisfaction
(x2).

\chapter{Discussion}
\label{sec:Discussion}

\section{Implications for practice}
To provide implications to practice the findings are mapped to the principles
of agile software development \citetsrc{beck2001agile} categorized by Patel \&
al. \citetsrc{1579312}. For each paragraph the naming by Patel et al. is used
and references to the agile practices is provided by numbers. \fixme{Korjaa
referenssit uuteen jakoon}

Communication and Collaboration (principles 4 and 6) was reflected in metrics
that motivated a team to act and improve, see \cref{sec:Motivate}. Also,
progress metrics were used to communicate the status of the project to the
stakeholders, see \cref{sec:IterationTracking}.

Team involvement (5,8) was reflected in metrics that motivated team to act and
improve, see \cref{sec:Motivate}. Also, to promote sustainable development
metrics were targeted to balance the flow of work, see
\cref{sec:IterationTracking}.

Reflection (12) was visible in metrics that were used to identify problems and
to change processes, see \cref{sec:ProblemIdentification} and
\cref{sec:ChangesInProcesses}.

Frequent delivery of working software (1,3,7) was directly identified in one
paper, where the team measured progress by demonstrating the product to the
customer [S30]. Additionally, there were cases where e.g.
completed web-pages [S12] were the primary progress measure. Also, many
metrics focused on progress tracking and timely completion of the iteration, see \cref{sec:IterationTracking}. However, some other
measures from \cref{sec:IterationTracking} show that instead of working code
agile teams followed completed tasks and velocity metrics. 

%\juha{haluaisin jotain tämänkaltaista keskustelua suorista laatumittareista,
% mutta voiko näin sanoa tämän tutkimuksen perusteella. Eetu, etenkin tuo
% viimeinen virke, onko linjassa sinun mielestäsi??}\eetu{Nyt muokattuna voi
% sanoa.}
An integral part of the concept of working software is measuring post-release
quality, see  \cref{sec:PostQuality}. This was measured by customer
satisfaction, feedback, and customer defect reports. It was also common to use
pre-release data to predict post-release quality. Agile developers tend to
measure the end product quality with customer based metrics instead of the
traditional quality models, such as ISO/IEC 25010 \citepsrc{10951538}.

Managing Changing Requirements (2) was seen in the metrics that support
prioritization of features each iteration, see \cref{sec:IterationPlanning}.
Additionally, different metrics helped keeping the internal quality of the
product high throughout the development which then provided safe development
of modifications from new ideas, see \cref{sec:PreQuality}.

Design (9,10,11) was seen in focus to measuring technical debt and using
metrics to enforce writing tests before actual code, see
\cref{sec:PreQuality}. Additionally, the status of build was continuously
monitored, see \cref{sec:ChangesInProcesses}. However, the use of velocity
metric had a negative effect on technical quality, see
\cref{sec:IterationTracking}.
% was seen from different perspectives: on one hand metrics focused on
% problem/waste identification, see \cref{sec:ProblemIdentification},
Many metrics focused on making sure that the right features were selected for
implementation, see \cref{sec:IterationPlanning}, thus avoiding unnecessary
work.

\begin{comment}

Agile principle \#1: ``Our highest priority is to satisfy the customer through
early and continuous delivery of valuable software.'' was seen in the team
measuring progress by demonstrating the product to the customer
\cite{S30}.

Agile principle \#2: ``Welcome changing requirements, even late in
development. Agile processes harness change for the customer's competitive
advantage.'' was seen in the metrics that support prioritization of features
per iteration, see \cref{sec:IterationPlanning}. Additionally, different
metrics helped keeping the internal quality of the product high throughout the
development which then provided safe development of modifications and new
ideas, see \cref{sec:PreQuality}.

Agile principle \#3: ``Deliver working software frequently, from a couple of
weeks to a couple of months, with a preference to the shorter timescale''  was
seen in many metrics focusing on tracking and timely completion of the
iteration, see \cref{sec:IterationTracking}

Agile principle \#4:``Business people and developers must work 
together daily throughout the project.'' was seen how different metrics
were used to share information to all stakeholders about the project, see
\cref{sec:Motivate} and \cref{sec:IterationTracking}. 

Agile principle \#5:``Build projects around motivated individuals. Give them
the environment and support they need, and trust them to get the job done''
was reflected in metrics that motivated team to act and improve, see
\cref{sec:Motivate}.

Agile principle \#6:``The most efficient and effective method of 
conveying information to and within a development 
team is face-to-face conversation.'' was seen in \cite{S4} where
a technical debt board measuring the level of technical debt was used to
facilitate face-to-face discussion on technical debt issues, see
\ref{sec:PreQuality}. 

Agile principle \#7:``Working software is the primary measure of progress'' 
was directly identified in one paper, where the team measured progress by
demonstrating the product to the customer. Additionally, there were cases
where for example completed web-pages \cite{S12} were the primary
progress measure. However, some other measures from \cref{sec:IterationTracking} show that instead of working code agile teams followed completed tasks and velocity metrics.

Agile principle \#8:``Agile processes promote sustainable development. The
sponsors, developers, and users should be able to maintain a
constant pace indefinitely.'' was followed with metrics targeted to balance
the flow of work, see \cref{sec:IterationTracking}.

Agile principle \#9:``Continuous attention to technical excellence 
and good design enhances agility.'' was seen in focus to measuring technical
debt and using metrics to enforce writing tests before actual code, see
\cref{sec:PreQuality}. Additionally, the status of build was continuously
monitored, see \cref{sec:ChangesInProcesses}. However, the use of velocity
metric had a negative effect on technical quality, see
\cref{sec:IterationTracking}.

Agile principle \#10:``Simplicity---the art of maximizing the amount 
of work not done---is essential.'' was seen from different perspectives: on one
hand metrics focused on problem/waste identification, see
\cref{sec:ProblemIdentification}, and on the other hand many metrics focused
on making sure that the right features were selected for implementation, see
\cref{sec:IterationPlanning}.

Agile principle \#11:``The best architectures, requirements, and designs
emerge from self-organizing teams.'' was seen in metrics that motivate the
team to improve, see \cref{sec:Motivate}. Other perspective is that since
effort estimation is done by the team, the team is then more motivated to
accomplish the goal, see \cref{sec:IterationPlanning}.

Agile principle \#12: ``At regular intervals, the team reflects on how to
become more effective, then tunes and adjusts its behavior accordingly'' was
visible in metrics that were used to identify problems and to change
processes, see \cref{sec:ProblemIdentification} and 
\cref{sec:ChangesInProcesses}.

\end{comment}

%\eetu{Mun mielestä seuraavat mittarit ei oo niin ketteriä, mut en osaa oikeen
%perustella miksi tai sanoa mitä periaatteita vastaan ne olisi: maintenance
%effort, cost types, defect amounts(?), defects deferred, revenue per
%customer(?), time to establish project foundation, test coverage, test growth
%ratio, cost performance index, schedule performance index. Näitä ei kaikkia
%ole myöskään kuvattu resultseissa, paitsi taulukossa mainittu.}

There were also metrics, or their usage, which were not agile in nature. E.g.,
maintaining velocity by cutting corners in quality instead of dropping
features from that iteration [S6]. Also, adding people to
project to reach a certain date [S5, S17] doesn't seem that
agile compared to removing tasks. Adding people can have a negative impact to
progress, considering the lack of knowledge and training time required.
%Moreover, the use of dates to plan interdependent tasks is not agile in
% nature [S12].Instead, interdependencies should be visible inchoosing the tasks to appropriate iterations. 
Also, the use of number of defects to delay a release [S10] is against agile
thinking as one should rather decrease the scope to avoid such a situation.

%While the flow metrics Ericsson have a good target of balancing workflow,
% they seem  (or at least they are presented) complicated to use---meaning that one
%might need considerable effort to generate and analyse the metrics, which
%doesn't fit to the light-weightness of agile.

%\juha{muotoilisin tämän hieman toisin}
% Contradictory to fifth principle, Talby et al [viite] enforce writing
% automated test cases as a measure of progress - so in a way they didn't
% trust the developers to write the test on their own? Similarly, [viite]
% measured the status of the build to make developers fix the build faster -
% again, not trusting them to do it on their own.
Some agile metrics that work well for an agile team, such as tracking progress
by automated tests [S28], or measuring the status of the build
[S14] can turn against the agile principles if used as an external
controlling mechanism. The fifth agile principle requires trust in the team,
but if the metrics are enforced outside of the team, e.g., from upper
management there is a risk that the metrics turn into control mechanisms and
the benefits for the team itself suffer.

%\subsection{Implications for research}
%It was interesting to notice that there wasn't many code metrics, only the
%ones mentioned in \cite{S14} even though we feel there are many
%% studies regarding the benefits of code metrics. Maybe there are some
% practical
%problems implementing and analysing the data from code metrics?

%How to measure unmeasured agile principles...

%Effort estimates are prerequisite for velocity metrics, and since velocity
%metrics were vastly identified in our study, e

%In general, we think there were many metrics that were targeted for the team
% - instead of high focus on managerial or upper management reporting metrics.
%Making metrics visible for the team enables them to independently act and
%improve without the need of rapid supervision and telling people what to do.

\fixme{Lisää important mittareista jotain tänne discussioniin, mutta mitä?
Mitä ajatuksia kyseiset löydökset herättää? Ei atm juuri mitään. Siinähän ne
on.}

\subsection{Comparison to prior studies}
Only few papers have broadly studied the reasons for software metrics use in
the context of agile software development. \citetsrc{1667571} also highlight process improvement as one of the reasons
for measurement in their agile metrics paper. Also, they emphasize that creation of value should
be the primary measure of progress - which was also seen in this study.

\citetsrc{Korhonen2009} found in her study that traditional defect
metrics could be reused in agile context - if modified. Defect metrics were
also used in many of the primary studies.

Kitchenham's mapping study \citepsrc{kitchenham_whats_2010} identified several
code metrics in academic literature. However, in this study almost no evidence
of code metric use in the industrial agile context was found. Maybe agile
practitioners consider code metrics self-evident and don't report them, or
maybe code metrics aren't widely used by agile industrial teams.

\fixme{Harkitse WETSOM kommenteissa mainittujen SLR:ien tarkastelua tässä.}

%Maybe it is time to
%re-evaluate the need for code metrics research if industry doesn't seem to
% use them.

%The reasons for the lack of code metric usage in agile
%contexts should be studied to evaluate the necessity of code metric research
% - or how code metric research could be modified to support agile development

%The lone case in our study where code metrics were used, the code
%metric usage was abstracted to a build tool, which would just indicate an
%error or broken build \cite{S14}. Maybe the use of code metrics should
%be heavily implemented through automated tools that handle the collection and
%analysing of code metric data?

\section{Limitations}

\fixme{Add SLR vs survey - eli pöhkö tutkimusmenetelmä valittu tehtävään}
The large shares of specific application domains in the primary documents is a
threat to external validity. Seven out of 29 studies were from enterprise
information systems domain and especially strong was also the share of ten
telecom industry studies out of which eight were from the same company,
Ericsson. Also, Israeli Air Force was the case organization in three studies.

The threats to reliability in this research include mainly issues related to
the reliability of primary study selection and data extraction. The main
threat to reliability was having a single researcher performing the study
selection and data extraction. It is possible that researcher bias could have
had an effect on the results. This threat was mitigated by analysing the
reliability of both study selection and data extraction as described in
\cref{sec:Method}.

%Sometimes it was hard to understand which metrics an author was referring
%when a ``why'' was described. Moreover, we had to sometimes assume that when
%author describes the reasons for using a tool, he would be actually talking
%about the metrics the tool shows.

Due to iterative nature of the coding process, it was challenging to make sure
that all previously coded primary documents would get the same treatment,
whenever new codes were discovered. In addition, the researcher's coding
``sense'' developed over time, so it is possible that data extraction accuracy
improved during the analysis. In order to mitigate these risks a pilot study
was conducted to improve the coding scheme, get familiar with the research
method, refine the method and tools.

%First author has positive mindset towards agile methods, as well as towards
%certain metrics over others.

Some data from low scoring papers, e.g [S3], are not justified very
detailed which could cause incorrect interpretations. \fixme{Jatka tätä vielä
lisää: heikkolaatuiset tutkimukset voi johtaa vääriin tuloksiin? Voisi myös
mainita sen että vain yksi henkilö teki laatuarvioinnit}


\chapter{Conclusions}%\juha{Kirjoitin tänne hieman lisää...} 
\label{sec:Conclusions}

This study presents the results from a systematic literature review from 29
\fixme{numero}primary studies. According to the researcher's knowledge there
is no previous systematic reviews of measurement use in the context of
industrial agile software development. This study classifies and describes the
main measurement types and areas that are reported in empirical studies. This
study provides descriptions of how and why metrics are used to support agile
software development.\fixme{lisää important metrics ja metrics categories}
This study also analyzed how the presented metrics support the twelve
principles of Agile Manifesto \citepsrc{beck2001agile}.

The results indicate that the reasons and use of metrics is focused on the
following areas:
\nameref{sec:IterationPlanning}, \nameref{sec:IterationTracking},
\nameref{sec:Motivate}, \nameref{sec:ProblemIdentification},
\nameref{sec:PreQuality}, \nameref{sec:PostQuality} and
\nameref{sec:ChangesInProcesses}.

This paper provides researchers and practitioners with an useful overview of
the measurements use in agile context and documented reasonings behind the
proposed metrics. This study can be used as a source of relevant sources
regarding researchers' interests and contexts.

Finally, this study identified few propositions for future research on
measuring in agile software development. First, in the academia lot of
emphasis has been given to code metrics yet this study found little evidence
of their use in agile context. Second, the applicable quality metrics for
agile development and the relationship of pre-release quality metrics and post-release quality are
important directions of future research. Third, this study found that planning
and tracking metrics for iteration were often used indicating a need to focus
future research efforts on these areas. Fourth, use of metrics for motivating
and enforcing process improvements can be an interesting future research
topic.

\fixme{Add future work - negative effects of metrics. negative motivation
effects of metrics.}

\fixme{lisää important metrics ja metric categories}
