\chapter{Search strings}
\label{app:Strings}

The first search string was:

TITLE-ABS-KEY(software AND (agile OR lean OR "crystal method" OR "crystal
clear" OR dsdm OR "dynamic systems development method" OR fdd OR "feature
driven development" OR "agile unified process" OR "agile modeling" OR scrumban
OR kanban OR scrum OR "extreme programming" OR xp) AND (measur* OR metric OR
diagnostic OR monitor*)) AND (LIMIT-TO(SUBJAREA, "COMP")) AND
(LIMIT-TO(LANGUAGE, "English"))\\

It found 512 hits 19 September 2013.\\\\

%Then we noticed that some previously found key papers were missing from the
%hits because they were under sub area ``Engineering'', thus 
The second search string was:\\

TITLE-ABS-KEY(software AND (agile OR lean OR "crystal method" OR "crystal
clear" OR dsdm OR "dynamic systems development method" OR fdd OR "feature
driven development" OR "agile unified process" OR "agile modeling" OR scrumban
OR kanban OR scrum OR "extreme programming" OR xp) AND (measur* OR metric OR
diagnosticOR monitor*)) AND (LIMIT-TO(LANGUAGE, "English")) AND
(LIMIT-TO(SUBJAREA, "ENGI")) AND (EXCLUDE (SUBJAREA, "COMP") OR
EXCLUDE(SUBJAREA, "PHYS") OR EXCLUDE(SUBJAREA,"MATE") OR EXCLUDE (SUBJAREA,
"BUSI") OR EXCLUDE(SUBJAREA, "MATH") OR EXCLUDE(SUBJAREA, "ENVI") OR
EXCLUDE (SUBJAREA, "EART") OR EXCLUDE(SUBJAREA, "DECI") OREXCLUDE (SUBJAREA,
"ENER"))\\

It found 220 hits 7 November 2013.\\\\

The third search string was:\\

TITLE-ABS-KEY(software AND (agile OR lean OR "crystal method" OR "crystal
clear" OR dsdm OR "dynamic systems development method" OR fdd OR "feature
driven development" OR "agile unified process" OR "agile modeling" OR scrumban
OR kanban OR scrum OR "extreme programming" OR xp) AND (measur* OR metric OR
diagnosticOR monitor*)) AND (LIMIT-TO(LANGUAGE, "English")) AND
(LIMIT-TO(SUBJAREA, "BUSI")) AND (EXCLUDE (SUBJAREA, "ENGI") OR
EXCLUDE(SUBJAREA, "COMP"))\\

It found 42 hits 10 December 2013.

\chapter{Inclusion and exclusion criteria}
\label{app:Criteria}

Inclusion criteria
\begin{itemize}
  \item Papers that present the use and experiences of metrics in an agile
  industry setting.
\end{itemize}

Exclusion criteria
\begin{itemize}
  \item Papers that do not contain empirical data from industry cases.
  \item Papers that are not in English.
  \item Papers that do not have agile context. There is evidence of
  clearly non-agile practices or there is no agile method named. For example,
  paper mentions agile but case company has only three releases per year.
  \item Paper is only about one agile practice, which is not related to
  measuring.
  \item Papers that do not seem to have any data about metric usage.
  Similarly, if there are only a few descriptions of metrics but no other info regarding
  reasons or usage.
  \item Papers that have serious issues with grammar or vocabulary and
  therefore it takes considerable effort to understand sentences.
  %\item Papers that refer to another paper where the actual case is discussed.
  %\item Papers that are in academic or semi-academic setting - customer or
  %part of workers are from industry. The reason for this is that it doesn't
  %fully represent industry setting as software development methods are likely
  %enforced by academia. \juha{tämä sisältyy tai vain tarkentaa ensimmäistä}
  \item Papers where the setting is not clear or results cannot be separated by setting, for example
  surveys where there is data both from academia and industry. %Similar
%  exclusion if results cannot be separated by software development method. 
  %\item Papers where the setting is not clear. For example only mention of
  %context is ``100 junior developers''.\juha{yhdistin edelliseen}

  %%\item Papers that are full conference proceedings. Individual papers should be already listed separately.
  \item Papers where the metrics are only used for the research. For
  example, author measures which agile practices correlate with success.
  %%\item Papers that don't even show measurement usage in a pilot setting. For
  %%example method or metric is used against static industrial data set. \juha{tämä sisältyy tai vain tarkentaa ensimmäistä}
\end{itemize}

\chapter{Quality assesment questions}
\label{app:quality}

Based on the quality evaluation form by \citetsrc{dyba_empirical_2008}.

\begin{enumerate}
  \item Is this a research paper?
  \item Is there are a clear statement of the aims of the research?
  \item Is there an adequate description of the context in which the research
  was carried out?
  \item Was the research design appropriate to address the aims of the
  research?
  \item Was the recruitment strategy appropriate to the aims of the research?
  \item Was there a control group with which to compare treatments?
  \item Was the data collected in a way that addressed the research issue?
  \item Was the data analysis sufficiently rigorous?
  \item Has the relationship between researcher and participants been
  considered adequately?
  \item Is there a clear statement of findings?
  \item Is the study of value for research or practice?
\end{enumerate}

\chapter{Definions of metrics}
\label{app:definitionsOfMetrics}

% Table generated by Excel2LaTeX from sheet 'DefinitionsOfMetrics'

{\small
\begin{longtable}{p{1,5cm}p{3cm}p{7,5cm}} 
\caption{Definitions of found metrics}\\
\toprule\\
Primary study &     Metric & Definition \\
\midrule\\

     [S10] & \# of defects & Issues found from quality assurance cycle including differences from expected behavior. \\

      [S7] & \# of defects found in system test & Number of defects found in system test phase. \\

     [S25] & \# of defects in backlog & All known and unresolved defects in the project. \\

      [S7] & \# of open defects & Number of open defects on the current release per day. \\

     [S22] & \# of requirements per phase & Number of requirements (work items/features) per phase. \\

     [S14] & \# of unit tests & Number of unit tests. \\

     [S23] & Average velocity & Not clearly defined in primary study.  \\

 [S4, S14] & Build status & Build broken or not. \\

[S5, S27, S28] &   Burndown & Remaining human resource days versus the remaining work days. \\

      [S7] &   Burndown & Not defined in primary study.  \\

      [S1] & Business value delivered & Not defined in primary study. Probably means delivered features per timeframe. \\

     [S19] & Change requests per requirement & Amount of change requests from customer per requirement. \\

[S5, S27, S28] & Check-ins per day & Number of commits (code, automated test, specification) per day. \\

     [S17] & Common tempo time & Net working days available per number of (work) units required. \\

     [S12] & Completed web pages & Completed web pages. \\

     [S16] & Cost performance index & Not defined in primary study.  \\

      [S3] & Critical defects sent by customer & No detailed definition in primary study.  \\

      [S1] & Customer satisfaction & Not defined in primary study.  \\

     [S17] & Customer satisfaction (Kano analysis) & Not clearly defined in primary study.  \\

     [S17] & Cycle time & Not defined in primary study.  \\

     [S23] & Cycle time & Time it takes for x size story to be completed. \\

      [S1] & Defect count after testing & Not defined in primary study. Probably means amount of defects after first round of testing. \\

     [S25] & Defect trend indicator & Indicates if amount of defects in the coming week will increase, stay the same or decrease from this week. \\

      [S7] & Defects deferred & Not defined in primary study. Probably means the amount of defects that are known but are not fixed for the release. \\

      [S9] & Effort estimate & Estimated effort per story in ideal pair days. \\

     [S12] & Effort estimate & Not clearly defined in primary study.  \\

[S15, S15, S15, S15] & Effort estimate & Not defined in primary study.  \\

     [S17] & Effort estimate kits & Tasks are broken down into kits of two to five staff-days of work. \\

     [S19] & Fault slips & Amount of issues that should have been found already in the previous phase. \\

      [S5] & Faults per iteration & Faults per iteration. \\

     [S13] & Fix time of failed build & Fix time of failed build. \\

     [S19] & Implemented vs wasted requirements & Ratio of implemented requirements and wasted requirements. Not all requirements are  always implemented but some work is put into them, e.g., in the form of technical specification. \\

     [S20] & Inventory of requirements over time & Amount of requirements (features/work items) in specific work phase over time. \\

     [S18] &  Lead time & The average time it takes for one request to go through the entire process from start to finish. \\

[S19, S22] &  Lead time & Time it takes for requirement to go through a sub-process or the whole process. \\

     [S24] &  Lead time & Not clearly defined in primary study.  \\

     [S19] & Maintenance effort & Costs related to fixing issues that have been found and reported by customers. \\

      [S7] & Net Promoter Score & Not defined in primary study. Probably measures how likely customers will recommend the product to another customer. \\

[S5, S27, S28] & Number of automated passing test steps & Number of automated passing test steps. \\

     [S17] & Number of bounce backs & Not defined in primary study. Probably the amount of defects that should have not occurred anymore if a root cause would have been fixed earlier. \\

     [S27] & Number of new and open defects & Number of new and open defects. \\

     [S20] & Number of requests from customers & Not defined in primary study.  \\

      [S1] & Number of test cases & Not defined in primary study.  \\

      [S3] & Open defects & Not defined in primary study.  \\

      [S8] & Operations' velocity & Not defined in primary study. Probably Operations department's completed story points per time unit. \\

     [S13] & Percentage of stories prepared for sprint & Percentage of stories prepared for sprint. \\

     [S16] & Planned velocity & Not clearly defined in primary study.  \\

     [S25] & predicted \# of defects & Predicted number of defects in backlog in the coming week. \\

     [S18] & Processing time & The time the request is being worked on by one person or a team. \\

     [S30] & Progress as working code & Product is demonstrated to the customer who then gives feedback. \\

     [S23] & Pseudo velocity & Not clearly defined in primary study.  \\

     [S26] &      Queue & Number of units remaining to be developed/processed by a given phase or activity. \\

     [S18] & Queue time & The average time between sub-processes that the request sits around waiting. \\

     [S21] & Rate of requirements per phase & Rate of requirements flow from a phase to next phase. \\

     [S16] & Release burndown & Amount of work remaining till the release. \\

      [S3] & Remaining task effort & Not defined in primary study.  \\

     [S21] & Requirement's cost types & Cost distribution of a requirement. \\

     [S11] & Revenue per customer & Amount of revenue from customer per feature.  \\

      [S1] & Running tested features & Not defined in primary study. Probably means amount of features delivered to customer that are passing unit tests. \\

     [S16] & Schedule performance index & Not defined in primary study.  \\

     [S16] & Sprint burndown & Amount of work remaining till the end of sprint. \\

     [S29] & Story complete percentage & Not clearly defined in primary study.  \\

     [S29] & Story estimate & Estimated days to complete the story. \\

      [S6] & Story estimates & Estimated time to develop a story. \\

     [S13] & Story flow percentage & Estimated implemention time per actual implemention time * 100. \\

      [S7] & Story points & Not defined in primary study.  \\

      [S8] & Story points & Estimated effort to complete the story in programmer days. \\

     [S12] &  Task done & Task done. \\

      [S8] & Task effort & Estimated effort to complete the task in programmer hours. \\

     [S12] & Task's expected end date & Date when a task is estimated to be finished. \\

      [S3] & Team effectiveness & Not defined in primary study.  \\

      [S4] & Technical debt board & Shows the status of each technical debt category per team. \\

      [S4] & Technical debt in effort & Technical debt in amount of hours it would take to fix all the issues increasing technical debt calculated by third party tool called Sonar. \\

     [S14] & Test coverage & How much Source Code executed during Test Execution. \\

      [S3] & Test failure rate & Not defined in primary study.  \\

     [S14] & Test growth ratio & Difference of amount of tests per difference of amount of Source Code. \\

      [S3] & Test success rate & Not defined in primary study.  \\

     [S26] & Throughput & Number of units processed by a given phase or activity per time. \\

     [S21] & Variance in handovers & Changes in amount of handed over requirements. \\

      [S2] &   Velocity & Amount of developed scenarios per developer per week. \\

      [S6] &   Velocity & Not defined in primary study.  \\

      [S8] &   Velocity & Not defined in primary study.  \\

     [S10] &   Velocity & Feature points developed per iteration. \\

     [S13] & Velocity of elaborating features & Not clearly defined in primary study. Probably the time it takes to clarify a feature from customer into requirements that can be implemented. \\

     [S13] & Velocity of implementing features & Not clearly defined in primary study. Probably the time it takes to implement a feature. \\

     [S14] & Violations of static code analysis & Amount of violations to static code analysis rules from tools like Findbugs, PMD and Checkstyle. \\

     [S17] & Work in progress & Amount of features or feature level integrations team is working on. \\

     [S23] & Work in progress & Amount of stories per work phase. \\

     [S24] & Work in progress & Amount of work items per phase. \\
     
     \bottomrule\\
     \label{tab:definitionsOfMetrics}

\end{longtable}  
}


% Table generated by Excel2LaTeX from sheet 'DefinitionsOfMetrics'


